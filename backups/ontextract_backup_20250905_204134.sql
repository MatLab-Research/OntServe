--
-- PostgreSQL database dump
--

\restrict 8GYRd8dpdd6bg7SNL43N3J9QUBKlnTLhKqsLiYNY5z3c9ztcl4FQhlXV9kk4Vkh

-- Dumped from database version 17.6 (Ubuntu 17.6-1.pgdg24.04+1)
-- Dumped by pg_dump version 17.6 (Ubuntu 17.6-1.pgdg24.04+1)

SET statement_timeout = 0;
SET lock_timeout = 0;
SET idle_in_transaction_session_timeout = 0;
SET transaction_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SELECT pg_catalog.set_config('search_path', '', false);
SET check_function_bodies = false;
SET xmloption = content;
SET client_min_messages = warning;
SET row_security = off;

--
-- Name: pg_trgm; Type: EXTENSION; Schema: -; Owner: -
--

CREATE EXTENSION IF NOT EXISTS pg_trgm WITH SCHEMA public;


--
-- Name: EXTENSION pg_trgm; Type: COMMENT; Schema: -; Owner: 
--

COMMENT ON EXTENSION pg_trgm IS 'text similarity measurement and index searching based on trigrams';


--
-- Name: pgcrypto; Type: EXTENSION; Schema: -; Owner: -
--

CREATE EXTENSION IF NOT EXISTS pgcrypto WITH SCHEMA public;


--
-- Name: EXTENSION pgcrypto; Type: COMMENT; Schema: -; Owner: 
--

COMMENT ON EXTENSION pgcrypto IS 'cryptographic functions';


--
-- Name: uuid-ossp; Type: EXTENSION; Schema: -; Owner: -
--

CREATE EXTENSION IF NOT EXISTS "uuid-ossp" WITH SCHEMA public;


--
-- Name: EXTENSION "uuid-ossp"; Type: COMMENT; Schema: -; Owner: 
--

COMMENT ON EXTENSION "uuid-ossp" IS 'generate universally unique identifiers (UUIDs)';


--
-- Name: vector; Type: EXTENSION; Schema: -; Owner: -
--

CREATE EXTENSION IF NOT EXISTS vector WITH SCHEMA public;


--
-- Name: EXTENSION vector; Type: COMMENT; Schema: -; Owner: 
--

COMMENT ON EXTENSION vector IS 'vector data type and ivfflat and hnsw access methods';


--
-- Name: update_context_anchor_frequency(); Type: FUNCTION; Schema: public; Owner: ontextract_user
--

CREATE FUNCTION public.update_context_anchor_frequency() RETURNS trigger
    LANGUAGE plpgsql
    AS $$
BEGIN
    IF TG_OP = 'INSERT' THEN
        INSERT INTO context_anchors (anchor_term, frequency, first_used_in, last_used_in)
        VALUES ((SELECT anchor_term FROM context_anchors WHERE id = NEW.context_anchor_id), 1, NEW.term_version_id, NEW.term_version_id)
        ON CONFLICT (anchor_term) DO UPDATE SET
            frequency = context_anchors.frequency + 1,
            last_used_in = NEW.term_version_id;
        RETURN NEW;
    ELSIF TG_OP = 'DELETE' THEN
        UPDATE context_anchors SET frequency = frequency - 1 
        WHERE id = OLD.context_anchor_id;
        RETURN OLD;
    END IF;
    RETURN NULL;
END;
$$;


ALTER FUNCTION public.update_context_anchor_frequency() OWNER TO ontextract_user;

--
-- Name: update_terms_updated_at(); Type: FUNCTION; Schema: public; Owner: ontextract_user
--

CREATE FUNCTION public.update_terms_updated_at() RETURNS trigger
    LANGUAGE plpgsql
    AS $$
BEGIN
    NEW.updated_at = CURRENT_TIMESTAMP;
    RETURN NEW;
END;
$$;


ALTER FUNCTION public.update_terms_updated_at() OWNER TO ontextract_user;

SET default_tablespace = '';

SET default_table_access_method = heap;

--
-- Name: analysis_agents; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.analysis_agents (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    agent_type character varying(20) NOT NULL,
    name character varying(200) NOT NULL,
    description text,
    version character varying(50),
    algorithm_type character varying(100),
    model_parameters json,
    training_data character varying(200),
    expertise_domain character varying(100),
    institutional_affiliation character varying(200),
    created_at timestamp with time zone,
    is_active boolean,
    user_id integer,
    CONSTRAINT analysis_agents_agent_type_check CHECK (((agent_type)::text = ANY ((ARRAY['SoftwareAgent'::character varying, 'Person'::character varying, 'Organization'::character varying])::text[])))
);


ALTER TABLE public.analysis_agents OWNER TO ontextract_user;

--
-- Name: context_anchors; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.context_anchors (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    anchor_term character varying(255) NOT NULL,
    frequency integer,
    first_used_in uuid,
    last_used_in uuid,
    created_at timestamp with time zone
);


ALTER TABLE public.context_anchors OWNER TO ontextract_user;

--
-- Name: document_embeddings; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.document_embeddings (
    id integer NOT NULL,
    document_id integer,
    term character varying(200) NOT NULL,
    period integer,
    embedding public.vector(384),
    model_name character varying(100),
    context_window text,
    extraction_method character varying(50),
    metadata jsonb,
    created_at timestamp without time zone DEFAULT now(),
    updated_at timestamp without time zone DEFAULT now()
);


ALTER TABLE public.document_embeddings OWNER TO ontextract_user;

--
-- Name: document_embeddings_id_seq; Type: SEQUENCE; Schema: public; Owner: ontextract_user
--

CREATE SEQUENCE public.document_embeddings_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.document_embeddings_id_seq OWNER TO ontextract_user;

--
-- Name: document_embeddings_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: ontextract_user
--

ALTER SEQUENCE public.document_embeddings_id_seq OWNED BY public.document_embeddings.id;


--
-- Name: documents; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.documents (
    id integer NOT NULL,
    title character varying(200) NOT NULL,
    content_type character varying(20) NOT NULL,
    document_type character varying(20) NOT NULL,
    reference_subtype character varying(30),
    file_type character varying(10),
    original_filename character varying(255),
    file_path character varying(500),
    file_size integer,
    source_metadata json,
    content text,
    content_preview text,
    detected_language character varying(10),
    language_confidence double precision,
    status character varying(20) NOT NULL,
    word_count integer,
    character_count integer,
    created_at timestamp without time zone NOT NULL,
    updated_at timestamp without time zone,
    processed_at timestamp without time zone,
    user_id integer NOT NULL,
    embedding character varying,
    parent_document_id integer
);


ALTER TABLE public.documents OWNER TO postgres;

--
-- Name: documents_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.documents_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.documents_id_seq OWNER TO postgres;

--
-- Name: documents_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.documents_id_seq OWNED BY public.documents.id;


--
-- Name: domains; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.domains (
    id integer NOT NULL,
    uuid uuid NOT NULL,
    name character varying(255) NOT NULL,
    display_name character varying(255),
    namespace_uri text NOT NULL,
    description text,
    metadata json,
    is_active boolean,
    created_at timestamp with time zone,
    updated_at timestamp with time zone
);


ALTER TABLE public.domains OWNER TO ontextract_user;

--
-- Name: domains_id_seq; Type: SEQUENCE; Schema: public; Owner: ontextract_user
--

CREATE SEQUENCE public.domains_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.domains_id_seq OWNER TO ontextract_user;

--
-- Name: domains_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: ontextract_user
--

ALTER SEQUENCE public.domains_id_seq OWNED BY public.domains.id;


--
-- Name: experiment_documents; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.experiment_documents (
    experiment_id integer NOT NULL,
    document_id integer NOT NULL,
    added_at timestamp without time zone
);


ALTER TABLE public.experiment_documents OWNER TO postgres;

--
-- Name: experiment_references; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.experiment_references (
    experiment_id integer NOT NULL,
    reference_id integer NOT NULL,
    include_in_analysis boolean,
    added_at timestamp without time zone,
    notes text
);


ALTER TABLE public.experiment_references OWNER TO postgres;

--
-- Name: experiments; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.experiments (
    id integer NOT NULL,
    name character varying(200) NOT NULL,
    description text,
    experiment_type character varying(50) NOT NULL,
    configuration text,
    status character varying(20) NOT NULL,
    results text,
    results_summary text,
    created_at timestamp without time zone NOT NULL,
    updated_at timestamp without time zone,
    started_at timestamp without time zone,
    completed_at timestamp without time zone,
    user_id integer NOT NULL
);


ALTER TABLE public.experiments OWNER TO postgres;

--
-- Name: experiments_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.experiments_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.experiments_id_seq OWNER TO postgres;

--
-- Name: experiments_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.experiments_id_seq OWNED BY public.experiments.id;


--
-- Name: extracted_entities; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.extracted_entities (
    id integer NOT NULL,
    entity_text character varying(500) NOT NULL,
    entity_type character varying(100) NOT NULL,
    entity_subtype character varying(100),
    context_before character varying(200),
    context_after character varying(200),
    sentence text,
    start_position integer,
    end_position integer,
    paragraph_number integer,
    sentence_number integer,
    confidence_score double precision,
    extraction_method character varying(50),
    properties text,
    language character varying(10),
    normalized_form character varying(500),
    created_at timestamp without time zone NOT NULL,
    updated_at timestamp without time zone,
    processing_job_id integer NOT NULL,
    text_segment_id integer
);


ALTER TABLE public.extracted_entities OWNER TO postgres;

--
-- Name: extracted_entities_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.extracted_entities_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.extracted_entities_id_seq OWNER TO postgres;

--
-- Name: extracted_entities_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.extracted_entities_id_seq OWNED BY public.extracted_entities.id;


--
-- Name: fuzziness_adjustments; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.fuzziness_adjustments (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    term_version_id uuid NOT NULL,
    original_score numeric(4,3) NOT NULL,
    adjusted_score numeric(4,3) NOT NULL,
    adjustment_reason text NOT NULL,
    adjusted_by integer,
    created_at timestamp with time zone
);


ALTER TABLE public.fuzziness_adjustments OWNER TO ontextract_user;

--
-- Name: ontologies; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.ontologies (
    id integer NOT NULL,
    uuid uuid NOT NULL,
    domain_id integer,
    name character varying(255) NOT NULL,
    base_uri text NOT NULL,
    description text,
    is_base boolean,
    is_editable boolean,
    parent_ontology_id integer,
    ontology_type character varying(20),
    metadata json,
    created_at timestamp with time zone NOT NULL,
    updated_at timestamp with time zone
);


ALTER TABLE public.ontologies OWNER TO ontextract_user;

--
-- Name: ontologies_id_seq; Type: SEQUENCE; Schema: public; Owner: ontextract_user
--

CREATE SEQUENCE public.ontologies_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.ontologies_id_seq OWNER TO ontextract_user;

--
-- Name: ontologies_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: ontextract_user
--

ALTER SEQUENCE public.ontologies_id_seq OWNED BY public.ontologies.id;


--
-- Name: ontology_entities; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.ontology_entities (
    id integer NOT NULL,
    ontology_id integer NOT NULL,
    entity_type character varying(50) NOT NULL,
    uri text NOT NULL,
    label character varying(255),
    comment text,
    parent_uri text,
    domain json,
    range json,
    properties json,
    embedding public.vector(384),
    created_at timestamp without time zone
);


ALTER TABLE public.ontology_entities OWNER TO ontextract_user;

--
-- Name: ontology_entities_id_seq; Type: SEQUENCE; Schema: public; Owner: ontextract_user
--

CREATE SEQUENCE public.ontology_entities_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.ontology_entities_id_seq OWNER TO ontextract_user;

--
-- Name: ontology_entities_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: ontextract_user
--

ALTER SEQUENCE public.ontology_entities_id_seq OWNED BY public.ontology_entities.id;


--
-- Name: ontology_mappings; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.ontology_mappings (
    id integer NOT NULL,
    ontology_uri character varying(500) NOT NULL,
    concept_label character varying(200) NOT NULL,
    concept_definition text,
    parent_concepts text,
    child_concepts text,
    related_concepts text,
    mapping_confidence double precision,
    mapping_method character varying(50),
    mapping_source character varying(100),
    semantic_type character varying(100),
    domain character varying(100),
    properties text,
    is_verified boolean,
    verified_by character varying(100),
    verification_notes text,
    alternative_mappings text,
    created_at timestamp without time zone NOT NULL,
    updated_at timestamp without time zone,
    verified_at timestamp without time zone,
    extracted_entity_id integer NOT NULL
);


ALTER TABLE public.ontology_mappings OWNER TO postgres;

--
-- Name: ontology_mappings_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.ontology_mappings_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.ontology_mappings_id_seq OWNER TO postgres;

--
-- Name: ontology_mappings_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.ontology_mappings_id_seq OWNED BY public.ontology_mappings.id;


--
-- Name: ontology_versions; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.ontology_versions (
    id integer NOT NULL,
    ontology_id integer NOT NULL,
    version_number integer NOT NULL,
    version_tag character varying(50),
    content text NOT NULL,
    content_hash character varying(64),
    change_summary text,
    created_by character varying(255),
    created_at timestamp with time zone NOT NULL,
    is_current boolean,
    is_draft boolean,
    workflow_status character varying(20),
    metadata json
);


ALTER TABLE public.ontology_versions OWNER TO ontextract_user;

--
-- Name: ontology_versions_id_seq; Type: SEQUENCE; Schema: public; Owner: ontextract_user
--

CREATE SEQUENCE public.ontology_versions_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.ontology_versions_id_seq OWNER TO ontextract_user;

--
-- Name: ontology_versions_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: ontextract_user
--

ALTER SEQUENCE public.ontology_versions_id_seq OWNED BY public.ontology_versions.id;


--
-- Name: processing_jobs; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.processing_jobs (
    id integer NOT NULL,
    job_type character varying(50) NOT NULL,
    job_name character varying(100),
    provider character varying(20),
    model character varying(50),
    parameters text,
    status character varying(20) NOT NULL,
    progress_percent integer,
    current_step character varying(100),
    total_steps integer,
    result_data text,
    result_summary text,
    error_message text,
    error_details text,
    retry_count integer,
    max_retries integer,
    tokens_used integer,
    processing_time double precision,
    cost_estimate double precision,
    created_at timestamp without time zone NOT NULL,
    started_at timestamp without time zone,
    completed_at timestamp without time zone,
    updated_at timestamp without time zone,
    user_id integer NOT NULL,
    document_id integer NOT NULL,
    parent_job_id integer
);


ALTER TABLE public.processing_jobs OWNER TO postgres;

--
-- Name: processing_jobs_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.processing_jobs_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.processing_jobs_id_seq OWNER TO postgres;

--
-- Name: processing_jobs_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.processing_jobs_id_seq OWNED BY public.processing_jobs.id;


--
-- Name: provenance_chains; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.provenance_chains (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    entity_id uuid,
    entity_type character varying(30) NOT NULL,
    was_derived_from uuid,
    derivation_activity uuid,
    derivation_metadata json,
    created_at timestamp with time zone
);


ALTER TABLE public.provenance_chains OWNER TO ontextract_user;

--
-- Name: search_history; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.search_history (
    id integer NOT NULL,
    query text NOT NULL,
    query_type character varying(50),
    results_count integer,
    execution_time double precision,
    user_id character varying(255),
    ip_address character varying(45),
    created_at timestamp without time zone
);


ALTER TABLE public.search_history OWNER TO ontextract_user;

--
-- Name: search_history_id_seq; Type: SEQUENCE; Schema: public; Owner: ontextract_user
--

CREATE SEQUENCE public.search_history_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.search_history_id_seq OWNER TO ontextract_user;

--
-- Name: search_history_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: ontextract_user
--

ALTER SEQUENCE public.search_history_id_seq OWNED BY public.search_history.id;


--
-- Name: semantic_drift_activities; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.semantic_drift_activities (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    activity_type character varying(50) NOT NULL,
    start_period character varying(50) NOT NULL,
    end_period character varying(50) NOT NULL,
    temporal_scope_years integer[],
    used_entity uuid,
    generated_entity uuid,
    was_associated_with uuid,
    drift_metrics json,
    detection_algorithm character varying(100),
    algorithm_parameters json,
    started_at_time timestamp with time zone,
    ended_at_time timestamp with time zone,
    activity_status character varying(20),
    drift_detected boolean,
    drift_magnitude numeric(4,3),
    drift_type character varying(30),
    evidence_summary text,
    created_by integer,
    created_at timestamp with time zone,
    CONSTRAINT semantic_drift_activities_activity_status_check CHECK (((activity_status)::text = ANY ((ARRAY['running'::character varying, 'completed'::character varying, 'error'::character varying, 'provisional'::character varying])::text[]))),
    CONSTRAINT semantic_drift_activities_drift_magnitude_check CHECK (((drift_magnitude >= (0)::numeric) AND (drift_magnitude <= (1)::numeric)))
);


ALTER TABLE public.semantic_drift_activities OWNER TO ontextract_user;

--
-- Name: term_version_anchors; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.term_version_anchors (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    term_version_id uuid NOT NULL,
    context_anchor_id uuid NOT NULL,
    similarity_score numeric(4,3),
    rank_in_neighborhood integer,
    created_at timestamp with time zone
);


ALTER TABLE public.term_version_anchors OWNER TO ontextract_user;

--
-- Name: term_versions; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.term_versions (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    term_id uuid NOT NULL,
    temporal_period character varying(50) NOT NULL,
    temporal_start_year integer,
    temporal_end_year integer,
    meaning_description text NOT NULL,
    context_anchor json,
    original_context_anchor json,
    fuzziness_score numeric(4,3),
    confidence_level character varying(10),
    certainty_notes text,
    corpus_source character varying(100),
    source_documents json,
    extraction_method character varying(50),
    generated_at_time timestamp with time zone,
    was_derived_from uuid,
    derivation_type character varying(30),
    version_number integer,
    is_current boolean,
    created_by integer,
    created_at timestamp with time zone,
    neighborhood_overlap numeric(4,3),
    positional_change numeric(4,3),
    similarity_reduction numeric(4,3),
    source_citation text,
    CONSTRAINT term_versions_confidence_level_check CHECK (((confidence_level)::text = ANY ((ARRAY['high'::character varying, 'medium'::character varying, 'low'::character varying])::text[]))),
    CONSTRAINT term_versions_fuzziness_score_check CHECK (((fuzziness_score >= (0)::numeric) AND (fuzziness_score <= (1)::numeric))),
    CONSTRAINT term_versions_neighborhood_overlap_check CHECK (((neighborhood_overlap >= (0)::numeric) AND (neighborhood_overlap <= (1)::numeric))),
    CONSTRAINT term_versions_positional_change_check CHECK (((positional_change >= (0)::numeric) AND (positional_change <= (1)::numeric))),
    CONSTRAINT term_versions_similarity_reduction_check CHECK (((similarity_reduction >= (0)::numeric) AND (similarity_reduction <= (1)::numeric)))
);


ALTER TABLE public.term_versions OWNER TO ontextract_user;

--
-- Name: COLUMN term_versions.source_citation; Type: COMMENT; Schema: public; Owner: ontextract_user
--

COMMENT ON COLUMN public.term_versions.source_citation IS 'Academic citation for this temporal version meaning (e.g., dictionary reference, paper, etc.)';


--
-- Name: terms; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.terms (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    term_text character varying(255) NOT NULL,
    entry_date timestamp with time zone,
    status character varying(20) NOT NULL,
    created_by integer,
    updated_by integer,
    created_at timestamp with time zone,
    updated_at timestamp with time zone,
    description text,
    etymology text,
    notes text,
    research_domain character varying(100),
    selection_rationale text,
    historical_significance text,
    CONSTRAINT terms_status_check CHECK (((status)::text = ANY ((ARRAY['active'::character varying, 'provisional'::character varying, 'deprecated'::character varying])::text[])))
);


ALTER TABLE public.terms OWNER TO ontextract_user;

--
-- Name: text_segments; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.text_segments (
    id integer NOT NULL,
    content text NOT NULL,
    segment_type character varying(50),
    segment_number integer,
    start_position integer,
    end_position integer,
    parent_segment_id integer,
    level integer,
    word_count integer,
    character_count integer,
    sentence_count integer,
    language character varying(10),
    language_confidence double precision,
    embedding character varying,
    embedding_model character varying(100),
    processed boolean,
    processing_notes text,
    topics text,
    keywords text,
    sentiment_score double precision,
    complexity_score double precision,
    created_at timestamp without time zone NOT NULL,
    updated_at timestamp without time zone,
    processed_at timestamp without time zone,
    document_id integer NOT NULL
);


ALTER TABLE public.text_segments OWNER TO postgres;

--
-- Name: text_segments_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.text_segments_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.text_segments_id_seq OWNER TO postgres;

--
-- Name: text_segments_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.text_segments_id_seq OWNED BY public.text_segments.id;


--
-- Name: users; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.users (
    id integer NOT NULL,
    username character varying(80) NOT NULL,
    email character varying(120) NOT NULL,
    password_hash character varying(256) NOT NULL,
    first_name character varying(50),
    last_name character varying(50),
    organization character varying(100),
    is_active boolean NOT NULL,
    is_admin boolean NOT NULL,
    created_at timestamp without time zone NOT NULL,
    updated_at timestamp without time zone,
    last_login timestamp without time zone
);


ALTER TABLE public.users OWNER TO postgres;

--
-- Name: users_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.users_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.users_id_seq OWNER TO postgres;

--
-- Name: users_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.users_id_seq OWNED BY public.users.id;


--
-- Name: document_embeddings id; Type: DEFAULT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.document_embeddings ALTER COLUMN id SET DEFAULT nextval('public.document_embeddings_id_seq'::regclass);


--
-- Name: documents id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.documents ALTER COLUMN id SET DEFAULT nextval('public.documents_id_seq'::regclass);


--
-- Name: domains id; Type: DEFAULT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.domains ALTER COLUMN id SET DEFAULT nextval('public.domains_id_seq'::regclass);


--
-- Name: experiments id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.experiments ALTER COLUMN id SET DEFAULT nextval('public.experiments_id_seq'::regclass);


--
-- Name: extracted_entities id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.extracted_entities ALTER COLUMN id SET DEFAULT nextval('public.extracted_entities_id_seq'::regclass);


--
-- Name: ontologies id; Type: DEFAULT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.ontologies ALTER COLUMN id SET DEFAULT nextval('public.ontologies_id_seq'::regclass);


--
-- Name: ontology_entities id; Type: DEFAULT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.ontology_entities ALTER COLUMN id SET DEFAULT nextval('public.ontology_entities_id_seq'::regclass);


--
-- Name: ontology_mappings id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.ontology_mappings ALTER COLUMN id SET DEFAULT nextval('public.ontology_mappings_id_seq'::regclass);


--
-- Name: ontology_versions id; Type: DEFAULT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.ontology_versions ALTER COLUMN id SET DEFAULT nextval('public.ontology_versions_id_seq'::regclass);


--
-- Name: processing_jobs id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.processing_jobs ALTER COLUMN id SET DEFAULT nextval('public.processing_jobs_id_seq'::regclass);


--
-- Name: search_history id; Type: DEFAULT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.search_history ALTER COLUMN id SET DEFAULT nextval('public.search_history_id_seq'::regclass);


--
-- Name: text_segments id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.text_segments ALTER COLUMN id SET DEFAULT nextval('public.text_segments_id_seq'::regclass);


--
-- Name: users id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.users ALTER COLUMN id SET DEFAULT nextval('public.users_id_seq'::regclass);


--
-- Data for Name: analysis_agents; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.analysis_agents (id, agent_type, name, description, version, algorithm_type, model_parameters, training_data, expertise_domain, institutional_affiliation, created_at, is_active, user_id) FROM stdin;
4a32b8b0-8581-4975-8785-929ec8c4878f	Person	Manual Curation	Human curator performing manual semantic analysis	1.0	Manual_Curation	\N	\N	\N	\N	\N	\N	\N
f959c050-3cc8-4549-a2f2-d3894198ca53	SoftwareAgent	HistBERT Temporal Embedding Alignment	Historical BERT model for temporal semantic alignment	1.0	HistBERT	\N	\N	\N	\N	\N	\N	\N
fdacd5b5-4b12-41fe-83d2-172a5581e53e	SoftwareAgent	Word2Vec Diachronic Analysis	Word2Vec model trained on temporal corpora	1.0	Word2Vec	\N	\N	\N	\N	\N	\N	\N
\.


--
-- Data for Name: context_anchors; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.context_anchors (id, anchor_term, frequency, first_used_in, last_used_in, created_at) FROM stdin;
a50cc9e3-f468-4a60-9278-14fc783189e5	agent	1	\N	\N	2025-08-24 13:18:57.911853-04
466fe463-f97c-40c8-b13f-63c539b619d8	cell	1	\N	\N	2025-08-24 13:26:19.747627-04
0f6ed02a-0d5d-4325-91cf-7f8701b4f480	ontology	1	\N	\N	2025-08-24 13:27:14.07582-04
e52c0e9a-c7b1-43c1-9db8-1e2be5f7af1a	hooligan	2	\N	703b355f-e602-40ad-a470-33a89d008dd8	2025-08-24 14:03:22.104634-04
99267944-387f-4c8d-a0ba-62bbc69d2f4d	usually	2	\N	703b355f-e602-40ad-a470-33a89d008dd8	2025-08-24 14:03:22.110767-04
0bd2bb6f-36f3-46cf-9b60-d7ab53b77fa5	young	2	\N	703b355f-e602-40ad-a470-33a89d008dd8	2025-08-24 14:03:22.113901-04
03406a50-523c-43c3-aeb4-e151c60f442a	engages	2	\N	703b355f-e602-40ad-a470-33a89d008dd8	2025-08-24 14:03:22.118188-04
187a15c2-0041-4dc1-a4fe-c6d5efc265e9	granularity	1	\N	\N	2025-08-24 13:37:42.66053-04
4e4d398a-e465-4d5a-a743-903ef78bf7d5	consisting	1	\N	\N	2025-08-24 13:37:42.669698-04
bbe2884b-d4b2-44ef-a40f-5bf2bc95d6af	appearing	1	\N	\N	2025-08-24 13:37:42.67267-04
c794432d-7542-45c4-9eb3-56f383f0a232	consist	1	\N	\N	2025-08-24 13:37:42.675239-04
20bbf849-cb9f-4bd1-b242-68da857075e3	finely	1	\N	\N	2025-08-24 13:37:42.677822-04
be420c20-5278-4184-a725-3df467434af4	detailed	1	\N	\N	2025-08-24 13:37:42.680428-04
ded8a9b4-6329-4289-8481-f3d3a580821b	word	2	\N	9bb70015-82d9-4393-b3ad-da13b1702378	2025-09-02 14:12:42.815052-04
e75826c3-2f6a-447e-b584-70fd0278bea9	language unit	2	\N	9bb70015-82d9-4393-b3ad-da13b1702378	2025-09-02 14:12:42.82656-04
acc4e4ad-04fc-42b6-9f37-b70e132b26f4	charade	2	\N	9bb70015-82d9-4393-b3ad-da13b1702378	2025-09-02 14:12:42.829576-04
14c18a05-b195-46a3-a5b0-bc3381db0894	meronym	2	\N	9bb70015-82d9-4393-b3ad-da13b1702378	2025-09-02 14:12:42.832622-04
\.


--
-- Data for Name: document_embeddings; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.document_embeddings (id, document_id, term, period, embedding, model_name, context_window, extraction_method, metadata, created_at, updated_at) FROM stdin;
\.


--
-- Data for Name: documents; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.documents (id, title, content_type, document_type, reference_subtype, file_type, original_filename, file_path, file_size, source_metadata, content, content_preview, detected_language, language_confidence, status, word_count, character_count, created_at, updated_at, processed_at, user_id, embedding, parent_document_id) FROM stdin;
63	Wooldridge and Jennings - 1995 - Intelligent agents theory and practice	file	document	\N	pdf	Wooldridge and Jennings - 1995 - Intelligent agents theory and practice.pdf	uploads/d639cec0a2e3435086c98cc8720d5040_Wooldridge_and_Jennings_-_1995_-_Intelligent_agents_theory_and_practice.pdf	1012945	\N	The Knowledge Engineering Review, Vol. 10:2, 1995, 115-152 \nIntelligent agents: theory and practice \nMICHAEL WOOLDRIDGE 1 and NICHOLAS R. JENNINGS 2 \n1 Deparrmenr of Compwing. Manche.,·ter Metropolitan Univeni1y, Chester Street, Manches1er MI 5GD, UK \n(M. Wooldridge(ri)doc.mmu.ac.uk) \n2nepartmmt of Electronic F.ngineering, Queen Mary & Westfield College, Mile End Road, London El 4NS, UK \n( N. R .JennmgI(0.1qm w. ac. uk) \nAbstract \nThe concept of an agent has become important in both artificial intelligence (AI) and mainstream \ncomputer science. Our aim in this paper is to point the reader at what we perceive to be the most \nimportant theoretical and practical issues associated with the design and construction of intelligent \nagents. For convenience, we divide these issues into three areas (though as the reader will see, the \ndivisions arc at times somewhat arbitrary). Agent theory is concerned with the question of what an \nagent is, and the use of mathematical formalisms for representing and reasoning about the \nproperties of agents. Agent architectures can he thought of as software engineering models of \nagents; researchers in this area are primarily concerned with the problem of designing software or \nhardware systems that will satisfy the properties specified by agent theorists. Finally, agent \nlanguages are software systems for programming and experimenting with agents; these languages \nmay embody principles proposed by theorists. The paper is not intended to serve as a tutorial \nintroduction to all the issues mentioned; we hope instead simply to identify the most important \nissues, and point to work that elaborates on them. The article includes a short review of current and \npotential applications of agent technology. \n1 Introduction \nWe begin our article with descriptions of three events that occur sometime in the future: \nI. The key air-traffic control system~ in the country of Ruritania suddenly fail, due to freak \nweather conditions. Fortunately, computerised air-traffic control systems in neighbouring \ncountries negotiate between themselves to track and deal with all affected flights, and the \npotentially disastrous situation passes without major incident. \n2. Upon logging in to your computer, you are presented with a list of email messages, sorted into \norder of importance by your personal digital assistant (PDA). You are then presented with a \nsimilar list of news articles; the assistant draws your attention to one particular article, which \ndescribes hitherto unknown work that is very close to your own. After an electronic discussion \nwith a number of other PD As, your PDA has already obtained a relevant technical report for \nyou from an FfP site, in the anticipation that it will be of interest. \n3. You are editing a file, when your PDA requests your attention: an email message has arrived, \nthat contaim notification about a paper you sent to an important conference, and the PDA \ncorrectly predicted that you would want to sec it as soon as possible. The paper has been \naccepted, and without prompting, the PDA begins to look into travel arrangements, by \nconsulting a number of databases and other networked information sources. A short time later, \nyou are pre~ented with a summary of the cheapest and most convenient travel options. \nWe shall not claim that computer systems of the sophistication indicated in these scenarios are just \naround the corner, hut serious academic research is underway into similar applications: air-traffic \n\nM. WOOLDRIDGE AND NICHOLAS JENNINGS 116 \ncontrol ha~ long been a research domain in distributed artificial intelligence (DAI) (Steeb et al., \n1988); various types of information manager, that filter and obtain information on behalf of their \nusers, have been prototyped (Maes, 1994a); and systems such as those that appear in the third \nscenario are discussed in (McGregor, 1992; Levy ct al., 1994). The key computer-based com­\nponents that appear in each of the above scenarios are known as agents. It is interesting to note that \none way of defining Al is by saying that it is the subfield of computer science which aims to construct \nagents that exhibit aspects of intelligent behaviour. The notion of an "agent" is thus central to AI. It \nis perhaps surprising, therefore, that until the mid to late 1980s, researchers from mainstream AI \ngave relatively little consideration to the issues surrounding agent synthesis. Since then, however, \nthere has been an intense flowering of interest in the subject: agents are now widely discussed by \nresearchers in mainstream computer science, as well as those working in data communications and \nconcurrent systems research, robotics, and user interface design. A British national daily paper \nrecently predicted that: \n"Agent-hased computing (ABC) is likely to be the next significant breakthrough in software development.·· \n(Sargent, 1992) \nMoreover, the UK-based consultancy firm Ovum has predicted that the agent technology industry \nwould be worth some US$3.5 billion worldwide by the year 2000 (Houlder, 1994). Researchers \nfrom both industry and academia arc thus taking agent technology seriously: our aim in this paper is \nto survey what we perceive to be the most important issues in the design and construction of \nintelligent agents, of the type that might ultimate appear in applications such as those suggested by \nthe fictional scenarios ahove. We begin our article, in the following sub-section, with a discussion \non the subject of exactly what an agent is. \nI. I What is an agent? \nCarl Hewitt recently remarked 1 that the question what is an agent? is embarrassing for the agent­\nbased computing community in just the same way that the question what is intelligence? is \nembarrassing for the mainstream AI community. The problem is that although the term is widely \nused, by many people working in closely related areas, it defies attempts to produce a single \nuniversally accepted definition. This need not necessarily be a problem: after all, if many people \nare successfully developing interesting and useful applications, then it hardly matters that they do \nnot agree on potentially trivial terminological details. However, there is also the danger that unless \nthe issue is discussed, "agent" might become a "noise'" term, subject to both abuse and misuse, to \nthe potential confusion of the research community. It is for this reason that we briefly consider the \nquestion. \nWe distinguish two general usages of the term "agent": the first is weak, and relatively \nuncontentious; the second is stronger, and potentially more contentious. \nI. I. I A Weak Notion of Agency \nPerhaps the most general way in which the term agent is used is to denote a hardware or (more \nusually) software-based computer system that enjoys the following properties: \n• autonomy: agents operate without the direct intervention of humans or others, and have some \nkind of control over their actions and internal state (Castelfranchi, 1995); \n• social ability: agents interact with other agents (and possibly humans) via some kind of agent­\ncommunication language (Genesereth & Ketchpel, 1994); \n• reactivity: agents perceive their environment (which may be the physical world, a user via a \ngraphical user interface, a collection of other agents, the Internet, or perhaps all of these \ncombined), and respond in a timely fashion to changes that occur in it; \n• pro-activeness: agents do not simply act in response to their environment, they are able to exhibit \ngoal-directed behaviour by taking the initiative. \n1 At the Thirteenth International Workshop on Distributed Al. \n\nIntelligent agents: theory and practice 117 \nA simple way of conceptualising an agent is thus as a kind of UNIX*like software process, that \nexhibits the properties listed above. This weak notion of agency has found currency with a \nsurprisingly wide range of researchers. For example, in mainstream computer science, the notion \nof an agent as a self-contained, concurrently executing software process, that encapsulates some \nstate and is able to communicate with other agents via message passing, is seen as a natural \ndevelopment of the objcct*based concurrent programming paradigm (Agha, 1986; Agha ct al., \n1993). \nThis weak notion of agency is also that used in the emerging discipline of agent-based software \nengineering: \n"[Agents} communicate with their peers by exchanging messages in an expressive agent communication \nLanguage. While agents can be as simple as subroutines, typically they are larger entities with some sort of \npersistent control." (Gcncscrcth & Kctchpcl, 1994, p.48) \nA softbot (software robot) is a kind of agent: \n"A softbot i~ an agent that interacts with a software environment by issuing commands and interpreting the \nenvironments feedback. A softbot's effectors arc commands (e.g. Unix shell commands such as mv or \ncompress) meant 10 change the external environments state. A softbot's sensors are commands (e.g. pwd \nor ls in Unix) meant to provide . . information." (Etzioni et al., 1994, p.10) \n1.1.2 A stronger notion of agency \nFor some researchers-particularly those working in AI- the term "agent" has a stronger and \nmore specific meaning than that sketched out above. These researchers generally mean an agent to \nbe a computer system that, in addition to having the properties identified above, is either \nconceptualised or implemented using concepts that arc more usually applied to humans. For \nexample, it is quite common in AI to characterise an agent using mentalistic notions, such as \nknowledge, belief, intention, and obligation (Shoham, 1993). Some AI researchers have gone \nfurther, and considered emotional agents (Bates et al., 1992a; Bates, 1994). (Lest the reader \nsuppose that this is just pointless anthropomorphism, it should be noted that there are good \narguments in favour of designing and building agents in terms of human-like mental states-sec \nsection 2.) Another way of giving agents human-like attributes is to represent them visually, \nperhaps by using a cartoon-like graphical icon or an animated face (Maes, 1994a, p. 36)-for \nobvious reasons, such agents are of particular importance to those interested in human-computer \ninterfaces. \n1.1.3 Other attributes of agency \nVarious other attributes arc sometimes discussed in the context of agency. For example: \n• mobility is the ability of an agent to move around an electronic network (White, 1994); \n• veracity is the assumption that an agent will not knowingly communicate false information \n(Galliers, 1988b, pp. 159-164); \n• benevolence is the assumption that agents do not have conflicting goals, and that every agent will \ntherefore always try to do what is asked of it (Rosenschein and Genesereth, 1985, p. 91); and \n• rationality is (crudely) the assumption that an agent will act in order to achieve its goals, and will \nnot act in such a way as to prevent its goals being achieved-at least insofar as its beliefs permit \n(Galliers, 1988b, pp. 49-54). \n(A discussion of some of these notions is given below; various other attributes of agency are \nformally defined in (Goodwin, 1993).) \n1.2 The structure of this article \nNow that we have at least a preliminary understanding of what an agent is, we can embark on a \nmore detailed look at their properties, and how we might go about constructing them. For \n\nM. WOOLDRIDGE AND NICHOLAS JENNINGS 118 \nconvenience, we identify three key issues, and structure our survey around these (cf. Seel, 1989, \np.1 ), \n• Agent theories are essentially specifications. Agent theorists address such questions as: How are \nwe to conceptualise agents? What properties should agents have, and how are we to formally \nrepresent and reason about these properties? \n• Agent architectures represent the move from specification to implementation. Those working in \nthe area of agent architectures address such questions as: How are we to construct computer \nsystems that satisfy the properties specified by agent theorists? What software and/or hardware \nstructures are appropriate? What is an appropriated separation of concerns? \n• Agent languages are programming languages that may embody the various principles proposed \nby theorists. Those working in the area of agent languages address such questions as: How are \nwe to program agents? What are the right primitives for this task? How are we to effectively \ncompile or execute agent programs? \nAs we pointed out above, the distinctions between these three areas are occasionally unclear. The \nissue of agent theories is discussed in the section 2. In section 3, we discuss architectures, and in \nsection 4, we discuss agent languages. A brief discussion of applications appears in section 5, and \nsome concluding remarks appear in section 6. Each of the three major sections closes with a \ndiscussion, in which we give a brief critical review of current work and open problems, and a section \npointing the reader to further relevant reading. \nFinally, some notes on the scope and aims of the article. First, it is important to realise that we \nare writing very much from the point of view of AI, and the material we have chosen to review \nclearly reflects this bias. Secondly, the article is not a intended as a review of Distributed AI, \nalthough the material we discuss arguably falls under this banner. We have deliberately avoided \ndiscussing what might be called the macro aspects of agent technology (i.e., those issues relating to \nthe agent society, rather than the individual (Gasser, 1991), as these issues are reviewed more \nthoroughly elsewhere (see Bond and Gasser, 1988, pp. 1-56, and Chaibdraa et al., 1992). Thirdly, \nwe wish to reiterate that agent technology is, at the time of writing, one of the most active areas of \nresearch in AI and computer science generally. Thus, work on agent theories, architectures, and \nlanguages is very much ongoing. In particular, many ofthe fundamental problems associated with \nagent technology can by no means be regarded as solved. This article therefore represents only a \nsnapshot of past and current work in the field, along with some tentative comments on open \nproblems and suggestions for future work areas. Our hope is that the article will introduce the \nreader to some of the different ways that agency is treated in D(AI), and in particular to current \nthinking on the theory and practice of such agents. \n2 Agent theories \nIn the preceding section, we gave an informal overview of the notion of agency. In this section, we \nturn our attention to the theory of such agents, and in particular, to formal theories. We regard an \nagent theory as a specification for an agent; agent theorists develop formalisms for representing the \nproperties of agents, and using these formalisms, try to develop theories that capture desirable \nproperties of agents. Our starting point is the notion of an agent as an entity 'which appears to be \nthe subject of beliefs, desires, etc.' (Seel, 1989, p. 1). The philosopher Dennett has coined the term \nintentional system to denote such systems. \n2. 1 Agents as intentional system~ \nWhen explaining human activity, it is often useful to make statements such as the following: \nJanine took her umbrella because she believed it was going to rain. \nMichael worked hard because he wanted to possess a PhD. \n\nIntelligent agents: theory and practice 119 \nThese statements make use of a folk psychology, by which human behaviour is predicted and \nexplained through the attribution of attitudes, such as believing and wanting (as in the above \nexamples), hoping, fearing and so on. This folk psychology is well established: most people reading \nthe above statements would say they found their meaning entirely clear, and would not give them a \nsecond glance. \nThe attitudes employed in such folk psychological descriptions are called the intentional notions. \nThe philosopher Daniel Dennett has coined the term intentional system to describe entities "whose \nbehaviour can be predicted by the method of attributing belief, desires and rational acumen" \n(Dennett, 1987, p. 49). Dennett identifies different "grades" of intentional system: \n"A first-order intentional system has beliefs and desires (etc.) but no beliefs and desires (and no doubt other \nintentional states) about beliefs and desires .... A second-order intentional system is more sophisticated; it \nhas beliefs and desires (and no doubt other intentional states) about beliefs and desires (and other \nintentional state~)-both those of others and its own" (Dennett, 1987, p. 243) \nOne can carry on this hierarchy of intentionality as far as required. \nAn obvious question is whether it is legitimate or useful to attribute beliefs, desires, and so on, to \nartificial agents. Isn't this just anthropomorphism? McCarthy, among others, has argued that there \nare occasions when the intentional stance is appropriate: \n·To ascribe beliefs, free will, intentions, consciousness, abilities, or wants to a machine is legitimate when \nsuch an ascription expresses the same information about the machine that it expresses about a person. It is \nuseful when the ascription helps us understand the structure of the machine, its past or future behaviour, or \nhow to repair or improve it. It is perhaps never logically required even for humans, but expressing \nreasonably briefly what is actually known about the state of the machine in a particular situation may \nrequire mental qualities or qualities isomorphic to them. Theories of belief, knowledge and wanting can be \nconstructed for machines in a simpler setting than for humans, and later applied to humans. Ascription of \nmental qualities is most straightforward for machines of known structure such as thermostats and computer \noperating systems, but is most useful when applied to entities whose structure is incompletely known." \n(McCarthy, 1978) (quoted in (Shoham, 1990)) \nWhat objects can be described by the intentional stance? As it turns out, more or less anything can. \nIn his doctoral thesis, Seel showed that even very simple, automata-like objects can be consistently \nascribed intentional descriptions (Seel 1989); similar work by Rosenschein and Kaelbling (albeit \nwith a different motivation), arrived at a similar conclusion (Rosenschein & Kaelbling, 1986). For \nexample, consider a light switch: \n"It is perfectly coherent to treat a light switch as a (very cooperative) agent with the capability of \ntran~mitting current at will, who invariably transmits current when it believes that we want it transmitted \nand not otherwise; flicking the switch is simply our way of communicating our desires.'' (Shoham, 1990, p. \n6) \nAnd yet most adults would find such a description absurd-perhaps even infantile. Why is this? \nThe answer seems to be that while the intentional stance description is perfectly consistent with the \nobserved behaviour of a light switch, and is internally consistent, \n.. it does not buy 11.S anything, since we essentially understand the mechanism sufficiently to have a \nsimpler, mechanistic description of its behaviour." (Shoham, 1990, p. 6) \nPut crudely, the more we know about a system, the less we need to rely on animistic, intentional \nexplanations of its behaviour. However, with very complex systems, even if a complete, accurate \npicture of the system's architecture and working is available, a mechanistic, design stance \nexplanation of its behaviour may not be practicable. Consider a computer. Although we might \nhave a complete technical description of a computer available, it is hardly practicable to appeal to \nsuch a description when explaining why a menu appears when we click a mouse on an icon. In such \nsituations, it may be more appropriate to adopt an intentional stance description, if that description \nis consistent, and simpler than the alternatives. The intentional notions are thus abstraction tools, \nwhich provide us with a convenient and familiar way of describing, explaining, and predicting the \nbehaviour of complex systems. \n\nM. WOOLDRIDGE AND NICHOLAS JE~NINGS 120 \nBeing an intentional system seems to be a necessary condition for agenthood. but is it a sufficient \ncondition? In his Master's thesis, Shardlow trawled through the literature of cognitive science and \nits component disciplines in an attempt to find a unifying concept that underlies the notion of \nagenthood. He was forced to the following conclusion: \n"Perhaps there is something more to an agent than its capacity for beliefs and desires, hut whatever that \nthing is, it admits no unified account within cognitive science." (Shardlow, 1990) \nSo, an agent is a system that is most conveniently described by the intentional stance; one whose \nsimplest consistent description requires the intentional stance. Before proceeding, it is worth \nconsidering exactly which attitudes are appropriate for representing agents. For the purposes of \nthis survey, the two most important categories are information attitudes and pro-attitudes: \ninformation attitudes {\nbelief \nknowledge \npro-attitudes I\ndes;ce \nintention \nobligation \nl\ncommitment \nchoice \nThus information attitudes are related to the information that an agent has about the world it \noccupies, whereas pro-attitudes are those that in some way guide the agent's actions. Precisely \nwhich combination of attitudes is most appropriate to characterise an agent is, as we shall sec later, \nan issue of some debate. However, it seems reasonable to suggest that an agent must be \nrepresented in terms of at least one information attitude, and at least one pro-attitude. Note that \npro-and information attitudes are closely !inked, as a rational agent will make choices and form \nintentions, etc., on the basis of the information it has about the world. Much work in agent theory is \nconcerned with sorting out exactly what the relationship between the different attitudes is. \nThe next step is to investigate methods for representing and reasoning about intentional \nnotions. \n2.2 Representing intentional notions \nSuppose one wishes to reason about intentional notions in a logical framework. Consider the \nfollowing statement (after Genesereth & Nilsson, 1987, pp. 210-211): \nJanine believes Cronos is the father of Zeus. (1) \nA naive attempt to translate (1) into first-order logic might result in the following: \nBel(Janine, Father(Zeus, Cronos)) (2) \nUnfortunately, this naive translation does not work, for two reasons. The first is syntactic: the \nsecond argument to the Bel predicate is a formula of first-order logic, and is not, therefore, a term. \nSo (2) is not a well-formed formula of classical first-order logic. The second problem is semantic, \nand is potentially more serious. The constants Zeus and Jupiter, by any reasonable interpretation, \ndenote the same individual: the supreme deity of the classical world. It is therefore acceptable to \nwrite, in first-order logic: \n(Zeus= Jupiter). (3) \nGiven (2) and (3), the standard rules of first-order logic would allow the derivation of the following: \nBel(Janine, Father(Jupiter, Cronos)) (4) \nBut intuition rejects this derivation as invalid: believing that the father of Zeus is Cronos is not the \nsame as believing that the father of Jupiter is Cronos. So what is the problem? Why does first-order \n\nIntelligent agents: theory and practice 121 \nlogic fail here? The problem is that the intentional notions-such as belief and desire-are \nreferentially opaque, in that they set up opaque contexts, in which the standard substitution rules of \nfirst-order logic do not apply. In classical (propositional or first-order) logic, the denotation, or \nsemantic value, of an expression is dependent solely on the denotations of its sub-expressions. For \nexample, the denotation of the propositional logic formulap /\\ q is a function of the truth-values of \np and q. The operators of classical logic are thus said to be truth functional. In contrast, intentional \nnotions such as belief are not truth functional. It is surely not the case that the truth value of the \nsentence: \nJanine believes p (5) \nis dependent solely on the truth value of p 2 So substituting equivalents into opaque contexts is not \ngoing to preserve meaning. This is what is meant by referential opacity. Clearly, classical logics are \nnot suitable in their standard form for reasoning about intentional notions: alternative formalisms \nare required. \nThe number of basic techniques used for alternative formalisms is quite small. Recall, from the \ndiscussion above, that there arc two problems to be addressed in developing a logical formalism for \nintentional notions: a syntatic one, and a semantic one. It follows that any formalism can be \ncharacterised in terms of two independent attributes: its language of formulation, and semantic \nmodel (Konolige, 1986a, p. 83). \nThere are two fundamental approaches to the syntactic problem. The first is to use a modal \nlanguage, which contains non-truth-functional modal operators, which arc applied to formulae. An \nalternative approach involves the use of a meta-language: a many-sorted first-order language \ncontaining terms that denote formulae of some other object-language. Intentional notions can be \nrepresented using a meta-language predicate, and given whatever axiomatisation is deemed \nappropriate. Both of these approaches have their advantages and disadavantagcs, and will be \ndiscussed in the sequel. \nAs with the syntactic problem, there arc two basic approaches to the semantic problem. The \nfirst, best-known, and probably most widely used approach is to adopt a possible worlds semantics, \nwhere an agent's beliefs, knowledge, goals, and so on, arc characterised as a set of so-called \npossible worlds, with and accessibility relation holding between them. Possible worlds semantics \nhave an associated correspondence theory which makes them an attractive mathematical tool to \nwork with (Chellas, 1980). However, they also have many associated difficulties, notably the well­\nknown logical omniscience problem, which implies that agents are perfect reasoners (we discuss \nthis problem in more detail below). A number of variations on the possible-worlds theme have \nbeen proposed, in an attempt to retain the correspondence theory, but without logical omnis­\ncience. The commonest alternative to the possible worlds model for belief is to use a sentential, or \ninterpreted symbolic structures approach. In this scheme, beliefs are viewed as symbolic formulae \nexplicitly represented in a data structure associated with an agent. An agent then believes q) if i:p is \npresent in its belief data structure. Despite its simplicity, the sentential model works well under \ncertain circumstances (Konolige, 1986a). \nIn the subsections that follow, we discuss various approaches in some more detail. We begin \nwith a close look at the basic possible world:-, model for logics of knowledge (episremic logics) and \nlogics of belief (doxastic logics). \n2.3 Possible worlds semantics \nThe possible worlds model for logics of knowledge and belief was originally proposed by Hintikka \n(1962), and is now most commonly formulated in a normal modal logic using the techniques \n2Note, however, that the sentence (5) is itself a proposition, in that its denotation is the value true or false. \n\nM. WOOLDRIDGE AND NICHOLAS JENNINGS 122 \ndeveloped by Kripke (1963).3 Hintikka's insight was to see that an agent's beliefs could be \ncharacterised as a set of possible worlds, in the following way. Consider an agent playing a card \ngame such as poker.4 In this game, the more one knows about the cards possessed by one's \nopponents, the better one is able to play. And yet complete knowledge of an opponent's cards is \ngenerally impossible (if one excludes cheating). The ability to play poker well thus depends, at least \nin part, on the ability to deduce what cards are held by an opponent, given the limited information \navailable. Now suppose our agent possessed the ace of spades. Assuming the agent's sensory \nequipment was functioning normally, it would be rational of her to believe that she possessed this \ncard. Now suppose she were to try to deduce what cards were held by her opponents. This could be \ndone by first calculating all the various different ways that the cards in the pack could possibly have \nbeen distributed among the various players. (This is not being proposed as an actual card playing \nstrategy, but for illustration!) For argument's sake, suppose that each possible configuration is \ndescribed on a separate piece of paper. Once the process is complete, our agent can then begin to \nsystematically eliminate from this large pile of paper all those configurations which are not possible, \ngiven what she knows. For example, any configuration in which she did not possess the ace of spades \ncould be rejected immediately as impossible. Call each piece of paper remaining after this process a \nworld. Each world represents one state of affairs considered possible, given what she knows. \nHintikka coined the term epistemic alternatives to describe the worlds possible given one's beliefs. \nSomething true in all our agent"s epistemic alternatives could be said to be believed by the agent. \nFor example, it will be true in all our agent's epistemic alternatives that she has the ace of spades. \nOn a first reading, this seems a peculiarly roundabout way of characterising belief. but it has two \nadvantages. First, it remains neutral on the subject of the cognitive structure of agents. It certainly \ndoesn't posit any internalised collection of possible worlds. It is just a convenient way of \ncharacterising belief. Second, the mathematical theory associated with the formalisation of \npossible worlds is extremely appealing (see below). \nThe next step is to show how possible worlds may be incorporated into the semantic framework \nof a logic. Epistemic logics arc usually formulated as normal modal logics using the semantics \ndeveloped by Kripke (1963). Before moving on to explicitly epistemic logics, we consider a simple \nnormal modal logic. This logic is essentially classical propositional logic, extended by the addition \nof two operators:'· □" (necessarily), and"◊" (possibly). Let Prop= {p, q, ... } be a countable set \nof atomic propositions. Then the syntax of the logic is defined by the following rules: (i) if p e Prop \nthen pis a formula; (ii) if rp, 1.jJ are formulae, then so are -,rp and i:p V 1.jJ; and (iii) if rp i~ a formula \nthen so arc □qi and ◊ff. The operators "-," (not) and "V" ( or) have their standard meanings. The \nremaining connectives of classical propositional logic can be defined as abbreviations in the usual \nway. The formula □q, is read: "necessarily cp" and the formula ◊ff is read: "possibly cp". The \nsemantics of the modal connectives arc given by introducing an accessibility relation into models for \nthe language. This relation defines what worlds are considered accessible from every other world. \nThe formula □qi is then true if q; is true in every world accessible from the current world; ◊rp is true \nif rp is true in at least one world accessible from the current world. The two modal operators are \nduals of each other, in the sense that the universal and existential quantifiers of first-order logic arc \nduals: \nIt would thus have been possible to take either one as primitive, and introduce the other as a \nderived operator. The two basic properties of this logic arc as follows. First, the following axiom \nschema is valid: □(q-, = 1.jJ) = (Oq, -=O1.JJ). This axiom is called K, in honour of Kripkc. The second \nproperty is as follows: if <pis valid, then □q; is valid. Now, since K is valid, it will be a theorem of any \n3In Hintikka's original work. he used a technique based on "model sets·•. which is equivalent to Kripke"s \nformalism, though less elegant. See Hughes and Cresswell (1968, pp. 351-352) for a compari~on and \ndiscussion of the two techniques. \n4This example was adapted from Halpern (1987). \n\nIntelligent agents: theory and practice 123 \ncomplete axiomatisation of normal modal logic. Similarly, the second property will appear as a rule \nof inference in any axiomisation of normal modal logic; it is generally called the necessitation rule. \nThese two properties turn out to be the most problematic features of normal modal logics when \nthey are used as logics of knowledge/belief (this point will be examined later). \nThe most intriguing properties of normal modal logics follow from the properties of the \naccessibility relation, R, in models. To illustrate these properties, consider the following axiom \nschema: □<p =-<p. It turns out that this axiom is characteristic of the class of models with a reflexive \naccessibility relation. (By characteristic, we mean that it is true in all and only those models in the \nclass.) There are a host of axioms which correspond to certain properties of R: the study of the way \nthat properties of R correspond to axioms is called correspondence theory. For our present \npurposes, we identify just four axioms: the axiom called T (which corresponds to a reflexive \naccessibility relation); D (serial accessibility relation); 4 (transitive accessibility relation); and 5 \n(euclidean accessibility relation): \nT □q; => q; D □,p ~ ◊'P \n4 □q; => □□q; 5 ◊'P ~ □◊'P-\nThe results of correspondence theory make it straightforward to derive completeness results for a \nrange of simple normal modal logics. These results provide a useful point of comparison for normal \nmodal logics, and account in a large part for the popularity of this style of semantics. \nTo use the logic developed above as an epistemic logic, the formula □q: is read as: "it is known \nthat rp". The worlds in the model are interpreted as epistemic alternatives, the accessibility relation \ndefines what the alternatives arc from any given world. \nThe logic defined above deals with the knowledge of a single agent. To deal with multi-agent \nknowledge, one adds to a model structure an indexed set of accessibility rehitions, one for each \nagent. The language is then extended by replacing the single modal operator "O" by an indexed set \nof unary modal operators { K 1}, where i E { 1, ... , n }. The formula K;r:r is read: •'i knows that cp". \nEach operator K, is given exactly the same properties as·' □". \nThe next step is to consider how well normal modal logic serves as a logic of knowledge/belief. \nConsider first the necessitation rule and axiom K, since any normal modal system is committed to \nthese. The necessitation rule tells us that an agent knows all valid formulae. Amongst other things, \nthis means an agent knows all propositional tautologies. Since there is an infinite number of these, \nan agent will have an infinite number of items of knowledge: immediately, one is faced with a \ncounter-intuitive property of the knowledge operator. Now consider the axiom K, which says that \nan agent's knowledge is closed under implication. Together with the necessitation rule, this axiom \nimplies that an agent's knowledge is closed under logical consequence: ~n agent believes all the \nlogical consequences of its beliefs. This also seems counter intuitive. For example, suppose, like \nevery good logician, our agent knows Pcano's axioms. Now Fermat's last theorem follows from \nPean o's axioms-but it took the combined efforts of some of the best minds over the past century to \nprove it. Yet if our agent's beliefs are closed under logical consequence, then our agent must know \nit. So consequential closure, implied by necessitation and the K axiom, seems an overstrong \nproperty for resource bounded reasoners. \nThese two problems-that of knowing all valid formulae, and that of knowledge/belief being \nclosed under logical consequence-together constitute the famous logical omniscience problem. It \nhas been widely argued that this problem makes the possible worlds model unsuitable for \nrepresenting resource bounded believers-and any real system is resource bounded. \n2.3.1 Axioms for knowledge and belief \nWe now tonsider the appropriateness of the axioms D. T, 4, and 5 for logics of knowledge/ \nbelief. The axiom D says that an agent's beliefs are non-contradictory; it can be re-written as: \nK,cp => -.K, ..,<p, which is read: '•if i knows rp, then i doesn't know -irp'". This axiom seems a \nreasonable property of knowledge/belief. The axiom Tis often called the knowledge axiom, since it \nsays that what is known is true. It is usually accepted as the axiom that distinguishes knowledge \n\nM. WOOLDRIDGE AND NICHOLAS JENNINGS 124 \nfrom belief: it seems reasonable that one could believe something that is false, but one would \nhesitate to say that one could know something false. Knowledge is thus often defined as true belief; \ni knows cp if i believes <p and <pis true. So defined, knowledge satisfies T. Axiom 4 is called the \npositive introspection axiom. Introspection is the process of examining one's own beliefs, and is \ndiscussed in detail in (Konolige, 1986a, Chapter 5). The positive introspection axiom says that an \nagent is aware of what it knows. Similarly, axiom 5 is the negative introspective axiom, which says \nthat an agent is aware of what it doesn't know. Positive and negative introspection together imply \nan agent has perfect knowledge about what it does and doesn't know (cf. (Konolige, 1986a, \nEquation (5.11), p. 79)). Whether or not the two types of introspection are appropriate properties \nfor knowledge/belief is-the subject of sonie debate. However, it"is g·~nerally accepted that positive \nintrospection is a less demanding property than negative introspection, and is thus a more \nreasonable property for resource bounded reasoners. \nGiven the comments above, the axioms KTD45 are often chosen as a logic of (idealised) \nknowledge, and KD45 as a logic of (idealised) belief. \n2.4 Alternatives to the possible worlds model \nAs a result of the difficulties with logical omniscience, many researchers have attempted to develop \nalternative formalisms for representing belief. Some of these are attempts to adapt the basic \npossible worlds model; others represent significant departures from it. In the subsections that \nfollow, we examine some of these attempts. \n2.4.1 Levesque-belief and awareness \nIn a 1984 paper, Levesque proposed a solution to the logical omniscience problem that involves \nmaking a distinction between explicit and implicit belief (Levesque, 1984). Crudely, the idea is that \nan agent has a relatively small set of explicit beliefs, and a very much larger (infinite) set of implicit \nbeliefs, which includes the logical consequences of the explicit beliefs. To fonnalise this idea, \nLevesque developed a logic with two operators; one each for implicit and explicit belief. The \nsemantics of the explicit belief operator were given in terms of a weakened possible worlds \nsemantics, by borrowing some ideas from situation semantics (Barwise & Perry, 1983; Devlin, \n1991). The semantics of the implicit belief operator were given in terms of a standard possible \nworlds approach. A number of objections have been raised to Levesque's model (Reichgelt, 1989b. \np. 135): first, it does not allow quantification-this drawback has been rectified by Lakemeycr \n(1991); second, it docs not seem to allow for nested beliefs; third, the notion of a situation, which \nunderlies Levesque's logic is, if anything, more mysterious than the notion of a world in possible \nworlds; and fourth, under certain circumstances, Levesque's proposal still makes unrealistic \npredictions about agent's reasoning capabilities. \nIn an effort to recover from this last negative result, Fagin and Halpern have developed a "logic \nof general awareness" based on a similar idea to Levesque's but with a very much simpler semantics \n(Fagin & Hapern, 1985). However, this proposal has itself been criticised by some (Konolige, \n1986b). \n2.4.2 Konolige-the deduction model \nA more radical approach to modelling resource bounded believers was proposed by Konolige \n(Konolige, 1986a). His deduction model of belief is, in essence, a direct attempt to model the \n"beliefs" of :.ymbolic Al systems. Konolige observed that a typical knowledge-based system has \ntwo key components: a database of symbolically represented "beliefs" (which may take the form of \nrules. frames, semantic nets, or, more generally, formulae in some logical language), and some \nlogically incomplete inference mechanism. Konolige modelled such systems in terms of deduction \nstructures. A deduction structure is a pair d = (ii, p), where ~ is a base set of formulae in some \nlogical language, and pis a set of inference rules (which may be logically incomplete), representing \nthe agent's reasoning mechanism. To simplify the formalism, Konolige assumed that an agent \n\nIntelligent agents: theory and practice 125 \nwould apply its inference rules wherever possible, in order to generate the deductive closure of its \nbase beliefs under its deduction rules. We model deductive closure in a function close: \nwhere 6.1--,, rp means that rpcan be proved from 6. using only the rules in p. A belief logic can then be \ndefined, with the semantics to a modal belief connective [i], where i is an agent, given in terms of the \ndeduction structured; modelling i's belief system: [i]qi iff rp e c!ose(d;). \nKonolige went on to examine the properties of the deduction model at some length, and \ndeveloped a variety of proof methods for his logics, including resolution and tableau systems \n(Geissler & Konolige, 1986). The deduction model is undoubtedly simple; however, as a direct \nmodel of the belief systems of AI agents, it has much to commend it. \n2.4.3 Meta-languages and syntactic modalities \nA meta-language is one in which it is possible to represent the properties of another language. A \nfirst-order meta-language is a first-order logic, with the standard predicates, quantifier, terms, and \nso on, whose domain contains formulae of some other language, called the object language. Using a \nmeta-language, it is possible to represent a relationship between a meta-language term denoting an \nagent, and an object language term denoting some formula. For example, the meta-language \nformula Bel(Janine,[Father(Zeus, Cronos)]) might be used to represent the example (1) that we \nsaw earlier. The quote marks, [ ... ], are used to indicate that their contents are a meta-language \nterm denoting the corresponding object-language formula. \nUnfortunately, meta-language formalisms have their own package of problems, not the least of \nwhich is that they tend to fall prey to inconsistency (Montague, 1963; Thomason, 1980). However, \nthere have been some fairly successful meta-language formalisms, including those by Konolige \n(1982), Haas (1986), Morgenstern (1987), and Davies (1993). Some results on retrieving consist­\nency appeared in the late 1980s (Pcrlis, 1985, 1988; des Rivieres & Levesque, 1986; Turner, 1990). \n2.5 Pro-attitudes: goals and desires \nAn obvious approach to developing a logic of goals or desires is to adapt possible worlds \nsemantics-sec, e.g .. Cohen and Levesque (1990a), Wooldridge (1994). In this view, each goal­\naccessible world represents one way the world might be if the agent's goals were realised. However, \nthis approach falls prey to the side effect problem, in that it predicts that agents have a goal of the \nlogical consequences of their goals (cf. the logical omniscience problem, discussed above). This is \nnot a desirable property: one might have a goal of going to the dentist, with the necessary \nconsequence of suffering pain, without having a goal of suffering pain. The problem is discussed (in \nthe context of intentions), in Bratman ( 1990). The basic possible worlds model has been adapted by \nsome researchers in an attempt to overcome this problem (Wainer, 1994). Other, related semantics \nfor goals have been proposed (Doyle et al., 1991; Kiss & Reichgelt, 1992; Rao& Georgeff, 1991b). \n2.6 Theories of agency \nAll of the formalisms considered so far have focused on just one aspect of agency. However, it is to \nbe expected that a realistic agent theory will be represented in a logical framework that combines \nthese various components. Additionally, we expect an agent logic to be capable of representing the \ndynamic aspects of agency. A complete agent theory, expressed in a logic with these properties, \nmust define how the attributes of agency are related. For example, it will need to show how an \nagent's information and pro-attitudes are related; how an agent's cognitive state changes over time; \nhow the environment affects an agent's cognitive state; and how an agent's information and pro­\nattitudes lead it to perform actions. Giving a good account of these relationships is the most \nsignificant problem faced by agent theorists. \n\nM. WOOLDRIDGE AND NICHOLAS JENNINGS 126 \nAn all-embracing agent theory is some time off, and yet signficant steps have been taken towards \nit. In the following subsections, we briefly review some of this work. \n2.6. I Moore-knowledge and action \nMoore was in many ways a pioneer of the use of logics for capturing aspects of agency (Moore, \n1990). His main concern was the study of knowledge pre-conditions for actions-the question of \nwhat an agent needs to know in order to be able to perform some action. He formalised a model of \nability in a logic containing a modality for knowledge, and a dynamic logic-like apparatus for \nmodelling action (cf. Hare!, 1984). This formalism allowed for the possibility of an agent having \nincomplete information about how to achieve some goal, and performing actions in order to find \nout how to achieve it. Critiques of the formalism (and attempts to improve on it) may be found in \nMorgenstern (1987) and Lesperance (1989). \n2.6.2 Cohen and Levesque-intention \nOne of the best-known and most influential contributions to the area of agent theory is due to \nCohen and Levesque (1990a). Their formalism was originally used to develop a theory of intention \n(as in "I intend to ... "), which the authors required as a pre-requisite for a theory of speech acts \n(Cohen & Levesque, 1990b). However, the logic has subsequently proved to be so useful for \nreasoning about agents that it has been used in an analysis of conflict and cooperation in multi­\nagent dialogue (Galliers, 1988a,b), as well as several studies in the theoretical foundations of \ncooperative problem solving (Levesque ct al., 1990; Jennings, 1992; Castelfranchi, 1990; Castel­\nfranchi et al., 1992). Here, we shall review its use in developing a theory of intention. \nFollowing Bratman (1990), Cohen and Levesque identify seven properties that must be satisfied \nby a reasonable theory of intention: \n1. Intentions pose problems for agents, who need to determine ways of achieving them. \n2. Intentions provide a "filter" for adopting other intentions, which must not conflict. \n3. Agents track the success of their intentions, and arc inclined to try again if their attempts fail. \n4. Agents believe their intentions are possible. \n5. Agents do not believe they will not bring about their intentions. \n6. Under certain circumstances, agents believe they will bring about their intentions. \n7. Agents need not intend ail the expected side effects of their intentions. \nGiven these criteria, Cohen and Levesque adopt a two-tiered approach to the problem of \nformalising intention. First, they construct a logic of rational agency, "being careful to sort out the \nrelationships among the basic modal operators" (Cohen & Levesque, 1990a, p. 221). Over this \nframework, they introduce a number of derived constructs, which constitute a "partial theory of \nrational action" (Cohen & Levesque, 1990a, p. 221); intention is one of these constructs. \nThe first major derived construct is the persistent goal. An agent has a persistent goal of rp iff: \nl. It has a goal that q; eventually becomes true, and believes that rp is not currently true. \n2. Before it drops the goal cp, one of the following conditions must hold: i the agent believes cp has \nbeen satisfied; or ii the agent believes cp will never be satisfied. \nIt is a small step from persistent goals to a first definition of intention, as in '•intending to act'': an \nagent intends to do action a iff it has a persistent goal to have brought about a state wherein it \nbelieved it was about to do (1, and then did a. Cohen and Levesque go on to show how such a \ndefinition meets manyofBratman'scritcria for a theory of intention (outlined above). A critique of \nCohen and Levesque's theory of intention may be found in Singh (1992). \n2.6.3 Rao and Georgeff-belief, desire, intention architectures \nAs we observed earlier, there is no clear consensus in either the Al or philosophy communities \nabout precisely which combination of information and pro-attitudes are best suited to characteris­\ning rational agents. In the work of Cohen and Levesque, described above, just two basic attitudes \n\nIntelligent agents: theory and practice 127 \nwere used: beliefs and goals. Further attitudes, such as intention, were defined in terms of these. In \nrelated work, Rao and Georgeff have developed a logical framework for agent theory based on \nthree primitive modalities: beliefs, desires and intentions (Rao & Georgeff, 1991a,b, 1993). Their \nformalism is based on a branching model of time (cf. Emerson & Halpern, 1986), in which belief-, \ndesire-and intention-accessible worlds are themselves branching time structures. \nThey are particularly concerned with the notion of realism-the question of how an agent's \nbeliefs about the future affect its desires and intentions. In other work, they also consider the \npotential for adding (social) plans to their formalism (Rao & Georgcff, 1992b; Kinny et al., 1992). \n2.6.4 Singh \nA quite different approach to modelling agents was taken by Singh, who has developed an \ninteresting family of logics for representing intentions, beliefs, knowledge, know-how, and \ncommunication in a branching-time framework (Singh, 1990, 199la,b; Singh & Asher, 1991); these \narticles are collected and expanded in Singh (1994). Singh's formalism is extremely rich, and \nconsiderable effort has been devoted to establishing its properties. However, its complexity \nprevents a detailed discussion here. \n2.6.5 Werner \nIn an extensive sequence of papers, Werner has laid the foundations of a general model of agency, \nwhich draws upon work in economics, game theory, situated automata theory, situation semantics, \nand philosophy (Werner, 1988, 1989, 1990, 1991). At the time of writing, however, the properties \nof this model have not been investigated in depth. \n2.6.6 Wooldridge-modelling multi-agent systems \nFor his 1992 doctoral thesis, Wooldridge developed a family of logics for representing the \nproperties of multi-agent systems (Wooldridge, 1992; Wooldridge & Fisher, 1992). Unlike the \napproaches cited above, Wooldridge's aim was not to develop a general framework for agent \ntheory. Rather, he hoped to construct formalisms that might be used in the specification and \nverification of realistic multi-agent systems. To this end, he developed a simple, and in some sense \ngeneral, model of multi-agent systems, and showed how the histories traced out in the execution of \nsuch a system could be used as the semantic foundation for a family of both linear and branching \ntime temporal belief logics. He then gave examples of how these logics could be used in the \nspecification and verification of protocols for cooperative action. \n2. 7 Communication \nFormalisms for representing communication in agent theory have tended to be based on speech act \ntheory, as originated by Austin (1962), and further developed by Searle (1969) and others (Cohen \n& Perrault, 1979; Cohen & Levesque, 1990a). Briefly, the key axiom of speech act theory is that \ncommunicative utterances arc actions, in just the sense that physical actions arc. They are \nperformed by a speaker with the intention of bringing about a desired change in the world: \ntypically, the speaker intends to bring about some particular mental state in a listener. Speech acts \nmay fail in the same way that physical actions may fail: a listener generally has control over her \nmental state, and cannot be guaranteed to react in the way that the speaker intends, Much work in \nspeech act theory has been devoted to classifying the various different types of speech acts. Perhaps \nthe two most widely recognised categories of speech acts are representatives ( of which informing is \nthe paradigm example), and directives (of which requesting is the paradigm example). \nAlthough not directly based on work in speech acts (and arguably more to do with architectures \nthan theories), we shall here mention work on agent communication languages (Genesereth & \nKetchpel, 1994). The best known work on agent communication languages is that by the ARPA \nknowledge sharing effort (Patil et al,, 1992). This work has been largely devoted to developing two \nrelated languages: the knowledge query and manipulation language (KQML) and the knowledge \n\nM. WOOLDRIDGE AND NICHOLAS JENNINGS 128 \ninterchange format (KIF). KOML provides the agent designer with a standard syntax for \nmessages, and a number of performatives that define the force of a message. Example performa* \ntives include tell, perform, and reply; the inspiration for these message types comes largely from \nspeech act theory. KIF provides a syntax for message content-KIF is essentially the first-order \npredicate calculus, recast in a LISP-like syntax. \n2.8 Discussion \nFormalisms for reasoning about agents have come a long way since Hintikka's pioneering work on \nlogics of knowledge and belief (Hintikka, 1962). Within AI, perhaps the main emphasis of \nsubsequent work has been on attempting to develop formalisms that capture the relationship \nbetween the various elements that comprise an agent's cognitive state; the paradigm example of \nthis work is the well-known theory of intention developed by Cohen and Levesque (1990a). \nDespite the very real progress that has been made, there still remain many fairly fundamental \nproblems and issues still outstanding. \nOn a technical level, we can identify a number of issues that remain open. First, the problems \nassociated with possible worlds semantics (notably, logical omniscience) cannot be regarded as \nsolved. As we observed above, possible worlds remain the semantics of choice for many \nresearchers, and yet they do not in general represent a realistic model of agents with limited \nresources-and of course all real agents are resource-bounded. One solution is to ground possible \nworlds semantics, giving them a precise interpretation in terms of the world. This was the approach \ntaken in Rosenschein and Kaelbling's situated automata paradigm, and can be very successful. \nHowever, it is not clear how such a grounding could be given to proattitudes such as desires or \nintentions (although some attempts have been made (Singh, 1990a; Wooldridge, 1992; Werner, \n1990)). There is obviously much work remaining to be done on formalisms for knowledge and \nbelief, in particular in the area of modelling resource bounded reasoners. \nWith respect to logics that combine different attitudes, perhaps the most important problems \nstill outstanding relate to intention. In particular, the relationship between intention and action has \nnot been formally represented in a satisfactory way The problem seems to be that having an \nintention to act makes it more likely that an agent will act, but does not generally guarantee it. \nWhile it seems straightforward to build systems that appear to have intentions (Wooldridge, 1995), \nit seems much harder to capture this relationship formally. Other problems that have not yet really \nbeen addressed in the literature include the management of multiple, possibly conflicting \nintentions, and the formation, scheduling, and reconsideration of intentions. \nThe question of exactly which combination of attitudes is required to characterise an agent is \nalso the subject of some debate. As we observed above, a currently popular approach is to use a \ncombination of beliefs, desires, and intentions (hence BDI architectures (Rao and Georgeff, \n199lb)). However, there are alternatives: Shoham, for example, suggests that the notion of choice \nis more fundamental (Shoham, 1990). Comparatively little work has yet been done on formally \ncomparing the suitability of these various combinations. One might draw a parallel with the use of \ntemporal logics in mainstream computer science, where the expressiveness of specification \nlanguages is by now a well-understood research area (Emerson & Halpern, 1986). Perhaps the \nobvious requirement for the short term is experimentation with real agent specifications, in order \nto gain a better understanding of the relative merits of different formalisms. \nMore general!y, the kinds of logics used in agent theory tend to be rather elaborate, typically \ncontaining many modalities which interact with each other in subtle ways. Very little work has yet \nbeen carried out on the theory underlying such logics (perhaps the only notable exception is \nCatach, 1988). Until the general principles and limitations of such multi-modal logics become \nunderstood, we might expect that progress with using such logics will be slow. One area in which \nwork is likely to be done in the near future is theorem proving techniques for multi-modal logics. \nFinally, there is often some confusion about the role played by a theory of agency. The view we \ntake is that such theories represent specifications for agents. The advantage of treating agent \n\nIntelligent agents: theory and practice 129 \ntheories as specifications, and agent logics as specification languages, is that the problems and \nissues we then face are familiar from the discipline of software engineering: How useful or \nexpressive is the specification language? How concise are agent specifications? How does one \nrefine or otherwise transform a specification into an implementation? However, the view of agent \ntheories as specifications is not shared by all researchers. Some intend their agent theories to be \nused as knowledge representation formalisms, which raises the difficult problem of algorithms to \nreason with such theories. Still others intend their work to formalise a concept of interest in \ncognitive science or philosophy (this is, of course, what Hintikka intended in his early work on \nlogics of knowledge of belief). What is clear is that it is important to be precise about the role one \nexpects an agent theory to play. \n2. 9 Further reading \nFor a recent discussion on the role of logic and agency, which lays out in more detail some \ncontrasting views on the subject, see Israel (1993, pp. 17-24). For a detailed discussion of \nintentionality and the intentional stance, see Dennett (1978, 1987). A number of papers on AI \ntreatments of agency may be found in Allen et al. ( 1990). For an introduction to modal logic, sec \nChellas (1980); a slightly older, though more wide ranging introduction, may be found in Hughes \nand Cresswell (1968). As for the use of modal logics to model knowledge and belief, see Halpern \nand Moses (1992), which includes complexity results and proof procedures. Related work on \nmodelling knowledge has been done by the distributed systems community, who give the worlds in \npossible worlds semantics a precise interpretation; for an introduction and further references, see \nHalpern (1987) and Fagin et al. (1992). Overviews of formalisms for modelling belief and \nknowledge may be found in Halpern (1986), Konolige (1986a), Reichgelt (1989a) and Wooldridge \n(1992). A variant of the possible worlds framework, called the recursive modelling method, is \ndescribed in Gmytrasiewicz and Durfee (1993); a deep theory of belief may be found in Mack \n(1994). Situation semantics, developed in the early 1980s and recently the subject of renewed \ninterest, represent a fundamentally new approach to modelling the world and cognitive systems \n(Barwise & Perry, 1983; Devlin, 1991). However, situation semantics are not (yet) in the \nmainstream of (D)AJ, and it is not obvious what impact the paradigm will ultimately have. \nLogics which integrate time with mental states are discussed in Kraus and Lehmann (1988), \nHalpern and Vardi (1989) and Wooldridge and Fisher (1994); the last of these presents a tableau­\nbased proof method for a temporal belief logic. Two other important references for temporal \naspects are Shoham (1988. 1989). Thomas has developed some logics. for representing agent \ntheories as part of her framework for agent programming languages; see Thomas et al. (1991) and \nThomas (1993) and section 4. For an introduction to temporal logics and related topics, see \nGoldblatt (1987) and Emerson (1990). A non-formal discussion of intention may be found in \nBratman (1987), or more briefly (Bratman, 1990). Further work on modelling intention may be \nfound in Grosz and Sidner (1990), Sadek (1992), Goldman and Lang (1991), Konolige and Pollack \n(1993), Bell (1995) and Dongha (1995). Related work, focusing less on single-agent attitudes, and \nmore on social aspects, is Levesque et al. (1990), Jennings (1993a), Wooldridge (1994) and \nWooldridge and Jennings (1994). \nFinally, although we have not discussed formalisms for reasoning about action here, we \nsuggested above that an agent logic would need to incorporate some mechanism for representing \nagent's actions. Our reason for avoiding the topic is simply that the field is so big, it deserves a \nwhole review in its own right. Good starting points for AI treatments of action arc Allen (1984), \nand Allen et al. (1990, 1991). Other treatments of action in agent logics arc based on formalisms \nborrowed from mainstream computer science, notably dynamic logic (originally developed to \nreason about computer programs) (Hare!, 1984). The logic of seeing to it that has been discussed in \nthe formal philosophy literature, but has yet to impact on (D)AI (Belnap & Perloff, 1988; Perloff, \n1991; Belnap, 1991; Segerberg, 1989). \n\nM. WOOLDRIDGE A:-!D NICHOi.AS JENNINGS 130 \n3 Agent architectures \nUntil now, this article has been concerned with agent theory-the construction of formalisms for \nreasoning about agents, and the properties of agents expressed in such formalisms. Our aim in this \nsection is to shift the emphasis from theory to practice. We consider the issues surrounding the \nconstruction of computer systems that satisfy the properties specified by agent theorists. This is the \narea of agent architectures. Maes defines an agent architecture as: \n"(A] particular methodology for building [agents]. It specifies how ... the agent can be decomposed into \nthe construction of a set of component modules and how these modules should be made to interact. The \ntotal set of modules and their interactions has to provide an answer to the question of how the sensor data \nand the current internal state of the agent determine the actions ... and future internal state of the agent. \nAn architecture cncompa~ses techniques and algorithms that support this methodology'· (Maes, 1991, \np.115) \nKaelbling considers an agent architecture to be: \n"(A] specific collection of software (or hardware) modules, typically designated by boxes with arrows \nindicating the data and control flow among the modules. A more abstract view of an architecture is as a \ngeneral methodology for designing particular modular decompositions for particular tasks.,. (Kaelbling, \n1991, p.86) \nThe classical approach to building agents is to view them as a particular type of knowledge-based \nsystem. This paradigm is known as symbolic Al: we begin our review of architectures with a look at \nthis paradigm, and the assumptions that underpin it. \n3.1 Classical approaches: deliberative architectures \nThe foundation upon which the symbolic AI paradigm rests is the physical-symbol system \nhypothesis, formulated by Newell and Simon (1976). A physical symbol system is defined to be a \nphysically realisable set of physical entities (symbols) that can be combined to form structures, and \nwhich is capable of running processes that operate on those symbols according to symbolically \ncoded sets of instructions. The physical-symbol system hypothesis then says that such a system is \ncapable of general intelligent action. \nft is a short step from the notion of a physical symbol system to McCarthy's dream of a sentential \nprocessing automaton, or deliberative agent. (The term "deliberative agent" seems to have derived \nfrom Genesercth"s use of the the term "deliberate agent'' to mean a specific type of symbolic \narchitecture (Genesereth and Ntlsson, 1987, pp. 325-327).) We define a deliberative agent or agent \narchitecture to be one that contains an explicitly represented, symbolic model of the world, and in \nwhich decisions (for example about what actions to perform) arc made via logical (or at least \npseudo-logical) reasoning, based on pattern matching and symbolic manipulation. The idea of \ndeliberative agents based on purely logical reasoning is highly seductive: to get an agent to realise \nsome theory of agency one might naively suppose that it is enough to simply give it logical \nrepresentation of this theory and "get it to do a bit of theorem proving" (Shardlow. 1990, section \n3.2). If one aims to build an agent in this way, then there are at least two important problems to be \nsolved: \n1. The transduction problem: that of translating the real world into an accurate, adequate \nsymbolic description, in time for that description to be useful. \n2. The representation/reasoning problem: that of how to symbolically represent information \nabout complex real-world entities and processes, and how to get agents to reason with this \ninformation in time for the results to be useful. \nThe former problem has led to work on vision, speech understanding, learning. etc. The latter has \nled to work on knowledge representation, automated reasoning, automatic planning, etc. Despite \nthe immense volume of work that these problems have generated, most researchers would accept \nthat neither is anywhere near solved. Even seemingly trivial problems, such as commonsense \n\nIntelligent agents: theory and practice 131 \nreasoning, have turned out to be extremely difficult (cf. the CYC project (Guba & Lenat, 1994)). \nThe underlying problem seems to be the difficulty of theorem proving in even very simple logics, \nand the complexity of symbol manipulation algorithms in general: recall that first-order logic is not \neven decidable, and modal extensions to it (including representations of belief, desire, time, and so \non) tend to be highly undecidable. Thus, the idea of building "agents as theorem provers"-what \nmight be called an extreme logicist view of agency-although it is very attractive in theory, seems, \nfor the time being at least, to be unworkable in practice. Perhaps more troubling for symbolic AI is \nthat many symbol manuipulation algorithms of interest are intractable. lt seems hard to build \nuseful symbol manipulation algorithms that will he guaranteed to terminate with useful results in an \nacceptable fixed time bound. And yet such algorithms seem essential if agents are to operate in any \nreal-world. time-constrained domain. Good discussions of this point appear in Kaelbling (1986) \nand Russell and Wefald (1991). \nIt is because of these problems that some researchers have looked to alternative techniques for \nbuilding agents; such alternatives are discussed in section 3.2. First, however, we consider efforts \nmade within the symbolic Al community to construct agents. \n3.1.1 Planning agents \nSince the early 1970s, the AI planning community has been closely concerned with the design of \nartificial agents; in fact, it seems reasonable to claim that most innovations in agent design have \ncome from this community. Planning is essentially automaticprngamming: the design of aeourse of \naction that, when executed, will result in the achievement of some desired goal. Within the \nsymbolic AI community, it has long been assumed that some form of Al planning system will be a \ncentral component of any artificial agent. Perhaps the best-known early planning system was \nSTRIPS (Fikes & Nilsson, 1971). This system takes a symbolic description of both the world and a \ndesired goal state, and a set of action descriptions, which characterise the pre-and post-conditions \nassociated with various actions. It then attempts to find a sequence of actions that will achieve the \ngoal, by using a simple means-ends analysis. which essentially involves matching the post­\nconditions of actions against the desired goal. The STRIPS planning algorithm was very simple, \nand proved to be ineffective on problems of even moderate complexity. Much effort was \nsubsequently devoted to developing more effective techniques. Two major innovations were \nhierarchical and non-linear planning (Sacerdoti, 1974, 1975). However, in the mid 1980s, Chapman \nestablished some theoretical results which indicate that even such refined techniques will ultimately \nturn out to be unusable in any time-constrained system (Chapman, 1987). These results have had a \nprofound influence on subsequent AI planning research; perhaps more than any other, they have \ncaused some researchers to question the whole symbolic AI paradigm, and have thus led to the \nwork on alternative approaches that we discuss in section 3.2. \nIn spite of these difficulties, various attempts have been made to construct agents whose primary \ncomponent is a planner. For example: the Integrated Planning, Execution and Monitoring (IPEM) \nsystem is based on a sophisticated non-linear planner (Ambros-Ingerson and Steel, 1988); Wood"s \nAUTODRIVE system has planning agents operating in a highly dynamic environment (a traffic \nsimulation) (Wood, 1993); Etzioni has built "soft bots" that can plan and act in a Unix environment \n(Etzioni et al., 1994); and finally, Cohen's PHOENIX system includes planner-based agents that \noperate in the domain of simulated forest fire management (Cohen et al., 1989). \n3.1.2 Rratman, Israel and Pollack-IRMA \nIn section 2, we saw that some researchers have considered frameworks for agent theory based on \nbeliefs, desires, and intentions (Rao & Georgeff, 1991b). Some researchers have also developed \nagent architectures based on these attitudes. One example is the Intelligent Resource-hounded \nMachine Architecture (IRMA) (Bratman et a!., 1988). This architecture has four key symbolic data \nstructures: a plan library, and explicit representations of beliefs, desires, and intentions. Addition­\nally, the architecture has: a reasoner, for reasoning about the world; a means-end analyser, for \ndetermining which plans might be used to achieve the agent's intentions; an opportunity analyser, \n\nM. WOOLDRIDGE AND NICHOLAS JENNINGS 132 \nwhich monitors the environment in order to determine further options for the agent; a filtering \nprocess; and a deliberation process. The filtering process is responsible for determining the subset \nof the agent's potential courses of action that have the property of being consistent with the agent's \ncurrent intentions. The choice between competing options is made by the deliberation process. The \nIRMA architecture has been evaluated in an experimental scenario known as the Tileworld \n(Pollack & Ringuette, 1990). \n3.1.3 Vere and Bickmore-HOMER \nAn interesting experiment in the design of intelligent agents was conducted by Vere and Bickmore \n(1990). They argued that the enabling technologies for intelligent agents are sufficiently developed \nto be able to construct a prototype autonomous agent, with linguistic ability, planning and acting \ncapabilities, and so on. They developed such an agent, and christened it HOMER. This agent is a \nsimulated robot submarine, which exists in a two-dimensional "Seaworld'', about which it has only \npartial knowledge. HOMER takes instructions from a user in a limited subset of English with about \nan 800 word vocubulary; instructions can contain moderately sophisticated temporal references. \nHOMER can plan how to achieve its instructions (which typically relate to collecting and moving \nitems around the Seaworld), and can then execute its plans, modifying them as required during \nexecution. The agent has a limited episodic memory, and using this, is able to answer questions \nabout its past experiences. \n3.2.4 Jennings-GRATE* \nGRATE* is a layered architecture in which the behaviour of an agent is guided by the mental \nattitudes of beliefs, desires, intentions and joint intentions (Jennings, 1993b). Agents are divided \ninto two distinct parts: a domain level system and a cooperation and control layer. The former \nsolves problems for the organisation; be it in the domain of industrial control, finance or \ntransportation. The latter is a meta-level controller which operates on the domain level system with \nthe aim of ensuring that the agent's domain level activities are coordinated with those of others \nwithin the community. The cooperation layer is composed of three generic modules: a control \nmodule which interfaces to the domain level system, a situation assessment module and a \ncooperation module. The assessment and cooperation modules provide an implementation of a \nmodel of joint responsibility (Jennings, 1992), which specifics how agents should act both locally \nand towards other agents whilst engaged in cooperative problem solving. The performance of a \nGRATE* community has been evaluated against agents which only have individual intentions, and \nagents which behave in a selfish manner, in the domain of electricity transportation management. \nA significant improvement was noted when the situation became complex and dynamic (Jennings, \n1995). \n3.2 Alternative approaches: reactive architectures \nAs we observed above, there arc many unsolved (some would say insoluble) problems associated \nwith symbolic Al. These problems have led some researchers to question the viability of the whole \nparadigm, and to the development of what are generally known as reactive architectures. For our \npurposes, we shall define a reactive architecture to be one that does not include any kind of central \nsymbolic world model, and does not use complex symbolic reasoning. \n3.2. l Brooks-behaviour languages \nPossibly the most vocal critic of the symbolic AI notion of agency has been Rodney Brooks, a \nresearcher at MIT who apparently became frustrated by AI approaches to building control \nmechanisms for autonomous mobile robots. In a 1985 paper, he outlined an alternative architec­\nture for building agents, the so called subsumption architecture (Brooks, 1986). The review of \nalternative approaches begins with Brooks' work. \nIn recent papers, Brooks (1990, 1991a,b) has propounded three key theses: \n\nIntelligent agents: theory and practice 133 \n1. Intelligent behaviour can be generated without explicit representations of the kind that symbolic \nAI proposes. \n2. Intelligent behaviour can be generated without explicit abstract reasoning of the kind that \nsymbolic AI proposes. \n3. Intelligence is an emergent property of certain complex systems. \nBrooks identifies two key ideas that have informed his research: \n1. Situatedness and embodiment: "Real" intelligence is situated in the world, not in disembodied \nsystems such as theorem provers or expert systems. \n2. Intelligence and emergence: "Intelligent" behaviour arises as a result of an agent's interaction \nwith its environment. Also, intelligence is "in the eye of the beholder"; it is not an innate, \nisolated property. \nIf Brooks was just a Dreyfus-style critic of AI, his ideas might not have gained much currency. \nHowever, to demonstrate his claims, he has built a number of robots, based on the suhsumption \narchitecture. A subsumption architecture is a hierarchy of task-accomplishing behaviours. Each \nbehaviour "competes" with others to exercise control over the robot. Lower layers represent more \nprimitive kinds of behaviour (such as avoiding obstacles), and have precedence over layers further \nup the hierarchy. It should be stressed that the resulting systems are, in terms of the amount of \ncomputation they need to do, extremely simple, with no explicit reasoning of the kind found in \nsymbolic AI systems. But despite this simplicity, Brooks has demonstrated the robots doing tasks \nthat would be impressive if they were accomplished by symbolic AI systems. Similar work has been \nreported by Steels, who described simulations of "Mars explorer" systems, containing a large \nnumber of subsumption-architecture agents, that can achieve near-optimal performance in certain \ntasks (Steels, 1990). \n3.2.2 Agre and Chapman-PENG! \nAt about the same time as Brooks was describing his first results with the subsumption architecture, \nChapman was completing his Master's thesis, in which he reported the theoretical difficulties with \nplanning described above, and was coming to similar conclusions about the inadequacies of the \nsymbolic AI model himself. Together with his co-worker Agre, he began to explore alternatives to \nthe AI planning paradigm (Chapman & Agre, 1986). \nAgre observed that most everyday activity is ··routine" in the sense that it requires little-if \nany-new abstract reasoning. Most tasks, once learned, can be accomplished in a routine way, with \nlittle variation. Agre proposed that an efficient agent architecture could be based on the idea of \n''running arguments". Crudely, the idea is that as most decisions are routine, they can be encoded \ninto a low-level structure (such as a digital circuit), which only needs periodic updating, perhaps to \nhandle new kinds of problems. His approach was illustrated with the celebrated PENGI system \n(Agre & Chapman, 1987). PENGI is a simulated computer game, with the central character \ncontrolled using a scheme such as that outlined above. \n3.2.3 Rosenschein and Kaelhling-situated automata \nAnother sophisticated approach is that of Rosenschein and Kaclbling (Rosenschein, 1985; \nRosenschein & Kaelbling, 1986; Kaelbling & Rosenschcin, 1990; Kaelbling, 1991). In their situated \nautomata paradigm, an agent is specified in declarative terms. This specification is then compiled \ndown to a digital machine, which satisfies the declarative specification. This digital machine can \noperate in a provably time-bounded fashion; it does not do any symbol manipulation, and in fact no \nsymbolic expressions arc represented in the machine at all. The logic used to specify an agent is \nessentially a modal logic of knowledge (see above). The technique depends upon the possibility of \ngiving the worlds in possible worlds semantics a concrete interpretation in terms of the states of an \nautomaton: \n\nM. WOOLDRIDGE AND NICHOLAS JENNINGS 134 \n"[An agent} ... xis said to carry the information that pin world states, written s I= K(x,p), if for all world \nstates in which x has the same value as it does ins, the proposition pis true." (Kae!bling & Rosenschcin, \n1990, p. 36) \nAn agent is specified in terms of two components: perception and action. Two programs are then \nused to synthesise agents: RULER is used to specify the perception component of an agent; \nGAPPS is used to specify the action component. \nRULER takes as its input three components: \n"f A j specification of the semantics of the [ agent's} inputs ("whenever bit 1 is on, it is raining"); a set of static \nfacts ("whenever it is raining, the ground is wet"); and a specification of the state transitions of the world ("if \nthe ground is wet, it stays wet until the sun comes out"). The programmer then specifies the desired \nsemantics for the output ("if this bit is on, the ground is wet"), and the compiler ... [synthesises] a circuit \nwhose output will have the correct semantics .... All that declarative '"knowledge" has been reduced to a \nvery simple circuit." (Kaelb!ing, 1991, p. 86) \nThe GAPPS program takes as its input a set of goal reduction rules (essentially rules that encode \ninformation about how goals can be achieved), and a top level goal, and generates a program that \ncan be translated into a digital circuit to realise the goal. Once again, the generated circuit does not \nrepresent or manipulate symbolic expressions; all symbolic manipulation is done at compile time. \nThe situated automata paradigm has attracted much interest, as it appears to combine the best \nelements of both reactive and symbolic, declarative systems. However, at the time of writing, the \ntheoretical limitations of the approach are not well understood; there are similarities with the \nautomatic synthesis of programs from temporal logic specifications, a complex area of much \nongoing work in mainstream computer science (see the comments in Emerson, 1990). \n3.2.4 Maes-Agent network architecture \nPattie Maes has developed an agent architecture in which an agent is defined as a set of competence \nmodules (Macs, 1989, 1990b, 1991 ). These modules loosely resemble the behaviours of Brooks' \nsubsumption architecture (above). Each module is specified by the designer in terms of pre-and \npost-conditions (rather like STRIPS operators), and an activation level, which gives a real-valued \nindication of the relevance of the module in a particular situation. The higher the activation level of \na module, the more likely it is that this module will influence the behaviour of the agent. Once \nspecified, a set of competence modules is compiled into a spreading activation network, in which the \nmodules are linked to one-another in ways defined by their pre-and post-conditions. For example, \nif module a has post-condition rp, and module b has pre-condition cp, then a and bare connected by \na successor link. Other types of link include predecessor links and conflicter links. When an agent is \nexecuting, various modules may become more active in given situations, and may be executed. The \nresult of execution may be a command to an effector unit, or perhaps the increase in activation !eve! \nof a successor module. \nThere are obvious similarities between the agent network architecture and neural network \narchitectures. Perhaps the key difference is that it is difficult to say what the meaning of a node in a \nneural net is; it only has a meaning in the context of the net itself. Since competence modules are \ndefined in declarative terms, however, it is very much easier to say what their meaning is. \n3.3 Hybrid architectures \nMany researchers have suggested that neither a completely deliberative nor completely reactive \napproach is suitable for building agents. They have argued the case for hybrid systems, which \nattempt to marry classical and alternative approaches. \nAn obvious approach is to build an agent out of two (or more) subsystems: a deliberative one, \ncontaining a symbolic world model, which develops plans and makes decisions in the way proposed \nby mainstream symbolic AI; and a reactive one, which is capable of reacting to events that occur in \nthe environment without engaging in complex reasoning. Often, the reactive component is given \n\nIntelligent agents: theory and practice 135 \nsome kind of precedence over the deliberative one, so that it can provide a rapid response to \nimportant environmental events. This kind of structuring leads naturally to the idea of a layered \narchitecture, of which TouringMachincs (Ferguson, 1992) and InteRRaP (Muller & Pischel, 1994) \nare good examples. (These architectures are described below.) In such an architecture, an agent's \ncontrol subsystems are arranged into a hierarchy, with higher layers dealing with information at \nincreasing levels of abstraction. Thus, for example, the very lowest layer might map raw sensor \ndata directly onto effector outputs, while the uppermost layer deals with long-term goals. A key \nproblem in such architectures is what kind of control framework to embed the agent's subsystems \nin, to manage the interactions between the various layers. \n3.3.1 Georgeff and Lansky-PRS \nOne of the best-known agent architectures is the Procedural Reasoning System (PRS), developed \nby Georgeff and Lansky (1987). Like IRMA (see above), the PRS is a belief-desire-intention \narchitecture, which includes a plan library, as well as explicit symbolic representations of beliefs, \ndesires, and intentions. Beliefs are facts, either about the external world or the system's internal \nstate. These facts are expressed in classical first-order logic. Desires are represented as system \nbehaviours (rather than as static representations of goal states). A PRSplan library contains a set of \npartially-elaborated plans, called knowledge areas (KA), each of which is associated with an \nini,ocation condition. This condition determines when the KA is to be actil'ated. KAs may be \nactivated in a goal-driven or data-driven fashion; KAs may also be reactive, allowing the PRS to \nrespond rapidly to changes in its environment. The set of currently active KAs in a system represent \nits intentions. These various data structures are manipulated by a system interpreter, which is \nresponsible for updating beliefs, invoking KAs, and executing actions. The PRS has been \nevaluated in a simulation of maintenance procedures for the space shuttle, as well as other domains \n(Georgcff & lngrand, 1989). \n3.3.2 Ferguson-Touring Machines \nFor his 1992 Doctoral thesis, Ferguson developed the Touring Machines hybrid agent architecture \n(Ferguson, 1992a,b).5 The architecture consists of perception and action subsystems, which \ninterface directly with the agent's environment, and three control layers, embedded in a control \nframework, which mediates between the layers. Each layer is an independent, activity-producing, \nconcurrently executing process. \nThe reactive layer generates potential courses of action in response to events that happen too \nquickly for other layers to deal with. It is implemented as a set of situation-action rules, in the style \nof Brooks' subsumption architecture (see above). \nThe planning layer constructs plans and selects actions to execute in order to achieve the agent's \ngoals. This layer consists of two components: a planner, and a focus of attention mechanism. The \nplanner integrates plan generation and execution. and uses a library of partially elaborated plans, \ntogether with a topological world map, in order to construct plans that will accomplish the agent's \nmain goal. The purpose of the focus of attention mechanism is to limit the amount of information \nthat the planner must deal with, and so improve its efficiency. It does this by filtering out irrelevant \ninformation from the environment. \nThe modelling layer contains symbolic representations of the cognitive state of other entities in \nthe agent's environment. These models are manipulated in order to identify and resolve goal \nconflicts-situations where an agent can no longer achieve its goals, as a result of unexpected \ninterference. \nThe three layers are able to communicate with each other (via message passing), and are \nembedded in a control framework. The purpose of this framework is to mediate between the \n51t i~ worth noting that Fcrgu~on's thesis gives a good overview of the problems and issues associated with \nbuilding rational, resource-bounded agents. Moreover. the description given of the TouringMachines \narchitecture is itself extremely clear. We recommend it as a point of departure for further reading. \n\nM. WOOLDRIDGE AND '.'JICHOLAS JENNINGS 136 \nlayers, and in particular, to deal with conflicting action proposals from the different layers. The \ncontrol framework does this by using control rules. \n3.3.3 BurmeLHer et al.-COSY \nThe COSY architecture is a hybrid BDI-architecture that includes elements of both the PRS and \nIRMA, and was developed specifically for a multi-agent testbed called DASEDIS (Burmeister & \nSundermeyer; Haddadi, 1994). The architecture has five main components: (i) sensors; (ii) \nactuators; (iii) communications (iv) cognition; and (v) intention. The first three components are \nstraightforward: the sensors receive non-communicative perceptual input, the actuators allow the \nagent to perform non-communicative actions, and the communications component allows the \nagent to send messages. Of the remaining two components, the intention component contains \n"long-term goals, attitudes, responsibilities and the like ... the control elements taking part in the \nreasoning and decision-making of the cognition component" (Haddadi, 1994, p. 15), and the \ncognition component is responsible for mediating between the intentions of the agent and its beliefs \nabout the world, and choosing an appropriate action to perform. Within the cognition component \nis the knowledge base containing the agent's beliefs, and three procedural components: a script \nexecution component, a protocol execution component, and a reasoning, deciding and reacting \ncomponent. A script is very much like a script in Schan k's original sense: it is a stereotypical recipe \nor plan for achieving a goal. Protocols are stereotypical dialogues representing cooperation \nframeworks such as the contract net (Smith, 1980). The reasoning, deciding and reacting \ncomponent is perhaps the key component in COSY. It is made up of a number of other subsystems, \nand is structured rather like the PRS and IRMA (see above). An agenda is maintained, that \ncontains a number of active scripts. These scripts may be invoked in a goal-driven fashion (to satisfy \none of the agent's intentions), or a data-driven fashion (in response to the agent's current \nsituation). A filter component chooses between competing scripts for exerntion. \n3.3.4 MUiler et al.~lnteRRaP \nIntcRRaP, like Ferguson's TouringMachines, is a layered architecture, with each successive layer \nrepresenting a higher level of abstraction than the one below it (Millier & Pischel, 1994; Millier et \nal., 1995; Millier, 1994). In InteRRaP, these layers are further subdivided into two vertical layers: \none containing layers of knowledge bases, the other containing various control components, that \ninteract with the knowledge bases at their level. At the lowest level is the world interface control \ncomponent, and the corresponding world level knowledge base. The world interface component, \nas its name suggests, manages the interface between the agent and its environment, and thus deals \nwith acting, communicating, and perception. \nAbove the world interface component is the behaviour-based component. The purpose of this \ncomponent is to implement and control the basic reactive capability of the agent. This component \nmanipulates a set of patterns of behaviour (PoB). A PoB is a structure containing a pre-condition \nthat defines when the PoB is to be activated, various conditions that define the circumstances under \nwhich the PoB is considered to have succeeded or failed, a post-condition (Ula STRIPS (Fikes & \nNilsson, 1971)), and an executable body, that defines what action should be performed if the PoB is \nexecuted. (The action may be a primitive, resulting in a call on the agent's world interface. or may \ninvolve calling on a higher-level layer to generate a plan.) \nAbove the behaviour-based component in TnteRRaP is the plan-based component. This \ncomponent contains a planner that is able to generate single-agent plans in response to requests \nfrom the behaviour-based component. The knowledge-base at this layer contains a set of plans, \nincluding a plan library. The highest layer of InteRRaP is the cooperation component. This \ncomponent is able to generate joint plans, that satisfy the goals of a number of agents, by \nelaborating plans selected from a plan library. These plans arc generated in response to requests \nfrom the plan-based component. \nControl in lnteRRaP is both data-and goal-driven. Perceptual input is managed by the world­\ninterface, and typically results in a change to the world model. As a result of changes to the world \n\nIntelligent agents: theory and practice 137 \nmodel, various patterns of behaviour may be activated, dropped, or executed. As a result of PoB \nexecution, the plan-based <;omponent and cooperation component may be asked to generate plans \nand joint plans respectively, in order to achieve the goals of the agent. This ultimately results in \nprimitive actions and messages being generated by the world interface. \n3.4 Discussion \nThe deliberative, symbolic paradigm is, at the time of writing, the dominant approach in (D)AL \nThis state of affairs is likely to continue, at least for the near future. There seem to be several \nreasons for this. Perhaps most importantly, many symbolic AI techniques (such as rule-based \nsystems) carry with them an associated technology and methodology that is becoming familiar to \nmainstream computer scientists and software engineers. Despite the well-documented problems \nwith symbolic AI systems, this makes symbolic AI agents (such as GRATE*, Jennings, 1993b) an \nattractive proposition when compared to reactive systems, which have as yet no associated \nmethodology. The need for a development methodology seems to be one of the most pressing \nrequirements for reactive systems. Anecdotal descriptions of current reactive systems implemen­\ntations indicate that each such system must be individually hand-crafted through a potentially \nlengthy period of experimentation (Wavish and Graham, 1995). This kind of approach seems \nunlikely to be usable for large systems. Some researchers have suggested that techniques from the \ndomain of genetic algorithms or machine learning might be used to get around these development \nproblems, though this work is at a very early stage. \nThere is a pressing need for research into the capabilities of reactive systems, and perhaps in \nparticular to the types of application for which these types of system are best suited; some \npreliminary work has been done in this area, using a problem domain known as the Tile World \n(Pollack & Ringuette, 1990) With respect to reactive systems, Ferguson suggests that: \n"jT]he strength of purely non-deliberative architectures lies in their ability to exploit local patterns of \nactivity in their current surroundings in order to generate more or less hardwired action responses .. for a \ngiven set of stimuli Successful operation using this method pre-supposes: i that the complete set of \nenvironmental stimuli required for unambiguously determining action sequences is always present and \nreadily identifiable-in other words, that the agent's activity can be sttuationally determined; ii that the \nagent has no global task constraints ... which need to be reasoned about at run time; and iii that the agent's \ngoal or desire system is capable of being represented implicitly in the agent's structure according to a fixed, \npre-compiled ranking scheme." (Ferguson. 1992a, pp. 29-30} \nHybrid architectures, such as the PRS, TouringMachines, InteRRaP, and COSY, are currently a \nvery active area of work, and arguably have some advantages over both purely deliberative and \npurely reactive architectures. However, an outstanding problem with such architectures is that of \ncombining multiple interacting subsystems (deliberative and reactive) cleanly, in a well-motivated \ncontrol framework. Humans seem to manage different levels of abstract behaviour with compari­\ntive ease; it is not clear that current hybrid architectures can do so. \nAnother area where as yet very little work has been done is the generation of goals and \nintentions. Most work in AT assumes that an agent has a single, well-defined goal that it must \nachieve. But if agents are ever to be really autonomous, and act pro-actively, then they must be \nable to generate their own goals when either the situation demands, or the opportunity arises. \nSome preliminary work in this area is Norman and Long (1995). Similarly, little work has yet been \ndone into the management and scheduling of multiple, possibly conflicting goals; some preliminary \nwork is reported in Dongha (1995). \nFinally, we turn to the relationship between agent theories and agent architectures. To what \nextent do the agent architectures reviewed above correspond to the theories discussed in section 2? \nWhat, if any, is the theory that underpins an architecture? With respect to purely deliberative \narchitectures, there is a wealth of underlying theory. The close relationship between symbolic \nprocessing systems and mathematical logic means that the semantics of such architectures can often \nbe represented as a logical system of some kind. There is a wealth of work establishing such \n\nM. WOOLORrDGE A.ND NICHOLAS JENNINGS 138 \nrelationships in Al, of which a particularly relevant example is Rao and Georgeff (1992a). This \narticle discusses the relationship between the abstract BDI logics developed by Rao et al. for \nreasoning about agents, and an abstract "agent interpreter", based on the PRS. However, the \nrelationship between the logic and the architecture is not formalised; the BDI logic is not used to \ngive a formal semantics to the architecture, and in fact it is difficult to see how such a logic could he \nused for this purpose. A serious attempt to define the semantics of a (somewhat simple) agent \narchitecture is presented in Wooldridge (1995), where a formal model of the system MyWorld, in \nwhich agents are directly programmed in terms of beliefs and intentions, is used as the basis upon \nwhich to develop a logic for reasoning about MyWorld systems. Although the logic contains \nmodalities for representing beliefs and intentions, the semantics of these modalities are given in \nterms of the agent architecture itself, and the problems associated with possible worlds do not, \ntherefore, arise; this work builds closely on Konolige's models of the beliefs of symbolic AI systems \n(Konoligc, 1986a). However, more work needs to be done using this technique to model more \ncomplex architectures, before the limitations and advantages of the approach are well-understood. \nLike purely deliberative architectures, some reactive systems are also underpinned by a \nrelatively transparent theory. Perhaps the best example is the situated automata paradigm, where \nan agent is specified in terms of a logic of knowledge, and this specification is compiled down to a \nsimple digital machine that can he realistically said to realise its corresponding specification. \nHowever, for other purely reactive architectures, based on more ad hoc principles, it is not clear \nthat there is any transparent underlying theory. It could be argued that hybrid systems also tend to \nbead hoc, in that while their structures are well-motivated from a design point of view, it is not clear \nhow one might reason about them, or what their underlying theory is. In particular, architectures \nthat contain a number of independent activity producing subsystems, which compete with each \nother in real time to control the agent's activities, seem to defy attempts at formalisation. It is a \nmatter of debate whether this needs he considered a serious disadvantage, but one argument is that \nunle~s we have a good theoretical model of a particular agent or agent architecture, then we shall \nnever really understand why it works. This is likely to make it difficult to generalise and reproduce \nresults in varying domains. \n3.5 Further reading \nMost introductory textbooks on Al discuss the physical symbol system hypothesis; a good recent \nexample of such a text is Ginsberg (1993). A detailed discussion of the way that this hypothesis has \naffected thinking in symbolic AI is provided in Shardlow (1990). There are many objections to the \nsymbolic AI paradigm, in addition to those we have outlined above. Again, introductory textbooks \nprovide the stock criticisms and replies. \nThere is a wealth of material on planning and planning agents. See Georgeff (1987) for an \noverview of the state of the art in planning (as it was in 1987), Allen et al. (1990) for a thorough \ncollection of papers on planning (many of the papers cited above are included), and Wilkins (1988) \nfor a detailed description of SIPE, a sophisticated planning system used in a real-world application \n(the control of a brewery!) Another important collection of planning papers is Georgeff and \nLansky (1986). The book by Dean and Wellman and the book by Allen et al. contain much useful \nrelated material (Dean and Wellman, 1991; Allen et al., 1991 ). There is now a regular international \nconference on planning; the proceedings of the first were published as Hendler (1992). \nThe collection of papers edited by Maes (1990a) contains many interesting papers on alterna­\ntives to the symbolic AI paradigm. Kaelbling (1986) presents a clear discussion of the issues \nassociated with developing resource-bounded rational agents, and proposes an agent architecture \nsomewhat similar to that developed by Brooks. A proposal by Nilsson for teleo reactive programs­\ngoal directed programs that nevertheless respond to their environment-is described in Nilsson \n(1992). The proposal draws heavily on the situated automata paradigm; other work based on this \nparadigm is described in Shoham (1990) and Kiss and Reichgelt (1992). Schoppcrs has proposed \ncompiling plans in advance, using traditional planning techniques, in order to develop universal \n\nIntelligent agents: theory and practice 139 \nplans, which are essentially decision trees that can be used to efficiently determine an appropriate \naction in any situation (Schoppers, 1987). Another proposal for building "reactive planners" \ninvolves the use of reactive action packages (Firby, 1987). \nOther hybrid architectures are described in Hayes-Roth (1990), Downs and Reichgelt (1991), \nAylett and Eustace (1994) and Bussmann and Demazeau (1994). \n4 Agent languages \nAs agent technology becomes more established, we might expect to see a variety of software tools \nbecome available for the design and construction of agent-based systems; the need for software \nsupport tools in this area was identified as long ago as the.mid-1980s (Gasser et al., 1987). The \nemergence of a number of prototypical agent languages is one sign that agent technol0gy is \nbecoming more widely used, and that many more agent-based applications are likely to be \ndeveloped in the near future. By an agent language, we mean a system that allows one to program \nhardware or software computer systems in terms of some of the concepts developed hy agent \ntheorists. At the very least, we expect such a language to include some structure corresponding to \nan agent. However, we might also expect to sec some other attributes of agency (beliefs, goals, or \nother mentalistic notion~) used to program agents. Some of the languages we consider below \nembody this strong notion of agency; others do not. However, all have properties that make them \ninteresting from the point of view of this review. \n4.0. I Concurrent object languages \nConcurrent object languages are in many respects the ancestors of agent languages. The notion of a \nself-contained concurrently executing object, with some internal state that is not directly accessible \nto the outside world, responding to messages from other such objects, is very close to the concept of \nan agent as we have defined it. The earliest concurrent object framework was Hewitt's Actor model \n(Hewitt, 1977; Agha, 1986); another well-known example is the ABCL system (Yonezawa, 1990). \nFor a discussion on the relationship between agents and concurrent object programming, see \nGasser and Briot (1992). \n4.0.2 Shoham-agent-oriented programming \nYoav Shoham has proposed a "new programming paradigm, based on a societal view of \ncomputation" (Shoham, 1990, p. 4; 1993). The key idea that informs this agent-oriented program­\nming (AOP) paradigm is that of directly programming agents in terms of the mentalistic, \nintentional notions that agent theorists have developed to represent the properties of agents. The \nmotivation behind such a proposal is that, as we observed in section 2, humans use the intentional \nstance as an abstraction mechanism for representing the properties of complex systems. ln the same \nway that we use the intentional stance to describe humans, it might be useful to use the intentional \nstance to program machines. \nShoham proposes that a fully developed AOP system will have three components: \n• a logical system for defining the mental state of agents; \n• an interpreted programming language for programming agents; \n• an "agentification" process, for compiling agent programs into low-level executable systems. \nAt the time of writing, Shoham has only published results on the first two components. (In Shoham \n(1990, p. 12), he wrote that "the third is still somewhat mysterious to me", though later in the paper \nhe indicated that he was thinking along the lines of Rosenschcin and Kaelbling·s situated automata \nparadigm (Rosenschcin & and Kaelbling, 1986).) Shoham·s first attempt at an AOP language was \nthe AGENT0 system. The logical component of this system is a quantified multi-modal logic, \nallowing direct reference to time. No semantics are given, but the logic appears to be based on \nThomas et al. (1991). The logic contains three modalities: belief, commitment and ability. The \nfollowing is an acceptable formula of the logic, illustrating its key properties: \n\nM. WOOLDRIDGE AND NICHOLAS JENNINGS 140 \nCA~ open(door)8 = Bl CA~ open (door)8. \nThis formula is read: "if at time 5 agent a can ensure that the door is open at time 8, then at time 5 \nagent b believes that at time 5 agent a can ensure that the door is open at time 8". \nCorresponding to the logic is the AGENT0 programming language. In this language, an agent is \nspecified in terms of a set of capabilities (things the agent can do), a set of initial beliefs and \ncommitments, and a set of commitment rules. The key component, which determines how the agent \nacts, is the commitment rule set. Each commitment rule contains a message condition, a mental \ncondition, and an action. To determine whether such a rule fires, the message condition is matched \nagainst the messages the agent has received; the mental condiiion is matched against the beliefs of \nthe agent. If the rule fires, then the agent becomes committed to the action. Actions may be private, \ncorresponding to an internally executed subroutine, or communicative, i.e., sending messages. \nMessages are constrained to be one of three types: "requests" or "unrequests" to perform or refrain \nfrom actions, and "inform" messages, which pass on information-Shoham indicates that he took \nhis inspiration for these message types from speech act theory (Searle, 1969; Cohen & Perrault, \n1979). Request and unrequest messages typically result in the agent's commitments being \nmodified; inform messages result in a change to the agent's beliefs. \n4.0.3 Tltomas-PLACA \nAGENT0 was only ever intended as a prototype, to illustrate the principles of AOP. A more \nrefined implementation was developed by Thomas, for her 1993 doctoral thesis (Thomas, 1993). \nHer Planning Communicating Agents (PLACA) language was intended to address one severe \ndrawback to AGENT0: the inability of agents to plan, and communicate requests for action via \nhigh-level goals. Agents in PLACA are programmed in much the same way as in AGENT0, in \nterms of mental change rules. The logical component of PLACA is similar to AGENTO's, but \nincludes operators for planning to do actions and achieve goals. The semantics of the logic and its \nproperties are examined in detail. However, PLACA is not at the "production" stage; it is an \nexperimental language. \n4.0.4 Fisher-Concurrent MetateM \nOne drawback with both AGENT0 and PLACA is that the relationship between the logic and \ninterpreted programming language is only loosely defined: in neither case can the programming \nlanguage be said to truly execute the associated logic. The Concurrent MetateM language \ndeveloped by Fisher can make a stronger claim in this respect (Fisher, 1994). A Concurrent \nMetateM system contains a number of concurrently executing agents. each of which is able to \ncommunicate with its peers via asynchronous broadcast message passing. Each agent is pro­\ngrammed by giving it a temporal logic specification of the behaviour that it is intended the agent \nshould exhibit. An agent's specification is executed directly to generate its behaviour. Execution of \nthe agent program corresponds to iteratively building a logical model for the temporal agent \nspecification. It is possible to prove that the procedure used to execute an agent specification is \ncorrect, in that if it is possible to satisfy the specification, then the agent will do so (Barringer et al., \n1989). \nThe logical semantics of Concurrent MetateM are closely related to the semantics of temporal \nlogic itself. This means that, amongst other things, the specification and verification of Concurrent \nMetateM systems is a realistic proposition (Fisher & Wooldridge, 1993). At the time of writing, \nonly prototype implementations of the language are available; full implementations are expected \nsoon. \n4.0.5 The IMAGINE Project-APRIL and MAIL \n,APRIL (McCabe & Clark, 1995) and MAIL (Haugeneder ct al., 1994) are two languages for \ndeveloping multi-agent applications that were developed as part of the ESPRIT project IMAGINE \n(Haugenedcr, 1994). The two languages arc intended to fulfil quite different roles. APRIL was \n\nIntelligent agents: theory and practice 141 \ndesigned to provide the core features required to realise most agent architectures and systems. \nThus APRIL provides facilities for multi-tasking (via processes, which are treated as first-class \nobjects, and a Unix-like fork facility), communication (with powerful message-passing facilities \nsupporting network-transparent agent-to-agent links); and pattern matching and symbolic process­\ning capabilities. The generality of APRIL comes at the expense of powerful abstractions-an \nAPRIL system builder must implement an agent or system architecture from scratch using \nAPRIL's primitives. In contrast, the MAIL language provides a rich collection of pre-defined \nabstractions, including plans and multi-agent plans. APRIL was originally envisaged as the \nimplementation language for MAIL. The MAIL system has been used to implement several \nprototype multi-agent systems, including an urban traffic management scenario (Haugeneder and \nSteiner, 1994). \n4.0.6 General Magic, lnc.-TELESCRIPT \nTELESCRIPT is a language-based environment for constructing agent societies that has been \ndeveloped by General Magic, Inc.: it is perhaps the first commercial agent language. \nTELESCRIPT technology is the name given by General Magic to a family of concepts and \ntechniques they have developed to underpin their products. There are two key concepts in \nTELESCRIPT technology: places and agents. Places are virtual locations that are occupied by \nagents. Agents are the providers and consumers of goods in the electronic marketplace applications \nthatTELESCRIPTwas developed to support. Agents are software processes, and are mobile: they \nare able to move from one place to another, in which case their program and state are encoded and \ntransmitted across a network to another place, where execution recommences. Agents are able to \ncommunicate with one-another: if they occupy different places, then they can connect across a \nnetwork, in much the standard way; if they occupy the same location, then they can meet one \nanother. \nFour components have been developed by General Magic to support TELESCRIPT tech­\nnology. The first is the TELESCRIPT language. This language "is designed for carrying out \ncomplex communication tasks: navigation, transportation, authentication, access control, ·and so \non" (White, 1994, p.17). The second component is the TELESCRIPT engine. An engine acts as an \ninterpreter for the TELESCRIPT language, maintains places, schedules agents for execution, \nmanages communication and agent transport, and finally, provides an interface with other \napplications. The third component is the TELESCRIPT protocol set. These protocols deal \nprimarily with the encoding and decoding of agents, to support transport between places. The final \ncomponent is a set of software tools to support the development of TELESCRIPT applications. \n4.0.7 Connah and Wavish-ABLE \nA group at Philips research labs in the UK have developed an Agent Behaviour Language (ABLE), \nin which agents are programmed in terms of simple, rule-like licences (Connah & Wavish, 1990; \nWavish, 1992). Licences may include some representation of time (though the language is not \nbased on any kind of temporal logic): they loosely resemble behaviours in the subsumption \narchitecture (see above). ABLE can be compiled down to a simple digital machine, realised in the \n"C" programming language. The idea is similar to situated automata, though there appears to \nbe no equivalent theoretical foundation. The result of the compilation process is a very fast \nimplementation, which has been used to control a Compact Disk-Interactive (CD-I) application. \nABLE has recently been extended to a version called Real-Time ABLE (RTA) (Wavish & \nGraham, 1995). \n4.1 Discussion \nThe emergence of various language-based software tools for building agent applications is clearly \nan important development for the wider acceptance and use of agent technology. The release of \nTELESCRIPT, a commercial agent language (albeit one that does not embody the strong notion of \n\nM. WOOLDRIDGE A:-1D NICHOLAS JENNINGS 142 \nagency discussed in this paper) is particularly important, as it potentially makes agent technology \navailable to a user base that is industrially (rather than academically) oriented. \nWhile the development of various languages for agent-based applications is of undoubted \nimportance, it is worth noting that all of the academically produced languages mentioned above are \nin some sense prototypes. Each was designed either to illustrate or examine some set of principles, \nand these languages were not, therefore, intended as production tools. Work is thus needed, both \nto make the languages more robust and usable, and to investigate the usefulness of the concepts \nthat underpin them. As with architectures, work is needed to investigate the kinds of domain for \nwhich the different languages are appropriate. \nFinally, we turn to the relationship between an agent language and the corresponding theories \nthat we discussed in section 2. As with architectures, it is possible to divide agent languages into \nvarious different categories. Thus AGENT0, PLACA, Concurrent MetateM, APRIL, and MAIL \narc deliberative languages, as they arc all based on traditional symbolic AI techniques. ABLE, on \nthe other hand, is a purely reactive language. With AGENT0 and PLACA, there is a clear (if \ninformal) relationship between the programming language and the logical theory the language is \nintended to realise. In both cases, the programming language represents a subset of the \ncorresponding logic, which can be interpreted directly. However, the relationship between logic \nand language is not formally defined. Like these two languages, Concurrent MetateM is intended \nto correspond to a logical theory. But the relationship hetween Concurrent MetateM and the \ncorresponding logic is much more closely defined, as this language is intended to be a directly \nexecutable version of the logic. Agents in Concurrent MetateM, however, are not defined in terms \nof mentalistic constructs. For a discussion on the relationship between Concurrent MetateM and \nAGENT0-like languages, see Fisher (1995). \n4.2 Further reading \nA recent collection of papers on concurrent object systems is Agha ct al. (1993). Various languages \nhave been proposed that marry aspects of object-based systems with aspects of Shoham·s agent­\noriented proposal. Two examples are AGENTSPEAK and DAISY. AGENTSPEAK is loosely \nbased on the PRS agent architecture, and incorporates aspects of concurrent-object technology \n(Weerasooriya et al., 1995). In contrast, DAISY is based on the concurrent-object language CUBL \n(Adorni & Poggi, 1993), and incorporates aspects of the agent-oriented proposal (Poggi, 1995). \nOther languages of interest include OZ (Henz et al., 1993) and IC PRO LOG Il (Chu, 1993). \nThe latter, as its name suggests, is an extension of PROLOG, which includes multiple-threads, \nhigh-level communication primitives, and some object-oriented features. \n5 Applications \nAlthough this article is not intended primarily as an applications review. it is nevertheless worth \npausing to examine some of the current and potential applications of agent technology. \n5.1 Cooperative problem solving and distributed Al \nAs we observed in section 1, there has been a marked flowering of interest in agent technology \nsince the mid-1980s. This interest is in part due to the upsurge of interest in Distributed Al \nAlthough DAI encompasses most ofthc issues we have discussed in this paper, it should be stressed \nthat the dassical emphasis in DAI has been on macro phenomena (the social level), rather than the \nmicro phenomena (the agent level) that we have been concerned with in this paper. DAI thus looks \nat such issues as how a group of agents can be made to cooperate in order to efficiently solve \nproblems, and how the activities of such a group can be efficiently coordinated. DAI researchers \nhave applied agent technology in a variety of areas. Example applications include power systems \nmanagement (Wittig, 1992; Varga et al., 1994), air-traffic control (Steeb et al., 1988), particle \n\nIntelligent agents: theory and practice 143 \naccelerator control (Jennings et al., 1993), intelligent document retrieval (Mukhopadhyay et al., \n1986), patient care (Huang et al., 1995), telecommunications network management (Weihmayer & \nVelthuijsen, 1994), spacecraft control (Schwuttke & Quan, 1993), computer integrated manufac­\nturing (Parunak, 1995), concurrent engineering (Cutkosky et al., 1993), transportation manage­\nment (Fischer et al., 1993), job shop scheduling (Morley & Schelberg, 1993), and steel coil \nprocessing control (Mori et al., 1988). The classic reference to DAI is Bond and Gasser (1988), \nwhich includes both a comprehensive review article and a collection of significant papers from the \nfield; a more recent review article is Chaib-draa et al. (1992). \n5.2 Interface agents \nMacs defines interface agents as: \n"[C]omputer programs that employ artificial Intelligence techniques in order to provide a~sistancc to a user \ndealing with a particular application .... The metaphor is that of a personal assistant who is collaborating \nwith the user in the same work environment." (Macs, 1994h, p. 71) \nThere are many interface agent prototype applications: for example, the NewT system is a \nUSENET news tilter (along the lines mentioned in the second scenario that introduced this article) \n(Maes, 1994a, pp. 38-39). A NcwT agent is trained by giving it series of examples, illustrating \narticles that the user would and would not choose to read. The agent then begins to make \nsuggestions to the user, and is given feedback on its suggestions. NewT agents are not intended to \nremove human choice, but to represent an extension of the human's wishes: the aim is for the agent \nto be able to bring to the attention of the user articles of the type that the user has shown a \nconsistent interest in. Similar ideas have been proposed by McGregor, who imagines prescient \nagents-intelligent administrative assistants that predict our actions, and carry out routine or \nrepetitive administrative procedures on our behalf (McGregor, 1992). \nThere is much related work being done by the computer supported cooperative work (CSCW) \ncommunity. CSCW is informally defined by Haecker to be "computer assisted coordinated activity \nsuch as problem solving and communication carried out by a group of collaborating individuals" \n(Haecker, 1993, p. l). The primary emphasis of CSCW is on the development of (hardware and) \nsoftware tools to support collaborative human work-the term gruupware has been coined to \ndescribe such tools. Various authors have proposed the use of agent technology in groupware. For \nexample, in his participant systems proposal, Chang suggests systems in which humans collaborate \nwith not only other humans, but also with artificial agents (Chang, 1987). We refer the interested \nreader to the collection of papers edited by Haecker (1993) and the article by Greif (1994) for more \ndetails on CSCW, \n5.3 Information agents and cooperative information systems \nAn information agent is an agent that has access to at least one, and potentially many information \nsources, and is able to collate and manipulate information obtained from these sources to answer \nqueries posed by users and other information agents (the network of interoperating information \nsources are often referred to as intelligent and cooperative information systems (Papazoglou et al., \n1992)). The information sources may be of many types, including, for example, traditional \ndatabases as well as other information agents. Finding a solution to a query might involve an agent \naccessing information sources over a network. A typical scenario is that of a user who has heard \nabout somebody at Stanford who has proposed something called agent-oriented programming, \nThe agent is asked to investigate, and, after a careful search of various FTP sites, returns with an \nappropriate tychnical report, as well as the name and contact details of the researcher involved. A \nnumber of studies have been made of information agents, including a theoretical study of how \nagents are able to incorporate information from different sources (Levy et al., 1994; Gruber, 1991 ), \nas well a prototype system called IRA (information retrieval agent) that is able to search for loosely \n\nM. WOOLDRIDGE AND NICHOLAS JENNINGS 144 \nspecified articles from a range of document repositories (Voorhees, 1994). Another important \nsystem in this area is called Carnot (Huhns et al., 1992), which allows pre-existing and hetero­\ngeneous database systems to work together to answer queries that are outside the scope of any of \nthe individual databases. \n5.4 Believable agents \nThere is ohvious potential for marrying agent technology with that of the cinema, computer games, \nand virtual reality. The Oz project6 was initiated to develop: \n.. artistically interesting, highly interactive, simulated worlds . . to give users the experience of living in \n(not merely watching) dramatically rich worlds that include moderately competent, emotional agents" \n(Batesetal., 1992b,p. 1) \nTo construct such simulated worlds, one must first develop believable agents: agents that "provide \nthe illusion oflife, thus permitting the audience's suspension of disbelief" (Bates, 1994, p. 122). A \nkey component of such agents is emotion: agents should not be represented in a computer game or \nanimated film as the flat, featureless characters that appear in current computer games. They need \nto show emotions; to act and react in a way that resonates in tune with our empathy and \nunderstanding of human behaviour. The Oz group have investigated various architectures for \nemotion (Bates et al., 1992a), and have developed at least one prototype implementation of their \nideas (Bates, 1994). \n6 Concluding remarks \nThis paper has reviewed the main concepts and issues associated with the theory and practice of \nintelligent agents. It has drawn together a very wide range of material, and has hopefully provided \nan insight into what an agent is, how the notion of an agent can be formalised, how appropriate \nagent architectures can be designed and implemented, how agents can be programmed, and the \ntypes of applications for which agent-based solutions have been proposed. The subject matter of \nthis review is important because it is increasingly felt, both within academia and industry, that \nintel!igent agents wilt be a key technology as computing systems become ever more distributed, \ninterconnected, and open. In such environments, the ability of agents to autonomously plan and \npursue their actions and goals, to cooperate, coordinate, and negotiate with others, and to respond \nflexibly and intelligently to dynamic and unpredictable situations will lead to significant improve­\nments in the quality and sophistication of the software systems that can be conceived and \nimplemented, and the application areas and problems which can be addressed. \nAcknowledgements \nMuch of this paper was adapted from the first author's 1992 PhD thesis (Wooldridge, 1992), and as \nsuch this work was supported by the UK Science and Engineering Research Council (now the \nEPSRC). We arc grateful to those people who read and commented on earlier drafts of this article, \nand in particular to the participants of the 1994 workshop on agent theories, architectures, and \nlanguages for their encouragement, enthusiasm, and helpful feedback. Finally, we would like to \nthank the referees of this paper for their perceptive and helpful comments. \nReferences \nAdorni, G and Poggi, A, 1993. "An object-oriented language for distributed artificial intelligence" Inter­\nnational Journal of Man-Machine Studies 38 435-453. \nAgha, G, 1986. ACTORS: A Model of Concurrent Computation in Di:,tributed Systems. MIT Press. \nAgha, G, Wegner, P and Yonezawa, A (eds.), 1993. Research Directions in Concurrent Objec1-0riented \nProgramming. MIT Pre:.s. \n6Not to be confused with the Oz programming language (Henz ct al., 1993). \n\nIntelligent agents: theory and practice 145 \nAgre, P and Chapman, D, 1987. "PENGI: An implementation of a theory of activity" In: Proceedings of the \nSixth National Conference on Artificial Intelligence (AAAI-87), pp 268-272, Seattle, WA. \nAllen, JF, 1984. "Towards a general theory of action and time" Artificial Intelligence 23 (2) 123--154. \nAllen, JF, Hendler, J and Tate, A (eds.), 1990. Readings in Planning. Morgan Kaufmann. \nAllen, JF, Kautz, H, Pelavin, Rand Tenenberg, J, 1991. Reasoning About Plans. Morgan Kaufmann. \nAmbros-Ingerson, J and Steel, S, 1988. "Integrating planning, execution and monitoring" In: Proceedings of \nthe Seventh National Conference on Artificial Intelligence (AAAI-88), pp 83-88, St. Paul, MN. \nAustin, JL, 1962. How to Do Things With Words. Oxford University Press. \nAy!ett, Rand Eustace, D, 1994. "Multiple cooperating robots-combining planning and behaviours'' In: SM \nDeen (ed) Proceedings of the 1993 Workshop on Cooperating Knowledge Based Systemv (CKBS-93), pp 3--\n11. DAKE Centre, University of Keele, UK. \nHaecker, RM (ed.) 1993. Readings in Groupware and Computer-Supported Cooperative Work. Morgan \nKaufmann. \nBarringer, H, Fisher, M, Gabbay, D, Gough, G and Owens, R, 1989. "MetateM: A framework for \nprogramming in temporal logic" In: REX Workshop on Stepwise Refinement of Di~trihwed System~: \nModels, Formalisms, Correctness (LNCS Volume 430) pp 94-129. Springer-Verlag. \nBarwise, J and Perry, J, 1983. Situations and Attitudes, MIT Press. \nBates, J, 1994. "The role of emotion in believable agents" Communications of the ACM 37 (7) 122-125. \nBates, J, Bryan Loyall, A and Scott Reilly, W, 1992a. "An architecture for action, emotion, and social \nbehaviour". Technical Report CMU-CS-92-144, School of Computer Science, Carnegie-Mellon Univer­\nsity, Pittsburgh, PA. \nBates, J, Bryan Loyall, A and Scott Reilly, W, 19926. "Integrating reactivity, goals, and emotion in a broad \nagent". Technical Report CMU-CS-92-142, School of Computer Science, Carnegie-Mellon University, \nPittsburgh, PA. \nBell, J, 1995. "Changing attitudes". In: M Wooldridge and NR Jennings (eds.) Intelligent Agents: Theories, \nArchitectures, and Languages (LNAI Volume 890), pp 40--55, Springer-Verlag. \nBelnap, N, 1991. ''Backwards and forwards in the modal logic of agency'' Philosophy and Phenomenological \nResearch LI (4) 777-807. \nBelnap, N and Perloff, M, 1988. "Seeing to it that: a canonical form for agentives" Theoria 54175-199. \nBond, AH and Gasser, L (eds.) 1988. Readings in Distribwed Artificial Intelligence, Morgan Kaufmann. \nBratman, ME, 1987. Intentions, Plans, and Practical Reason, Harvard University Press. \nBratman, ME, 1990. "What is intention?" In: PR Cohen, JL Morgan and ME Pollack (eds.) Intentions in \nCommunication, pp 15-32, MIT Press. \nBratman, ME, Israel, DJ and Pollack, ME, 1988. "Plans and resource-bounded practical reasoning'' \nComputational Intelligence 4 349-355. \nBrooks, RA, 1986. "A robust layered control system for a mobile robot" IEEE Journal of Robotics and \nAulomation 2 (1) 14-23. \nBrooks, RA, 1990. "Elephants don't play chess" In: P Maes (ed.) Designing Aulonomous Agenl.~, pp 3-15, \nMIT Press. \nBrooks, RA, 1991a. "Intelligence without reason" In: Proceedings of the Twelflh International Joint \nConference on Artificial Intelligence (JJCAl-91), pp 569-595, Sydney, Australia. \nBrooks, RA, 19916. "Intelligence without representation" Artificial Intelligence 47 139-159. \nBurmeister, Band Sundermeyer, K. 1992. ''Cooperative problem solving guided by intentions and percep­\ntion" In: E Werner and Y Demazeau (e<l~.) Decentralized Al 3-Proceedings of the Third European \nWorkshop on Modelling Auronomous Agents and Multi-Agent Worlds (MAAMAW-91), pp 77-92, \nElsevier. \nBussman, Sand Demazeau, Y, 1994. "An agent model combining reactive and cognitive capabilities" In: \nProceedings of the IEEE International Conference on Intelligent Robots and Systeml· (IROS-94), Munich, \nGermany. \nCaste!franchi, C, 1990. "Social power" In: Y Demazeau and J-P MU!ler(eds.) Decentralized Al-Proceedings \nof the First European Workshop on Modelling Autonomous Agents in Multi-Agent Worlds (MAAMA W-\n8Y), pp 49--62, Elsevier. \nCastelfranchi, C, 1995. "Guarantees for autonomy in cognitive agent architecture" In: M Wooldridge and NR \nJennings (eds.) Intelligent Agents. Theories, Architectures, and Languages (LNAI Volume 890), pp 56--70, \nSpringer-Verlag. \nCastelfranchi, C, Miceli, Mand Cesta, A, 1992. '·Dcpcndcncc relations among autonomous agents" In: E \nWerner and 'Y Dema:ceau (eds.) Decentralized Al 3-Proceedings of the Third European Workshop on \nModeUing Awonomous Agents and Multi-Agent Worlds (MAAMA W-Yl), pp 215-231, Elsevier. \nCatach, L, 1988. "Norma! multimodal logics"' In: Proceedings of the Seventh National Conference on Anificial \nIntelligence (AAAJ-88), pp 491-495. St. Paul, MN. \n\nM. WOOLDRIDGE AND NICHOLAS JENNINGS 146 \nChaib-draa, B, Moulin, B, Mandiau, Rand Millot, P, 1992. "Trends in distributed artificial intelligence·· \nArtificial Intelligence Review 6 35-66. \nChang, E, 1987. "Participant systems" In: M Huhns (ed.) Distributed Artificial Intelligence, pp 311-340, \nPitman. \nChapman, D, 1987. "Planning for conjunctive goals" Artificial Intelligence 32 333--378. \nChapman, D and Agre, P, 1986. "Abstract reasoning as emergent from concrete activity" In: MP Georgeff \nand AL Lansky (eds.) Reasoning About Actions & Plans~Proceedings of the 1986 Workshop pp 411-424, \nMorgan Kaufmann. \nChel\\as, B, 1980. Modal Logic: An Introduction, Cambridge University Press. \nChu, D, 1993. "l.C. PROLOG JI: A language for implementing multi-agent systems" In: SM Deen (ed.) \nProceedings of the 1992 Workshop on Cooperating Knowledge Based Systems (CKBS-92), pp 61-74, \nDAKE Centre, University of Keele, UK. \nCohen, PR, Greenberg ML, Hart OM and Howe AE. 1989. "Trial by fire: Understanding the design \nrequirements for agents in complex environments" Al Magazine 10 (3) 32-48. \nCohen, PR and Levesque, HJ, 1990a. "'Intention is choice with commitment" Artificial Intelligence 42 213-\n261. \nCohen, PR and Levesque, HJ, 1990h. ·'Rational interaction as the basis for communication" In: PR Cohen, J \nMorgan and ME Pollack (eds.) Intentions in Communication, pp 221-256. MIT Press. \nCohen, PR and Perrault, CR, 1979. ·'Elements of a plan based theory of speech acts" Cognitive Science 3177-\n212. \nConnah, D and Wavish. P, 1990. '·An experiment in cooperation" In: Y Demazeau and J-P Miiller (eds.) \nDecentralized AI-Proceedings of the First European Workshop on Modelling Autonomous Agents in \nMulti-Agent Worlds (MAAMA W-89), pp 197-214, Elsevier. \nCutkosky, MR, Engelmorc, RS, Fikes. RE, Gcnesereth. MR, Gruber, T, Mark, WS, Tenenbaum, JM and \nWeber, JC, 1993. "PACT: An experiment in integrating concurrent engineering systems'' IEEE Computer \n26 (l) 28-37. \nDavies, NJ, 1993. Truth, Modality, and Action, PhD thesis, Department of Computer Science, University of \nEssex, Colchester, UK. \nDean, TL and Wellman, MP. 1991. Planning and Control, Morgan Kaufmann. \nDennett, DC, 1978. Brainstorm~, MIT Press. \nDennett, DC. 1987. The Intentional Stance, MIT Press. \ndes Rivieres. J and Levesque. HJ, 1986. ''The consistency of ~yntactical treatments of knowledge" In: JY \nHalpern ( ed,) Proceedings of the 1986 Conference on Theoretical Aspects of Reasoning About Knowledge, \npp 115-130, Morgan Kaufmann. \nDevlin, K, 1991. Logic and Information, Cambridge University Press. \nDongha, P, 1995 "Toward a formal model of commitment for rc~ource-bounded agent~" In: M Wooldridge \nand NR Jennings (eds.) Intelligent Agents: Theories, Architectures, and Languages ( LNAJ Volume 890), pp \n86--101. Springer-Verlag. \nDown~, J and Rcichgelt, II, 1991. "Integrating classical and rcm.:tive planning within an architecture for \nautonomous agents·· In: J Hertzberg (ed,) European Worhhop on Planning (LNAI Volume 522), pp 13--\n26. \nDoyle. J, Shoham, Y and Wellman, MP, 1991. ''A logic of relative desire" In: ZW Ras and M Zemankova \n(cd~.) Methodologies for Intelligent Systems-Sixth International Symposium, ISMIS-91 (LNAI Volume \n542). Springer-Verlag. \nEmerson EA, 1990. "Temporal and modal logic" In: J van Leeuwen (ed.) HandhookofThroretical Computer \nScience. pp 996--1072, Elsevier. \nEmerson, EA and Halpern, JY, 1986. "'Sometimes' and 'not never' revisited: on branching time versus linear \ntime temporal logic" Journal of the ACM 33 (1) 151-178. \nEtzioni, 0. Lesh, N and Segal, R, 1994. ·'Building soft bots for UNIX'' In: 0 Etzioni (ed.) Software Agents­\nPaper~ from the 1994 Spring Symposium (Technical Report SS-94-03), pp 9-16, AAAI Press. \nFagin, Rand Halpern, JY, 1985. "Belief, awarenes~. and limited reasoning" In: Proceedings of the Ninth \nInternational Joint Conference on Artificial Intelligence (IJCAl-85), pp 480--490. Los Angeles, CA. \nFagin. R, Halpern, JY and Yard!, MY, 1992. "What can machines know? on the properties of knowledge in \ndistributed system~·· Journal of rhe A CM 39 (2) 328--376. \nFerguson IA. 1992a. Touring Machines: An Architecture for Dynamic, Rational, Mobile Agents, PhD thesis, \nClare Hall, University of Cambridge, UK. (Also available as Technical Report No. 273, University of \nCambridge Computer Laboratory.) \nFerguson. IA, 1992h. ·Towar<ls an architecture for adaptive, rational, mobile agents·• In: E Werner and Y \nDemazeau (eds.) Decentralized Al 3-Proceedings of the Third European Workshop on Modelling \nAutonomous Agents and Multi-Agent Worlds (MAAMA W-91), pp 249-262, Elsevier. \n\nIntelligent agents: theory and practice 147 \nFikes, RE and Nilsson. N, 1971. "STRIPS: A new approach to the application of theorem proving to problem \nsolving'· Artificial Intelligence 5 (2) 189-208. \nFirby, JA. 1987. "An investigation into reactive planning in complex domains"' In: Proceedings of the Tenth \nInternational Joint Conference on Artificial Intelligence (IJCA/-87), pp 202-206, Milan, Italy. \nFischer, K, Kuhn. N, MUiler, HJ, MUiler, JP and Pischel, M, 1993. ·'Sophisticated and distributed: The \ntransportation domain" In: Proceedings of the Fifth European Workshop on Modelling Autonomous \nAgents and Multi-Agent Worlds (MAAMA W-93), Neuchatel, Switzerland. \nFisher, M, 1994. "A survey ofConcurrcnt MetateM-the language and its applications"" In: DM Gabbay and \nHJ Ohlbach (eds.) Temporal Logic-Proceedings of the First International Conference (LNAJ Volume \n827). pp 480-505, Springer-Verlag. \nFisher, M. 1995. "Representing and executing agent-based systems·· In: M Wooldridge and NR Jennings \n(eds.) Intelligent Agents: Theories, Architectures, and Languages (LNAJ Volume 890), pp 307-323, \nSpringer-Verlag. \nFisher, M and Wooldridge, M, 1993. "Specifying and verifying distributed intelligent systems" In: M \nFilgueiras and L Damas (eds.) Progress in Artificial Intelligence-Sixth Portuguese Conference on Anificial \nIntelligence (LNAI Volume 727), pp 13-28. Springer-Verlag. \nGalliers. JR, 1988a. "A strategic framework for multi-agent cooperative dialogue" In: Proceedings of the \nEighth European Conference on Artificial Intelligence (ECAl-88), pp 415-420, Munich, Germany. \nGalliers, JR, 1988b. A Theoretical Framework for Computer Models of Cooperative Dialogue, Acknowledg­\ning Multi-Agent Conflict. PhD thesis, Open University, UK. \nGasser, L, 1991. "Social conceptions of knowledge and action: DAI foundations and open systems semantics" \nArtificial Intelligence 47 107-138. \nGasser, L, Braganza, C and Hermann, N, 1987. "MACE: A flexible testbed for distributed AI research" In: \nM Huhns (ed.) Distributed Artificial Intelligence, pp 119-152, Pitman. \nGasser, Land Briot, JP, 1992. "Object-based concurrent programming and DAI" In: Distributed Artificial \nIntelligence: Theory and Praxis. pp 81-108, Kluwer Academic. \nGeissler, C and Konolige. K, 1986. "A resolution method for quantified modal logics of knowledge and \nhelief'· In: JY Halpern (ed.) Proceedings of the 1986 Conference on Theoretical Aspects of Reasoning About \nKnowledge, pp 309-324, Morgan Kaufmann. \nGenescreth, MR and KetchpeL SP, 1994. "Software agents" Communications of the ACM 37 (7) 48-53. \nGenesereth. MR and Nils~on, N, 1987. Logical Foundations of Artificial Intelligence, Morgan Kaufmann. \nGcorgeff, MP, 1987. "Planning" Annual Review of Computer Science 2 359-400. \nGeorgeff, MP and lngrand, FF, 1989. "Decision-making in an embedded reasoning system" In: Proceedings \nof the Eleventh International Joint Conference on Artificial Intelligence ( IJCAl-89), pp 972-978, Detroit, \nML \nGcorgcff. MP and Lansky, AL (eds.) 1986. Reasoning About Actions & Plans-Proceedings of the 1986 \nWorkshop, Morgan Kaufmann. \nGeorgeff, MP and Lansky, AL, 1987. "Reactive reasoning and planning In: Proceedings of the Sixth National \nConference on Artificial Intelligence (AAAJ-87), pp 677-682, Seattle, WA. \nGinsberg, M. 1993. Essentials of Artificial Intelligence, Morgan Kaufmann. \nGmytrasiewicz, P and Durfee. EH, 1993. "Elements of a utilitarian theory of knowledge and action" In: \nProceedings of the Thirteenth International Joint Conference on Artificial Intelligence (IJCA/-93), pp 396--\n402, ChambCry. France. \nGoldblatt, R. 1987. Logics of Time and Computation, Centre for the Study of Language and lnformation­\nLccturc Notes Series. (Distributed by Chicago University Press.) \nGoldman, RP and Lang. RR, 1991 "Intentions in time"', Technical Report TUTR 93-101, Tulane University. \nGoodwin, R. 1993. "Formalizing properties of agents", Technical Report CMU-CS-93-159, School of \nComputer Science. Carnegie-Mellon University, Pittsburgh, PA. \nGreif. I, 1994. "Desktop agent~ in group-enabled products" Communications of the ACM 37 (7) 100-105. \nGrosz, BJ and Sidner, CL, 1990. ''Plans for discourse" In: PR Cohen, J Morgan and ME Pollack (eds.) \nIntentions in Communication. pp 417-444, MIT Press. \nGruber. TR, 1991. "The role of common ontology in achieving sharable, reusable knowledge bases'· In: R \nFikes and E Sandcwall (eds.) Proceedings of Knowledge Representation and Reasoning (KR&R-91), \nMorgan Kaufmann. \nGuha, RV and Lcnat, DB, 1994. "Enabling agents to work together" Communications of the ACM 37 (7) 127-\n142. \nHaas, A, 1986. "A syntactic theory of belief and knowledge" Artificial Intelligence 28 (3) 245-292. \nHaddadi, A. 1994. "A hybrid architecture for multi-agent systems" In: SM Deen (ed.) Proceedings of the 1993 \nWorkshop 011 Cooperating Knowledge Based Systems (CKBS-93), pp 13-26. DAKE Centre, University of \nKeele, UK. \n\nM. WOOLDRIDGE AND NICHOLAS JENNINGS 148 \nHalpern, JY, 1986. "Reasoning about knowledge: An overview" In: JY Halpern (ed.) Proceedings of the 1986 \nConference on Theoretical Aspects of Reasoning About Knowledge, pp 1-18, Morgan Kaufmann. \nHalpern, JY, 1987. "Using reasoning about knowledge to analyze distributed systems" Annual Review of \nComputer Science 2 37--68. \nHalpern, JY and Moses, Y, 1992. "A guide to completeness and complexity for modal logics of knowledge and \nbelief' Artificial Intelligence 54 319-379. \nHalpern, JY and Vardi, MY, 1989. "The complexity of reasoning about knowledge and time. I. Lower \nbounds" Journal of Computer and System Sciences 38 195-237. \nHare!, D, 1984. "Dynamic logic" In: D Gabbay and F Guenther (eds.) Handbook of Philosophical Logic \nVolume ll-Extensions of Classical Logic, pp 497-604, Reidel. \nHaugeneder, H, 1994. IMAGINE final project report. \nHaugeneder, Hand Steiner, D, 1994. "A multi-agent approach to cooperation in urban traffic" In: SM Deen \n(ed.) Proceedings of the 1993 Workshop on Cooperating Knowledge Based Systems (CKBS-93), pp 83-98, \nDAKE Centre, University of Keele, UK. \nHaugeneder, H, Steiner, D and McCabe, FG, 1994. "IMAGINE: A framework for building multi-agent \nsystems" In: SM Deen (ed.) Proceedings of the 1994 Jnternational Working Conference on Cooperating \nKnowledge Based Systems (CKBS-94), pp 31--64, DAKE Centre, University of Keele, UK. \nHayes-Roth, B, 1990. "Architectural foundations for real-time performance in intelligent agents" The \nJournal of Real-Time Systems 2 99-125. \nHendler, J (ed.) 1992. Artificial intelligence Planning: Proceedings of the First International Conference, \nMorgan Kaufmann. \nHenz, M, Smolka, G and Wuertz, J, 1993. "Oz-a programming language for multi-agent systems" ln: \nProceedings of the Thirteenth international Joint Conference on Artificial Intelligence ( IJCAI-93), pp 404-\n409, ChambCry, France. \nHewitt, C, 1977. "Viewing control structures as patterns of passing messages" Artificial intelligence 8 (3) 323-\n364. \nHintikka, J, 1962. Knowledge and Belief, Cornell University Press. \nHoulder, V, 1994. "Special agents" In: Financial Times, 15 August, p 12. \nHuang, J, Jennings, NR and Fox. J, 1995. "An ag-ent architecture for distributed medical care" In: M \nWooldridge and NR Jennings (eds.) Intelligent Agents: Theories, Architectures, and Languages (LNA! \nVolume 890), pp 219-232, Springer-Verlag. \nHughes, GE and Cresswell, MJ, 1968. lntroduction to Modal Logic, Methuen. \nHuhns, MN, Jacobs, N, Ksiezyk, T, Shen, WM, Singh, MP and Cannata, PE, 1992. "Integrating enterprise \ninformation models in Carnot" In: Proceedings of the International Conference on Intelligent and \nCooperative information Systems, pp 32-42, Rotterdam, The Netherlands. \nIsrael, DJ, 1993. "The role(s) of logic in artificial intelligence" In: DM Gabbay, CJ Hogger and JA Robinson \n(eds.) Handbook of Logic in Artificial Intelligence and Logic Programming, pp 1-29, Oxford University \nPress. \nJennings, NR, 1992. "On being responsible" In: E Werner and Y Demazeau (eds.) Decentralized Al]­\nProceedings of the Third European Workshop on Modelling Autonomous Agents and Multi-Agent Worlds­\n(MAAMA W-91), pp 93-102, Elsevier. \nJennings. NR, 1993a. "Commitments and conventions: The foundation of coordination in multi-agent \nsystems" Knowledge Engineering Review 8 (3) 223-250. \nJennings, NR, 1993b. "Specification and implementation of a belief desire joint-intention architecture for \ncollaborative problem solving" Journal of intelligent and Cooperative Information Systems 2 (3) 289-318. \nJennings, NR, 1995. "Controlling cooperative problem solving in industrial multi-agent systems using joint \nintentions" Artificial Intelligence 14 (2) (to appear). \nJennings, NR. Varga, LZ, Aarnts, RP, Fuchs, J and Skarek, P, 1993. "Transforming standalone expert \nsystems into a community of cooperating agents'' International Journal of Engineering Applications of \nArtificial Intelligence 6 ( 4) 317-331. \nKaelbling, LP, 1986. "An architecture for intelligent reactive systems" In: MP George ff and AL Lansky (eds.) \nReasoning About Actions and Plans-Proceedingofthe 1986 Workshop, pp 395-410, Morgan Kaufmann. \nKaelbling, LP, 1991. ·'A situated automata approach to the design of embedded agents" SIG ART Bulletin 2 \n(4) 85-88. \nKaelbling, LP and Roscnschein, SJ, 1990. "Action and planning in embedded agents" In: P Maes (ed.) \nDesigning Autonomous Agents, pp 35-48, MIT Press. \nKinny, D, Ljungberg, M, Rao, AS, Sonenberg, E, Tidhar, G and Werner, E, 1992. "Planned team activity" \nIn: C Castelfranchi and E Werner (eds.) Artificial Social Systems-Selected Papers from the Fourth \nEuropean Workshop on Modelling Autonomous Agents and Multi-Agent Worlds, MAAAMA W-92 (LNA! \nVolume 830), pp 226-256, Springer-Verlag. \n\nIntelligent agents: theory and practice 149 \nKiss, G and Reichgelt, H, 1992. "Towards a semantics of desires" In: E Werner and Y Demazeau (eds.) \nDecentralized Al 3-Proceedings of the Third European Workshop on Modelling Autonomous Agents and \nMulti-Agent Worlds (MAAMAW-91), pp 115-128, Elsevier. \nKonolige, K, 1982. "A first-order formalization of knowledge and action for a multi-agent planning system" \nIn: JE Hayes, D Michie and Y Pao (eds.) Machine Intelligence JO, pp 41-72, Ellis Horwood. \nKonolige, K, 1986a. A Deduction Model of Belief, Pitman. \nKonolige, K, 1986b. "What awareness isn't: A sentential view of implicit and explicit belief (position paper)" \nIn: JY Halpern (ed.) Proceedings of the 1986 Conference on Theoretical Aspects of Reasoning About \nKnowledge, pp 241-250, Morgan Kaufmann. \nKonolige, Kand Pollack, ME, 1993. "A representationalist theory of intention" In: Proceedings of the \nThirteenth International Joint Conference on Artificial Intelligence (IJCAJ-93), pp 390--395, Chambefy, \nFrance. \nKraus, Sand Lehmann, D (1988) "Knowledge, belief and time" Theoretical Computer Science 58 155-174. \nKripke, S, 1963. "Semantical analysis of modal logic" Zeitschrift fUr Mathematische Logik und Grundlagen \nder Mathematik 9 67-96. \nLakemeyer, G, 1991. "A computationally attractive first-order logic of belief" In: JELIA-90: Proceedings of \nthe European Workshop on Logics in AI (LNAI Volume 478), pp 333-347, Springer-Verlag. \nLesperance, Y, 1989. "A formal account of self knowledge and action" In: Proceedings of the Eleventh \nImernational Joint Conference on Artificial Intelligence (IJCAI-89), pp 868-874, Detroit, Ml. \nLevesque, HJ, 1984. "A logic of implicit and explicit belief" In: Proceedings of the Fourth National Conference \non Artificial Intelligence (AAAI-84), pp 198-202, Austin, TX. \nLevesque, HJ, Cohen, PR and Nunes, JHT, 1990. "On acting together" In: Proceedings of the Eighth National \nConference on Artificial Intelligence (AAAJ-90), pp 94-99, Boston, MA. \nLevy, A Y, Sagiv, Y and Srivastava, D, 1994. "Towards efficient information gathering agents" In: 0 Etzioni \n(ed.) Software Agents-Papers from the 1994 Spring Symposium (Technical Report SS-94-03), pp 64-70, \nAAAI Press. \nMack, D, 1994. "A new formal model of belief" In: Proceedings of the Eleventh European Conference on \nArtificial Intelligence (ECAI-94), pp 573-577, Amsterdam, The Netherlands. \nMaes, P, 1989. "The dynamics of action selection" In: Proceedings of the Eleventh International Joint \nConference on Artificial Intelligence (IJCAI-89), pp 991-997, Detroit, MI. \nMaes, P (ed.) 1990a. Designing Autonomous Agents, MIT Press. \nMaes, P, 1990b. "Situated agents can have goals" In: P Maes (ed.) Designing Autonomous Agents, pp49-70, \nMIT Press. \nMaes, P, 1991. "The agent network architecture (ANA)" SIG ART Bulletin 2 (4) 115-120. \nMaes, P, 1994a. "Agents that reduce work and information overload" Communications of the ACM 37 (7) 31-\n40. \nMacs, P, 1994b. "Social interface agents: Acquiring competence by learning from users and other agents" In: \n0 Etzioni (ed.) Software Agents-Papers from the J 994 Spring Symposium (Technical Report SS-94-03), pp \n71-78, AAA! Press. \nMcCabe, FG and Clark, KL, 1995. "April-agent process interaction language" In: M Wooldridge and NR \nJennings (eds.) Intelligent Agents: Theories, Architectures, and Languages (LNAI Volume 890), pp 324-\n340, Springer-Verlag. \nMcCarthy, J, 1978. "Ascribing mental qualities to machines." Technical report, Stanford University Al Lab., \nStanford, CA 94305. \nMcGregor, SL, 1992. "Prescient agents" In: D Coleman (ed.) Proceedings of Groupware-92, pp 228-230. \nMontague, R, 1963. "Syntactical treatments of modality, with corollaries on reflexion principles and finite \naxiomatizations" Acta Philosophica Fennica 16153-167. \nMoore, RC, 1990. '"A formal theory of knowledge and action" In: JF Allen, J Hendler and A Tate (eds.) \nReadings in Planning, pp 480-519, Morgan Kaufmann. \nMorgenstern, L, 1987. "Knowledge preconditions for actions and plans" In: Proceedings of the Tenth \nInternational Joint Conference on Artificial Intelligence (JJCAI-87), pp 867-874, Milan, Italy. \nMori, K, Torikoshi, H, Nakai, Kand Masuda, T, 1988. "Computer control system for iron and steel plants" \nHitachi Review 37 (4) 251-258. \nMorley, RE and Schelberg, C, 1993. "An analysis of a plant-specific dynamic scheduler'· In: Proceedings of the \nNSF Workshop on Dynamic Scheduling, Cocoa Beach, Florida. \nMukhopadhyay, U, Stephens, Land Huhns, M, 1986. "An intelligent system for document retrieval in \ndistributed office environments'' Journal of the American Society for Information Science 37 123-135. \nMillier, JP, 1994. "A conceptual model for agent interaction" In: SM Deen (ed.) Proceedings of the Second \nInternational Working Conference on Cooperating Knowledge Based Systems (CKBS-94), pp 213-234, \nDAKE Centre, University of Keele, UK. \n\nM. WOOLDRIDGE AND N[CHOLAS JENNINGS 150 \nMUiler, JP and Pischel, M, 1994. "Modelling interacting agents in dynamic environments .. In: Proceedings of \nthe Eleventh European Conference on Artificial Intelligence (ECAI-94), pp 709-713, Amsterdam, The \nNetherlands. \nMi.i\\ler, JP, Pischcl, M and Thiel, M, 1995. "Modelling reactive behaviour in vertically layered agent \narchitectures" In: M Wooldridge and NR Jennings (eds.) Intelligent Agents: Theories, Architectures, and \nLanguages (LNAI Volume 890), pp 261-276, Springer-Verlag. \nNewell, A and Simon, HA, 1976. "Computer science as empirical enquiry" Communications of the ACM 19 \n113-126. \nNilsson, NJ, 1992. "Towards agent programs with circuit semantics", Technical Report STAN-CS-92-1412, \nComputer Science Department, Stanford University, Stanford, CA 94305. \nNorman, TJ and Long, D, 1995. "Goal creation in motivated agents" In: M Wooldridge and NR Jennings \n(eds.) Intelligent Agents: Theories, Architectures, and Languages (LNAI Volume 890), pp 277-290, \nSpringer-Verlag. \nPapazoglou, MP, Laufman, SC and Sel\\is, TK, 1992. "An organizational framework for cooperating \nintelligent information systems" Journal of Intelligent and Cooperative Information Systems 1 (1) 169-202. \nParunak, HVD, 1995. "Applications of distributed artificial intelligence in industry'" In: GMP O'Hare and \nNR Jennings (eds.) Foundations of Distributed Al, John Wiley. \nPatil, RS, Fikes, RE, Patel-Schneider, PF, McKay, D, Finin, T, Gruber, T and Neches, R, 1992. "The \nDARPA knowledge sharing effort: Progress report" In: C Rich, W Swartout and B Nebel (eds.) \nProceedings of Knowledge Representation and Reasoning (KR&R-92), pp 777-788. \nPerlis, D, 1985. "Languages with self reference I: Foundations" Artificial Intelligence 25 301-322. \nPerlis, D, 1988. "Languages with self reference II: Knowledge, belief, and modality" Artificial Intelligence 34 \n179-212. \nPerloff, M, 1991. "STIT and the language of agency" Synthese 86 379-408. \nPoggi, A, 1995. "DAISY: An object-oriented system for distributed artificial intelligence" In· M Wooldridge \nand NRJennings (eds.) Intelligent Agents: Theories, Architectures, and Languages (LNA!Volume890). pp \n341-354, Springer-Verlag. \nPollack, ME and Ringuette, M, 1990. "Introducing the Tileworid: Experimentally evaluating agent architec­\ntures" In: Proceedings of the Eighth National Conference on Artificial Intelligence (AAAl-90), pp 183-189, \nBoston, MA. \nRao, AS and Georgeff, MP, 1991a. '"Asymmetry thesis and side-effect problems in linear time and branching \ntime intention logics'" In: Proceedings of the Twelfth International Joint Conference on Artificial Intelligence \n(IJCAl-91), pp 498-504, Sydney, Australia. \nRao, AS and Georgeff, MP, 1991b. "Modeling rational agents within a BDI-architccture·• In: R Fikes and E \nSandewal! (eds.) Proceedings of Knowledge Representation and Reasoning (KR&R-9/), pp 473-484, \nMorgan Kaufmann. \nRao, AS and Georgeff, MP, 1992a. "An abstract architecture for rational agents"' In: C Rich, W Swartout and \nB Nebel (eds.) Proceedings of Knowledge Representation and Reasoning (KR&R-92), pp 439-449. \nRao, AS and Georgeff, MP, 1992b. "Social plans: Preliminary report" In: E Werner and Y Demazeau (eds.) \nDecentralized Al 3-Proceedings of the Third European Workshop on Modelling Autonomous Agents and \nMulti-Agent Worlds (MAAMA W-9I), pp 57-76, Elsevier. \nRao, AS and Georgeff, MP, 1993. "A model-theoretic approach to the verification of situated reasoning \nsystems" In: Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (IJCAI-\n93), pp 318-324, Chambery, France. \nReichgclt, H, 1989a "A comparison of first-order and modal logics of time" In: P Jackson, H Reichge!t and F \nvan Harmclen (eds.) Logic Based Knowledge Representation, pp 143-176, MIT Press. \nReichgelt. H, 198%. "Logics for reasoning about knowledge and belief" Knowledge Engineering Review4 (2) \n119-139. \nRosenschein, JS and Genesereth, MR, 1985 "Deals among rational agents·• In: Proceedings of the Ninth \nInternational Joint Conference on Anificial Intelligence (IlCAl-85), pp 91-99, Los Angeles, CA \nRosenschein, S, 1985. "Formal theories of knowledge in AI and robotics'' New Generation Computing, pp \n345-357. \nRosenschein, S and Kae\\bling, LP, 1986. '·Toe synthesis of digital machines with provable epistemic \nproperties" In: JY Halpern (ed.) Proceedings of the 1986 Conference on Theoretical Aspects of Reasoning \nAbout Knowledge, pp 83-98, Morgan Kaufmann. \nRussell, SJ and Wefald, E, 1991 Do the Right Thing-Studies in Limited Rationality. MIT Press. \nSacerdoti. E, 1974. "Planning in a hierarchy of abstraction spaces" Artificial Intelligence 5 115-135. \nSacerdoti, E, 1975. '"The non-linear nature of plans'' In: Proceedings of the Fourth International Joint \nConference on Artificial Intelligence ( IlCAl-75), pp 206--214. Stanford, CA. \nSadek. MD, 1992. '·A study in the logic of intention" In: C Rich, W Swartout and B Nebel (eds.) Proceedings \nof Knowledge Representation and Reasoning (KR&R-92). pp 462-473. \n\nIntelligent agents: theory and practice 151 \nSargent, P, 1992. "Back to school for a brand new ABC" In: The Guardian, 12 March, p 28. \nSchoppeN;, MJ, 1987. "Universal plans for reactive robots in unpredictable environments" In: Proceedings of \nthe Tenth International Joint Conference on Artificial Intelligence ( /JCAl-87), pp 1039-1046, Milan, Italy. \nSchwuttke, UM and Quan, AG, 1993. '·Enhancing performance of cooperating agents in real-time diagnostic \nsystems" In: Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (JJCAl-\n93), pp 332-337, Chambfay, France. \nSearle, JR, 1969. Speech Acts: An Essay in the Philosophy of Language, Cambridge University Press. \nSeel, N, 1989. Agent Theories and Architectures, PhD thesis, Surrey University, Guildford, UK. \nSegerbcrg, K, 1989. "Bringing it about" Journal of Philosophical Logic 18 327-347. \nShardlow, N, 1990. "Action and agency in cognitive science", Master's thesis, Department of Psychology, \nUniversity of Manchester, Oxford Road, Manchester M13 9PL, UK. \nShoham, Y, 1988. Reasoning About Change: Time and Causation from the Standpoint of Artificial Intelligence, \nMIT Press. \nShoham, Y, 1989. "Time for action: on the relation between time, knowledge and action" In: Proceedings of \nthe Eleventh /nternationalJointConferenceonArtificial Intelligence (lJCAl-89), pp 954-959, Detroit, Ml. \nShoham, Y, 1990. "Agent-oriented programming", Technical Report STAN-CS-1335-90, Computer Science \nDepartment, Stanford University, Stanford, CA 94305. \nShoham, Y, 1993. "Agent-oriented programming" Artificial Intelligence 60 (1) 51-92. \nSingh, MP, l990a. '·Group intentions" In: Proceedings of the Tenth International Workshop on Distributed \nArtificial Intelligence (IWDAI-90). \nSingh, MP. 1990b. "Towards a theory of situated know-how" In: Proceedings of the Ninth European \nConference on Artificial Intelligence (ECAJ-90), pp 604-609, Stockholm, Sweden. \nSingh, MP, 1991a. "Group ability and structure" In: Y Dcmazeau and JP MUiler (eds.) Decentralized Al 2-\nProceedings of the Second European Workshop on Modelling Autonomous Agents and Multi-Agent Worlds \n(MAAMAW-90), pp 127-146, Elsevier. \nSingh, MP, 1991b '·Towards a formal theory of communication for multi-agent systems" In: Proceedings of rhe \nTwelfth International Joint Conference on Artificial Intelligence (lJCAI-91), pp 69-74, Sydney, Australia. \nSingh, MP, 1992. "A critical examination of the Cohen-Levesque theory of intention" In: Proceedings of the \nTenth European Conference on Artificial Intelligence (ECAl-92), pp 364-368. Vienna, Austria. \nSingh, MP, 1994. Multiagent Systems: A Theoretical Framework for Intentions, Know-How, and Communi­\ncations (LNAI Volume 799), Springer-Verlag. \nSingh, MP and Asher, NM, 1991. "Towards a formal theory of intentions" In: Logics in Al-Proceedings of \nthe European Workshop JELIA-90 (LNA! Volume 478), pp 472-486, Springer-Verlag. \nSmith, RG, 1980. A Framework for Distributed Problem Solving, UMI Research Press. \nSteeb, R, Cammarata S, Hayes-Roth FA, Thorndyke PW and Wesson RB, 1988. "Distributed intelligence for \nair fleet control" In: AH Bond and L Gasser (eds.) Readings in Distributed Artificial Intelligence, pp 90-\n101, Morgan Kaufmann. \nSteels, L, 1990. "Cooperation between distributed agents through self organization" In: Y Demazeau and JP \nMUiler (eds.) Decentralized Al-Proceedings of the First European Workshop on Modelling Autonomous \nAgents in Multi-Agent Worlds (MAAMA W-89), pp 175-196, Elsevier. \nThomas, SR, 1993. PLACA, an Agent Oriented Programming Language, PhD thesis, Computer Science \nDepartment, Stanford University, Stanford, CA 94305. (Available as technical report STAN-CS-93-\n1487). \nThoma~, SR, Shoham Y, Schwartz A and Kraus S, 1991. "Preliminary thoughts on an agent description \nlanguage" International Journal of Intelligent Systems 6 497-508. \nThomason, R, 1980. "A note on syntactical treatments of modality" Synthese 44 391-395. \nTurner, R, 1990. Truth and Modality for Knowledge Representation, Pitman. \nVarga, LZ, Jennings, NR and Cockburn, D, 1994. "Integrating intelligent systems into a cooperating \ncommunity for electricity distribution management" International Journal of Expert Systems with Appli­\ncations 1 (4) 563-579. \nVere, Sand Bickmore, T, 1990. "A basic agent" Computational Intelligence 6 41-60. \nVoorhees, EM, 1994. •'Software agents for information retrieval" In: 0 Etzioni (ed.) Software Agents­\nPapers from the 1994 Spring Symposium (Technical Report SS-94-03), pp 126--129, AAAI Press. \nWainer, J, 1994. "Yet another semantics of goals and goal priorities" In: Proceedings of the Eleventh \nEuropean Conference on Artificial Intelligence (ECIA-94), pp 269-273, Amsterdam, The Netherlands. \nWavish, P, 1992. "Exploiting emergent behaviour in multi-agent systems" In: E Werner and Y Demazeau \n(eds.) Decentralized Al 3-Proceedings of the Third European Workshop on Modelling Autonomous \nAgents and Multi-Agent Worlds (MAAMA W-91), pp 297-310, Elsevier. \nWavish, P and Graham, M, 1995. "Role, skills, and behaviour: a situated action approach to organising \nsystems of interacting agents" In: M Wooldridge and NR Jennings (eds.) Intelligent Agents: Theories, \nArchi1ectures, and Languages (LNAI Volume 890), pp 371-385, Springer-Verlag. \n\nM. WOOLDRIDGE AND NICHOLAS JENNINGS 152 \nWeerasooriya, D, Rao, A and Ramamohanarao, K, 1995. "Design of a concurrent agent-oriented language" \nIn: M Wooldridge and NR Jennings (eds.) Intelligent Agents: Theories, Architectures, and Languages \n(LNAJ Volume 890), pp 386----402, Springer-Verlag. \nWeihmayer, Rand Velthuijsen, H, 1994. "Application of distributed AI and cooperative problem solving to \ntelecommunications" In: J Liebowitz and D Prcreau (eds.) Al Approaches to Telecommunications and \nNetwork Management, IOS Press. \nWerner, E, 1988. "Toward a theory of communication and cooperation for multiagent planning" In: MY \nVar di (ed.) Proceedings of the Second Conference on Theoretical Aspects of Reasoning About Knowledge, \npp 129-144, Morgan Kaufmann. \nWerner, E, 1989. "Cooperating agents: A unified theory of communication and social structure" lq: L Gasser \nand M Huhns (eds.) Distributed Artificial Intelligence Volume 11, pp 3---36, Pitman. \nWerner, E, 1990. "Wh.at·can agents do together: A semantics of cooperative ability" In: Proceedings of the \nNinth European Conference on Artificial Intelligence (ECAJ-90), pp 694-701, Stockholm, Sweden. \nWerner, E, 1991. "A unified view of information, intention and ability" In: Y Demazeau and JP Miiller (eds.) \nDecentralized Al 2-Proceedings of the Second European Workshop on Modelling Autonomous Agents \nand Multi-Agent Worlds (MAAMA W-90), pp 109-126, Elsevier. \nWhite, JE, 1994. "Telescript technology: The foundation for the electronic marketplace'', White paper, \nGeneral Magic, Inc., 2465 Latham Street, Mountain View, CA 94040. \nWilkins, D, 1988. Practical Planning: Extending the Classical Al Planning Paradigm, Morgan Kaufmann. \nWittig, T (ed.) 1992. ARCH ON: An Architecture for Multi-Agent Systems, Ellis Horwood. \nWood, S, 1993. Planning and Decision Making in Dynamic Domains, Ellis Horwood. \nWooldridge, M, 1992. The Logical Modelling of Computational Multi-Agent Systems, PhD thesis, Depart­\nment of Computation, UMIST, Manchester, UK. (Also available as Technical Report MMU-DOC-94-01, \nDepartment of Computing, Manchester Metropolitan University, Chester Street, Manchester, UK.) \nWooldridge, M, 1994. ·"Coherent social action" In: Proceedings of the EleJ.'enth European Conference on \nArtificial Intelligence (ECAI-94), pp 279-283, Amsterdam, The Netherlands. \nWooldridge, M, 1995. "This is MYWORLD: The logic of an agent-oriented testbed for DAI" In: M \nWooldridge and NR Jennings (eds.) Intelligent Agents: Theories, Architectures, and Languages (LNAI \nVolume 890), pp 160-178, Springer-Verlag. \nWooldridge, M and Fisher M, 1992. "A first-order branching time logic of multi-agent systems" In: \nProceedings of the Tenth European Conference on Artificial Intelligence (ECAl-92), pp 234-238, Vienna, \nAustria. \nWooldridge, Mand Fisher M, 1994. "A decision procedure for a temporal belief logic" In: OM Gabbay and \nHJ Ohlbach. (eds.) Temporal Logic-Proceedings of the First International Conference (LNAI Volume \n827), pp 317-331, Springer-Verlag. \nWooldridge, M and Jennings NR, 1994. "Formalizing the cooperative problem solving process" In: \nProceedings of the Thirteenth International Workshop on Distributed Artificial Intelligence (IWDAJ-94), pp \n403---417, Lake Quinalt, WA. \nYonczawa, A (ed.) 1990. ABCL-An Object-Oriented Concurrent System, MIT Press. 	The Knowledge Engineering Review, Vol. 10:2, 1995, 115-152 \nIntelligent agents: theory and practice \nMICHAEL WOOLDRIDGE 1 and NICHOLAS R. JENNINGS 2 \n1 Deparrmenr of Compwing. Manche.,·ter Metropolitan Univeni1y, Chester Street, Manches1er MI 5GD, UK \n(M. Wooldridge(ri)doc.mmu.ac.uk) \n2nepartmmt of Electronic F.ngineering, Queen Mary & Westfield College, Mile End Road, London El 4NS, UK \n( N. R .JennmgI(0.1qm w. ac. uk) \nAbstract \nThe concept of an agent has become important in both artificial i...	en	0.9	uploaded	24657	163725	2025-09-05 18:59:36.069925	2025-09-05 18:59:36.069928	\N	1	\N	\N
64	Jensen - Theory of the Firm Managerial Behavior, Agency Costs and Ownership Structure	file	document	\N	pdf	Jensen - Theory of the Firm Managerial Behavior, Agency Costs and Ownership Structure.pdf	uploads/e3009d2bfbbd4c2fbf442d19033396eb_Jensen_-_Theory_of_the_Firm_Managerial_Behavior_Agency_Costs_and_Ownership_Structure.pdf	383070	\N	Electronic copy available at: http://ssrn.com/abstract=94043Electronic copy available at: http://ssrn.com/abstract=94043\nTheory of the Firm: Managerial Behavior,  Agency Costs and Ownership Structure Michael C. Jensen Harvard Business School MJensen@hbs.edu  And  William H. Meckling University of Rochester Abstract This paper integrates elements from the theory of agency, the theory of property rights and the theory of finance to develop a theory of the ownership structure of the firm. We define the concept of agency costs, show its relationship to the ‘separation and control’ issue, investigate the nature of the agency costs generated by the existence of debt and outside equity, demonstrate who bears costs and why, and investigate the Pareto optimality of their existence. We also provide a new definition of the firm, and show how our analysis of the factors influencing the creation and issuance of debt and equity claims is a special case of the supply side of the completeness of markets problem.  The directors of such [joint-stock] companies, however, being the managers rather of other people’s money than of their own, it cannot well be expected, that they should watch over it with the same anxious vigilance with which the partners in a private copartnery frequently watch over their own. Like the stewards of a rich man, they are apt to consider attention to small matters as not for their master’s honour, and very easily give themselves a dispensation from having it. Negligence and profusion, therefore, must always prevail, more or less, in the management of the affairs of such a company.   —  Adam Smith (1776) Keywords: Agency costs and theory, internal control systems, conflicts of interest, capital structure, internal equity, outside equity, demand for security analysis, completeness of markets, supply of claims, limited liability   ©1976 Jensen and Meckling Journal of Financial Economics, October, 1976, V. 3, No. 4, pp. 305-360. Reprinted in Michael C. Jensen, A Theory of the Firm: Governance,  Residual Claims and Organizational Forms (Harvard University Press, December 2000)  available at http://hupress.harvard.edu/catalog/JENTHF.html Also published in Foundations of Organizational Strategy,  Michael C. Jensen, Harvard University Press, 1998.  You may redistribute this document freely, but please do not post the electronic file on the web. I welcome web links to this document at: http://papers.ssrn.com/abstract=94043. I revise my papers regularly, and providing a link to the original ensures that readers will receive the most recent version. Thank you, Michael C. Jensen\n\n\nElectronic copy available at: http://ssrn.com/abstract=94043Electronic copy available at: http://ssrn.com/abstract=94043\n* Associate Professor and Dean, respectively, Graduate School of Management, University of Rochester.  An\nearlier version of this paper was presented at the Conference on Analysis and Ideology, Interlaken,\nSwitzerland, June 1974, sponsored by the Center for Research in Government Policy and Business at the\nUniversity of Rochester, Graduate School of Management.  We are indebted to F. Black, E. Fama, R.\nIbbotson, W. Klein, M. Rozeff, R. Weil, O. Williamson, an anonymous referee, and to our colleagues and\nmembers of the Finance Workshop at the University of Rochester for their comments and criticisms, in\nparticular G. Benston, M. Canes, D. Henderson, K. Leffler, J. Long, C. Smith, R. Thompson, R. Watts, and J.\nZimmerman.\nTheory of the Firm: Managerial Behavior,\nAgency Costs and Ownership Structure\nMichael C. Jensen\nHarvard Business School\nand\nWilliam H. Meckling*\nUniversity of Rochester\n1.  Introduction\n1.1.Motivation of the Paper\nIn this paper we draw on recent progress in the theory of (1) property rights, (2) agency,\nand (3) finance to develop a theory of ownership structure 1 for the firm.  In addition to tying\ntogether elements of the theory of each of these three areas, our analysis casts new light on and\nhas implications for a variety of issues in the professional and popular literature including the\ndefinition of the firm, the “separation of ownership and control,” the “social responsibility” of\nbusiness, the definition of a “corporate objective function,” the determination of an optimal capital\nstructure, the specification of the content of credit agreements, the theory of organizations, and the\nsupply side of the completeness of markets problems.\n                                                                \n1 We do not use the term ‘capital structure’ because that term usually denotes the relative quantities of\nbonds, equity, warrants, trade credit, etc., which represent the liabilities of a firm.  Our theory implies there is\nanother important dimension to this problem—namely the relative amount of ownership claims held by\ninsiders (management) and outsiders (investors with no direct role in the management of the firm).\n\n\nElectronic copy available at: http://ssrn.com/abstract=94043Electronic copy available at: http://ssrn.com/abstract=94043\nJensen and Meckling 2 1976\nOur theory helps explain:\n1. why an entrepreneur or manager in a firm which has a mixed financial structure\n(containing both debt and outside equity claims) will choose a set of activities for the\nfirm such that the total value of the firm is less than it would be if he were the sole\nowner and why this result is independent of whether the firm operates in monopolistic\nor competitive product or factor markets;\n2. why his failure to maximize the value of the firm is perfectly consistent with\nefficiency;\n3. why the sale of common stock is a viable source of capital even though managers do\nnot literally maximize the value of the firm;\n4. why debt was relied upon as a source of capital before debt financing offered any tax\nadvantage relative to equity;\n5. why preferred stock would be issued;\n6. why accounting reports would be provided voluntarily to creditors and stockholders,\nand why independent auditors would be engaged by management to testify to the\naccuracy and correctness of such reports;\n7. why lenders often place restrictions on the activities of firms to whom they lend, and\nwhy firms would themselves be led to suggest the imposition of such restrictions;\n8. why some industries are characterized by owner-operated firms whose sole outside\nsource of capital is borrowing;\n9. why highly regulated industries such as public utilities or banks will have higher debt\nequity ratios for equivalent levels of risk than the average nonregulated firm;\n10. why security analysis can be socially productive even if it does not increase portfolio\nreturns to investors.\n\n\nJensen and Meckling 3 1976\n1.2 Theory of the Firm:  An Empty Box?\nWhile the literature of economics is replete with references to the “theory of the firm,”\nthe material generally subsumed under that heading is not actually a theory of the firm but rather a\ntheory of markets in which firms are important actors.  The firm is a “black box” operated so as\nto meet the relevant marginal conditions with respect to inputs and outputs, thereby maximizing\nprofits, or more accurately, present value.  Except for a few recent and tentative steps, however,\nwe have no theory which explains how the conflicting objectives of the individual participants are\nbrought into equilibrium so as to yield this result.  The limitations of this black box view of the firm\nhave been cited by Adam Smith and Alfred Marshall, among others.  More recently, popular and\nprofessional debates over the “social responsibility” of corporations, the separation of ownership\nand control, and the rash of reviews of the literature on the “theory of the firm” have evidenced\ncontinuing concern with these issues.2\nA number of major attempts have been made during recent years to construct a theory of\nthe firm by substituting other models for profit or value maximization, with each attempt motivated\nby a conviction that the latter is inadequate to explain managerial behavior in large corporations.3\nSome of these reformulation attempts have rejected the fundamental  principle of maximizing\n                                                                \n2 Reviews of this literature are given by Peterson (1965), Alchian (1965, 1968), Machlup (1967), Shubik (1970),\nCyert and Hedrick (1972), Branch (1973), Preston (1975).\n3 See Williamson (1964, 1970, 1975), Marris (1964), Baumol (1959), Penrose (1958), and Cyert and March\n(1963).  Thorough reviews of these and other contributions are given by Machlup (1967) and Alchian (1965).\nSimon (1955) developed a model of human choice incorporating information (search) and computational\ncosts which also has important implications for the behavior of managers.  Unfortunately, Simon’s work has\noften been misinterpreted as a denial of maximizing behavior, and misused, especially in the marketing and\nbehavioral science literature.  His later use of the term “satisficing” (Simon, 1959) has undoubtedly\ncontributed to this confusion because it suggests rejection of maximizing behavior rather than maximization\nsubject to costs of information and of decision making.\n\n\nJensen and Meckling 4 1976\nbehavior as well as rejecting the more specific profit-maximizing model.  We retain the notion of\nmaximizing behavior on the part of all individuals in the analysis that follows.4\n1.3 Property Rights\nAn independent stream of research with important implications for the theory of the firm\nhas been stimulated by the pioneering work of Coase, and extended by Alchian, Demsetz, and\nothers.5  A comprehensive survey of this literature is given by Furubotn and Pejovich (1972).\nWhile the focus of this research has been “property rights”,6 the subject matter encompassed is\nfar broader than that term suggests.  What is important for the problems addressed here is that\nspecification of individual rights determines how costs and rewards will be allocated among the\nparticipants in any organization.  Since the specification of rights is generally affected through\ncontracting (implicit as well as explicit), individual behavior in organizations, including the behavior\nof managers, will depend upon the nature of these contracts.  We focus in this paper on the\nbehavioral implications of the property rights specified in the contracts between the owners and\nmanagers of the firm.\n1.4 Agency Costs\nMany problems associated with the inadequacy of the current theory of the firm can also\nbe viewed as special cases of the theory of agency relationships in which there is a growing\n                                                                \n4 See Meckling (1976) for a discussion of the fundamental importance of the assumption of resourceful,\nevaluative, maximizing behavior on the part of individuals in the development of theory.  Klein (1976) takes\nan approach similar to the one we embark on in this paper in his review of the theory of the firm and the law.\n5 See Coase (1937, 1959, 1960), Alchian (1965, 1968), Alchian and Kessel (1962), Demsetz (1967), Alchian and\nDemsetz (1972), Monson and Downs (1965), Silver and Auster (1969), and McManus (1975).\n6 Property rights are of course human rights, i.e., rights which are possessed by human beings.  The\nintroduction of the wholly false distinction between property rights and human rights in many policy\ndiscussions is surely one of the all time great semantic flimflams.\n\n\nJensen and Meckling 5 1976\nliterature.7  This literature has developed independently of the property rights literature even\nthough the problems with which it is concerned are similar; the approaches are in fact highly\ncomplementary to each other.\nWe define an agency relationship as a contract under which one or more persons (the\nprincipal(s)) engage another person (the agent) to perform some service on their behalf which\ninvolves delegating some decision making authority to the agent.  If both parties to the relationship\nare utility maximizers, there is good reason to believe that the agent will not always act in the best\ninterests of the principal.  The principal  can limit divergences from his interest by establishing\nappropriate incentives for the agent and by incurring monitoring costs designed to limit the\naberrant activities of the agent.  In addition in some situations it will pay the agent to expend\nresources (bonding costs) to guarantee that he will not take certain actions which would harm the\nprincipal or to ensure that the principal will be compensated if he does take such actions.\nHowever, it is generally impossible for the principal or the agent at zero cost to ensure that the\nagent will make optimal decisions from the principal’s viewpoint.  In most agency relationships the\nprincipal and the agent will incur positive monitoring and bonding costs (non-pecuniary as well as\npecuniary), and in addition there will be some divergence between the agent’s decisions 8 and\nthose decisions which would maximize the welfare of the principal.  The dollar equivalent of the\nreduction in welfare experienced by the principal as a result of this divergence is also a cost of the\nagency relationship, and we refer to this latter cost as the “residual loss.”  We define agency\ncosts as the sum of:\n                                                                \n7  Cf. Berhold (1971), Ross (1973, 1974a), Wilson (1968, 1969), and Heckerman (1975).\n8  Given the optimal monitoring and bonding activities by the principal and agent.\n\n\nJensen and Meckling 6 1976\n1. the monitoring expenditures by the principal,9\n2. the bonding expenditures by the agent,\n3. the residual loss.\nNote also that agency costs arise in any situation involving cooperative effort (such as the co-\nauthoring of this paper) by two or more people even though there is no clear-cut principal-agent\nrelationship.  Viewed in this light it is clear that our definition of agency costs and their importance\nto the theory of the firm bears a close relationship to the problem of shirking and monitoring of\nteam production which Alchian and Demsetz (1972) raise in their paper on the theory of the firm.\nSince the relationship between the stockholders and the managers of a corporation fits the\ndefinition of a pure agency relationship, it should come as no surprise to discover that the issues\nassociated with the “separation of ownership and control” in the modern diffuse ownership\ncorporation are intimately associated with the general problem of agency.  We show below that an\nexplanation of why and how the agency costs generated by the corporate form are born leads to a\ntheory of the ownership (or capital) structure of the firm.\nBefore moving on, however, it is worthwhile to point out the generality of the agency\nproblem.  The problem of inducing an “agent” to behave as if he were maximizing the\n“principal’s” welfare is quite general.  It exists in all organizations and in all cooperative efforts—\nat every level of management in firms,10 in universities, in mutual companies, in cooperatives, in\n                                                                \n9 As it is used in this paper the term monitoring includes more than just measuring or observing the behavior\nof the agent.  It includes efforts on the part of the principal to ‘control’ the behavior of the agent through\nbudget restrictions, compensation policies, operating rules, etc.\n10 As we show below the existence of positive monitoring and bonding costs will result in the manager of a\ncorporation possessing control over some resources which he can allocate (within certain constraints) to\nsatisfy his own preferences.  However, to the extent that he must obtain the cooperation of others in order\nto carry out his tasks (such as divisional vice presidents) and to the extent that he cannot control their\nbehavior perfectly and costlessly they will be able to appropriate some of these resources for their own\nends.  In short, there are agency costs generated at every level of the organization.  Unfortunately, the\nanalysis of these more general organizational issues is even more difficult than that of the ‘ownership and\n\n\nJensen and Meckling 7 1976\ngovernmental authorities and bureaus, in unions, and in relationships normally classified as agency\nrelationships such as those common in the performing arts and the market for real estate.  The\ndevelopment of theories to explain the form which agency costs take in each of these situations\n(where the contractual relations differ significantly), and how and why they are born will lead to a\nrich theory of organizations which is now lacking in economics and the social sciences generally.\nWe confine our attention in this paper to only a small part of this general problem—the analysis of\nagency costs generated by the contractual arrangements between the owners and top\nmanagement of the corporation.\nOur approach to the agency problem here differs fundamentally from most of the existing\nliterature.  That literature focuses almost exclusively on the normative aspects of the agency\nrelationship; that is, how to structure the contractual relation (including compensation incentives)\nbetween the principal and agent to provide appropriate incentives for the agent to make choices\nwhich will maximize the principal’s welfare, given that uncertainty and imperfect monitoring exist.\nWe focus almost entirely on the positive aspects of the theory.  That is, we assume individuals\nsolve these normative problems, and given that only stocks and bonds can be issued as claims, we\ninvestigate the incentives faced by each of the parties and the elements entering into the\ndetermination of the equilibrium contractual form characterizing the relationship between the\nmanager (i.e., agent) of the firm and the outside equity and debt holders (i.e., principals).\n1.5 General Comments on the Definition of the firm\nRonald Coase in his seminal paper entitled “The Nature of the Firm” (1937) pointed out\nthat economics had no positive theory to determine the bounds of the firm.  He characterized the\n                                                                                                                                                                                                \ncontrol’ issue because the nature of the contractual obligations and rights of the parties are much more\nvaried and generally not as well specified in explicit contractual arrangements.  Nevertheless, they exist and\nwe believe that extensions of our analysis in these directions show promise of producing insights into a\nviable theory of organization.\n\n\nJensen and Meckling 8 1976\nbounds of the firm as that range of exchanges over which the market system was suppressed and\nwhere resource allocation was accomplished instead by authority and direction.  He focused on\nthe cost of using markets to effect contracts and exchanges and argued that activities would be\nincluded within the firm whenever the costs of using markets were greater than the costs of using\ndirect authority.  Alchian and Demsetz (1972) object to the notion that activities within the firm are\ngoverned by authority, and correctly emphasize the role of contracts as a vehicle for voluntary\nexchange.  They emphasize the role of monitoring in situations in which there is joint input or team\nproduction.11  We are sympathetic to with the importance they attach to monitoring, but we believe\nthe emphasis that Alchian and Demsetz place on joint input production is too narrow and therefore\nmisleading.  Contractual relations are the essence of the firm, not only with employees but with\nsuppliers, customers, creditors, and so on.  The problem of agency costs and monitoring exists for\nall of these contracts, independent of whether there is joint production in their sense; i.e., joint\nproduction can explain only a small fraction of the behavior of individuals associated with a firm.\nIt is important to recognize that most organizations are simply legal fictions12 which serve\nas a nexus for a set of contracting relationships among individuals.  This includes firms, non-profit\ninstitutions such as universities, hospitals, and foundations, mutual organizations such as mutual\nsavings banks and insurance companies and co-operatives, some private clubs, and even\ngovernmental bodies such as cities, states, and the federal government, government enterprises\nsuch as TVA, the Post Office, transit systems, and so forth.\n                                                                \n11 They define the classical capitalist firm as a contractual organization of inputs in which there is ‘(a) joint\ninput production, (b) several input owners, (c) one party who is common to all the contracts of the joint\ninputs, (d) who has rights to renegotiate any input’s contract independently of contracts with other input\nowners, (e) who holds the residual claim, and (f) who has the right to sell his contractual residual status.’\n12 By legal fiction we mean the artificial construct under the law which allows certain organizations to be\ntreated as individuals.\n\n\nJensen and Meckling 9 1976\nThe private corporation or firm is simply one form of legal fiction which serves as a nexus\nfor contracting relationships and which is also characterized by the existence of divisible residual\nclaims on the assets and cash flows of the organization which can generally be sold without\npermission of the other contracting individuals.  Although  this definition of the firm has little\nsubstantive content, emphasizing the essential contractual nature of firms and other organizations\nfocuses attention on a crucial set of questions—why particular sets of contractual relations arise\nfor various types of organizations, what the consequences of these contractual relations are, and\nhow they are affected by changes exogenous to the organization.  Viewed this way, it makes little\nor no sense to try to distinguish those things that are “inside” the firm (or any other organization)\nfrom those things that are “outside” of it.  There is in a very real sense only a multitude of\ncomplex relationships (i.e., contracts) between the legal fiction (the firm) and the owners of labor,\nmaterial and capital inputs and the consumers of output.13\nViewing the firm as the nexus of a set of contracting relationships among individuals also\nserves to make it clear that the personalization of the firm implied by asking questions such as\n“what should be the objective function of the firm?” or “does the firm have a social\nresponsibility?” is seriously misleading.  The firm is not an individual.  It is a legal fiction which\nserves as a focus for a complex process in which the conflicting objectives of individuals (some of\nwhom may “represent” other organizations) are brought into equilibrium within a framework of\ncontractual relations.  In this sense the “behavior” of the firm is like the behavior of a market, that\nis, the outcome of a complex equilibrium process.  We seldom fall into the trap of characterizing\n                                                                \n13 For example, we ordinarily think of a product as leaving the firm at the time it is sold, but implicitly or\nexplicitly such sales generally carry with them continuing contracts between the firm and the buyer.  If the\nproduct does not perform as expected the buyer often can and does have a right to satisfaction.  Explicit\nevidence that such implicit contracts do exist is the practice we occasionally observe of specific provision\nthat ‘all sales are final.’\n\n\nJensen and Meckling 10 1976\nthe wheat or stock market as an individual, but we often make this error by thinking about\norganizations as if they were persons with motivations and intentions.14\n1.6  Overview of the Paper\nWe develop our theory in stages.  Sections 2 and 4 provide analyses of the agency costs\nof equity and debt respectively.  These form the major foundation of the theory.  In Section 3, we\npose some questions regarding the existence of the corporate form of organization and examines\nthe role of limited liability.  Section 5 provides a synthesis of the basic concepts derived in sections\n2-4 into a theory of the corporate ownership structure which takes account of the trade-offs\navailable to the entrepreneur-manager between inside and outside equity and debt.  Some\nqualifications and extensions of the analysis are discussed in section 6, and section 7 contains a\nbrief summary and conclusions.\n2.  The Agency Costs of Outside Equity\n2.1 Overview\nIn this section we analyze the effect of outside equity on agency costs by comparing the\nbehavior of a manager when he owns 100 percent of the residual claims on a firm with his\nbehavior when he sells off a portion of those claims to outsiders.  If a wholly-owned firm is\nmanaged by the owner, he will make operating decisions that maximize his utility.  These decisions\n                                                                \n14 This view of the firm points up the important role which the legal system and the law play in social\norganizations, especially, the organization of economic activity.  Statutory laws sets bounds on the kinds of\ncontracts into which individuals and organizations may enter without risking criminal prosecution.  The\npolice powers of the state are available and used to enforce performance of contracts or to enforce the\ncollection of damages for non-performance. The courts adjudicate conflicts between contracting parties and\nestablish precedents which form the body of common law.  All of these government activities affect both the\nkinds of contracts executed and the extent to which contracting is relied upon.  This in turn determines the\nusefulness, productivity, profitability and viability of various forms of organization.  Moreover, new laws as\nwell as court decisions often can and do change the rights of contracting parties ex post, and they can and\ndo serve as a vehicle for redistribution of wealth.  An analysis of some of the implications of these facts is\ncontained in Jensen and Meckling (1978) and we shall not pursue them here.\n\n\nJensen and Meckling 11 1976\nwill involve not only the benefits he derives from pecuniary returns but also the utility generated by\nvarious non-pecuniary aspects of his entrepreneurial activities such as the physical appointments\nof the office, the attractiveness of the office staff, the level of employee discipline, the kind and\namount of charitable contributions, personal relations (“friendship,” “respect,” and so on) with\nemployees, a larger than optimal computer to play with, or purchase of production inputs from\nfriends.  The optimum mix (in the absence of taxes) of the various pecuniary and non-pecuniary\nbenefits is achieved when the marginal utility derived from an additional dollar of expenditure\n(measured net of any productive effects) is equal for each non-pecuniary item and equal to the\nmarginal utility derived from an additional dollar of after-tax purchasing power (wealth).\nIf the owner-manager sells equity claims on the corporation which are identical to his own\n(i.e., which share proportionately in the profits of the firm and have limited liability), agency costs\nwill be generated by the divergence between his interest and those of the outside shareholders,\nsince he will then bear only a fraction of the costs of any non-pecuniary benefits he takes out in\nmaximizing his own utility.  If the manager owns only 95 percent of the stock, he will expend\nresources to the point where the marginal utility derived from a dollar’s expenditure of the firm’s\nresources on such items equals the marginal utility of an additional 95 cents in general purchasing\npower (i.e., his share of the wealth reduction) and not one dollar.  Such activities, on his part, can\nbe limited (but probably not eliminated) by the expenditure of resources on monitoring activities by\nthe outside stockholders.  But as we show below, the owner will bear the entire wealth effects of\nthese expected costs so long as the equity market anticipates these effects.  Prospective minority\nshareholders will realize that the owner-manager’s interests will diverge somewhat from theirs;\nhence the price which they will pay for shares will reflect the monitoring costs and the effect of\nthe divergence between the manager’s interest and theirs.  Nevertheless, ignoring for the moment\nthe possibility of borrowing against his wealth, the owner will find it desirable to bear these costs\n\n\nJensen and Meckling 12 1976\nas long as the welfare increment he experiences from converting his claims on the firm into\ngeneral purchasing power15 is large enough to offset them.\nAs the owner-manager’s fraction of the equity falls, his fractional claim on the outcomes\nfalls and this will tend to encourage him to appropriate larger amounts of the corporate resources\nin the form of perquisites. This also makes it desirable for the minority shareholders to expend\nmore resources in monitoring his behavior.  Thus, the wealth costs to the owner of obtaining\nadditional cash in the equity markets rise as his fractional ownership falls.\nWe shall continue to characterize the agency conflict between the owner-manager and\noutside shareholders as deriving from the manager’s tendency to appropriate perquisites out of the\nfirm’s resources for his own consumption.  However, we do not mean to leave the impression that\nthis is the only or even the most important source of conflict.  Indeed, it is likely that the most\nimportant conflict arises from the fact that as the manager’s ownership claim falls, his incentive to\ndevote significant effort to creative activities such as searching out new profitable ventures falls.\nHe may in fact avoid such ventures simply because it requires too much trouble or effort on his\npart to manage or to learn about new technologies.  Avoidance of these personal costs and the\nanxieties that go with them also represent a source of on-the-job utility to him and it can result in\nthe value of the firm being substantially lower than it otherwise could be.\n2.2 A Simple Formal Analysis of the Sources of Agency Costs of Equity and Who Bears Them\nIn order to develop some structure for the analysis to follow we make two sets of\nassumptions.  The first set (permanent assumptions) are those which will carry through almost all\nof the analysis in sections 2-5.  The effects of relaxing some of these are discussed in section 6.\n                                                                \n15 For use in consumption, for the diversification of his wealth, or more importantly, for the financing of\n‘profitable’ projects which he could not otherwise finance out of his personal wealth.  We deal with these\nissues below after having developed some of the elementary analytical tools necessary to their solution.\n\n\nJensen and Meckling 13 1976\nThe second set (temporary assumptions) are made only for expositional purposes and are relaxed\nas soon as the basic points have been clarified.\nPermanent assumptions\n(P.1) All taxes are zero.\n(P.2) No trade credit is available.\n(P.3) All outside equity shares are non-voting.\n(P.4) No complex financial claims such as convertible bonds or preferred  stock or\nwarrants can be issued.\n(P.5) No outside owner gains utility from ownership in a firm in any way other than\nthrough its effect on his wealth or cash flows.\n(P.6) All dynamic aspects of the multiperiod nature of the problem are ignored by\nassuming there is only one production-financing decision to be made by the\nentrepreneur.\n(P.7) The entrepreneur-manager’s money wages are held constant throughout the\nanalysis.\n(P.8) There exists a single manager (the peak coordinator) with ownership interest in\nthe firm.\nTemporary assumptions\n(T.1) The size of the firm is fixed.\n(T.2) No monitoring or bonding activities are possible.\n(T.3) No debt financing through bonds, preferred stock, or personal borrowing (secured\nor unsecured) is possible.\n\n\nJensen and Meckling 14 1976\n(T.4) All elements  of the owner-manager’s decision problem involving portfolio\nconsiderations induced by the presence of uncertainty and the existence of\ndiversifiable risk are ignored.\nDefine:\nX = {x1, x2, . . .,x n} = vector of quantities of all factors and activities within the\nfirm from which the manager derives non-pecuniary benefits;16 the xi are defined\nsuch that his marginal utility is positive for each of them;\nC(X) = total dollar cost of providing any given amount of these items;\nP(X) = total dollar value to the firm of the productive benefits of X;\nB(X) = P(X)-C(X) = net dollar benefit to the firm of X ignoring any effects of X on\nthe equilibrium wage of the manager.\nIgnoring the effects of X on the manager’s utility and therefore on his equilibrium wage\nrate, the optimum levels of the factors and activities X are defined by X* such\nthat\n¶B( X*)\n¶X *  =  ¶P( X*)\n¶X *  −  ¶C(X*)\n¶X *  =  0.\nThus for any vector X ≥ X* (i.e., where at least one element of X is greater than its\ncorresponding element of X*), F ≡ B(X*) - B(X)  > 0 measures the dollar cost to the firm (net of\nany productive effects) of providing the increment X - X*  of the factors and activities which\ngenerate utility to the manager.  We assume henceforth that for any given level of cost to the firm,\nF, the vector of factors and activities on which F is spent on those, ˆ X , which yield the manager\nmaximum utility.  Thus F ≡ B(X*) - B( ˆ X ).\n                                                                \n16  Such as office space, air conditioning, thickness of the carpets, friendliness of employee relations, etc.\n\n\nJensen and Meckling 15 1976\nWe have thus far ignored in our discussion the fact that these expenditures on X occur\nthrough time and therefore there are trade-offs to be made across time as well as between\nalternative elements of X.  Furthermore, we have ignored the fact that the future expenditures are\nlikely to involve uncertainty (i.e., they are subject to probability distributions) and therefore some\nallowance must be made for their riskiness.  We resolve both of these issues by defining C, P, B,\nand F to be the current market values  of the sequence of probability distributions on the period-\nby-period cash flows involved.17\nGiven the definition of F as the current market value of the stream of manager’s\nexpenditures on non-pecuniary benefits, we represent the constraint which a single owner-\nmanager faces in deciding how much non-pecuniary income he will extract from the firm by the\nline \nV F  in fig. 1.  This is analogous to a budget constraint.  The market value of the firm is\nmeasured along the vertical axis and the market value of the manager’s stream of expenditures on\nnon-pecuniary benefits, F, is measured along the horizontal axis.  OV  is the value of the firm\nwhen the amount of non-pecuniary income consumed is zero.  By definition V  is the maximum\nmarket value of the cash flows generated by the firm for a given money wage for the manager\nwhen the manager’s consumption of non-pecuniary benefits are zero.  At this point all the factors\nand activities within the firm which generate utility for the manager are at the level X* defined\nabove.  There is a different budget constraint \nV F  for each possible scale of the firm (i.e., level of\ninvestment, I) and for alternative levels of money wage, W, for the manager.  For the moment we\npick an arbitrary level of investment (which we assume has already been made) and hold the\nscale of the firm constant at this level.  We also assume that the manager’s money wage is fixed\n                                                                \n17 And again we assume that for any given market value of these costs, F, to the firm the allocation across\ntime and across alternative probability distributions is such that the manager’s current expected utility is at a\nmaximum.\n\n\nJensen and Meckling 16 1976\nat the level W* which represents the current market value of his wage contract 18 in the optimal\ncompensation package which consists of both wages, W*, and non-pecuniary benefits, F*.  Since\none dollar of current value of non-pecuniary benefits withdrawn from the firm by the manager\nreduces the market value of the firm by $1, by definition, the slope of V F  is -1.\nThe owner-manager’s tastes for wealth and non-pecuniary benefits is represented in fig.\n1 by a system of indifference curves, U1, U2, and so on.19  The indifference curves will be convex\nas drawn as long as the owner-manager’s marginal rate of substitution between non-pecuniary\nbenefits and wealth diminishes with increasing levels of the benefits. For the 100 percent owner-\nmanager, this presumes that there are not perfect substitutes for these benefits available on the\noutside, that is, to some extent they are job-specific.  For the fractional owner-manager this\npresumes that the benefits cannot be turned into general purchasing power at a constant price.20\nWhen the owner has 100 percent of the equity, the value of the firm will be V* where\nindifference curve U2 is tangent to VF, and the level of non-pecuniary benefits consumed is F*.\nIf the owner sells the entire equity but remains as manager, and if the equity buyer can, at zero\n                                                                \n18 At this stage when we are considering a 100% owner-managed firm the notion of a ‘wage contract’ with\nhimself has no content.  However, the 100% owner-managed case is only an expositional device used in\npassing to illustrate a number of points in the analysis, and we ask the reader to bear with us briefly while\nwe lay out the structure for the more interesting partial ownership case where such a contract does have\nsubstance.\n19 The manager’s utility function is actually defined over wealth and the future time sequence of vectors of\nquantities of non-pecuniary benefits, Xt.  Although the setting of his problem is somewhat different, Fama\n(1970b, 1972) analyzes the conditions under which these preferences can be represented as a derived utility\nfunction defined as a function of the money value of the expenditures (in our notation F) on these goods\nconditional on the prices of goods.  Such a utility function incorporates the optimization going on in the\nbackground which define \nˆ X  discussed above for a given F.  In the more general case where we allow a time\nseries of consumption, ˆ X t, the optimization is being carried out across both time and the components of Xt\nfor fixed F.\n20 This excludes, for instance, (a) the case where the manager is allowed to expend corporate resources on\nanything he pleases in which case F would be a perfect substitute for wealth, or (b) the case where he can\n‘steal’ cash (or other marketable assets) with constant returns to scale—if he could the indifference curves\nwould be straight lines with slope determined by the fence commission.\n\n\nJensen and Meckling 17 1976\ncost, force the old owner (as manager) to take the same level of non-pecuniary benefits as he did\nas owner, then V* is the price the new owner will be willing to pay for the entire equity.21\nFig. 1. The value of the firm (V) and the level of non-pecuniary benefits consumed (F) when the fraction\nof outside equity is (1- α)V, and Uj(j = 1,2,3) represents owner’s indifference curves between wealth and\nnon-pecuniary benefits.\n                                                                \n21 Point D defines the fringe benefits in the optimal pay package since the value to the manager of the fringe\nbenefits F* is greater than the cost of providing them as is evidenced by the fact that U2 is steeper to the\nleft of D than the budget constraint with slope equal to -1.\nThat D is indeed the optimal pay package can easily be seen in this situation since if the conditions of the\nsale to a new owner specified that the manager would receive no fringe benefits after the sale he would\nrequire a payment equal to V3 to compensate him for the sacrifice of his claims to V* and fringe benefits\namounting to F* (the latter with total value to him of V3-V*).  But if F = 0 , the value of the firm is only V .\nTherefore, if monitoring costs were zero the sale would take place at V* with provision for a pay package\nwhich included fringe benefits of F* for the manager.\nThis discussion seems to indicate there are two values for the ‘firm’, V3 and V*.  This is not the case if we\nrealize that V* is the value of the right to be the residual claimant on the cash flows of the firm and V3-V* is\nthe value of the managerial rights, i.e., the right to make the operating decisions which include access to F*.\nThere is at least one other right which has value which plays no formal role in the analysis as yet—the value\nof the control right.  By control right we mean the right to hire and fire the manager and we leave this issue\nto a future paper.\n\n\nJensen and Meckling 18 1976\nIn general, however, we could not expect the new owner to be able to enforce identical\nbehavior on the old owner at zero costs.  If the old owner sells a fraction of the firm to an\noutsider, he, as manager, will no longer bear the full cost of any non-pecuniary benefits he\nconsumes.  Suppose the owner sells a share of the firm, 1-α, (0 < α < 1) and retains for himself a\nshare, a.  If the prospective buyer believes that the owner-manager will consume the same level\nof non-pecuniary benefits as he did as full owner, the buyer will be willing to pay (1-α)V* for a\nfraction (1-α) of the equity.  Given that an outsider now holds a claim to (1- α) of the equity,\nhowever, the cost to the owner-manager of consuming $1 of non-pecuniary benefits in the firm\nwill no longer be $1.  Instead, it will be α x $1.  If the prospective buyer actually paid (1-α)V* for\nhis share of the equity, and if thereafter the manager could choose whatever level of non-\npecuniary benefits he liked, his budget constraint would be V1P1 in fig. 1 and has a slope equal to -\nα,  Including the payment the owner receives from the buyer as part of the owner’s post-sale\nwealth, his budget constraint, V1P1, must pass through D, since he can if he wishes have the same\nwealth and level of non-pecuniary consumption he enjoyed as full owner.\nBut if the owner-manager is free to choose the level of perquisites, F, subject only to the\nloss in wealth he incurs as a part owner, his welfare will be maximized by increasing his\nconsumption of non-pecuniary benefits.  He will move to point A where V1P1 is tangent to U1\nrepresenting a higher level of utility.  The value of the firm falls from V*, to V0, that is, by the\namount of the cost to the firm of the increased non-pecuniary expenditures, and the owner-\nmanager’s consumption of non-pecuniary benefits rises from F* to F0.\nIf the equity market is characterized by rational expectations the buyers will be aware that\nthe owner will increase his non-pecuniary consumption when his ownership share is reduced.  If\nthe owner’s response function is known or if the equity market makes unbiased estimates of the\n\n\nJensen and Meckling 19 1976\nowner’s response to the changed incentives, the buyer will not pay (1- α)V* for (1- α) of the\nequity.\nTheorem.  For a claim on the firm of (1-a) the outsider will pay only (1-a) times the\nvalue he expects the firm to have given the induced change in the behavior of the owner-manager.\nProof.  For simplicity we ignore any element of uncertainty introduced by the lack of\nperfect knowledge of the owner-manager’s response function.  Such uncertainty will not affect\nthe final solution if the equity market is large as long as the estimates are rational (i.e., unbiased)\nand the errors are independent across firms.  The latter condition assures that this risk is\ndiversifiable and therefore that equilibrium prices will equal the expected values.\nLet W represent the owner’s total wealth after he has sold a claim equal to 1- α of the\nequity to an outsider.  W has two components.  One is the payment, So, made by the outsider for\n1-α of the equity; the rest, Si, is the value of the owner’s (i.e., insider’s) share of the firm, so that\nW, the owner’s wealth, is given by\nW  = So + Si = So + αV(F, α),\nwhere V(F, α) represents the value of the firm given that the manager’s fractional\nownership share is α and that he consumes perquisites with current market value of F.  Let V2P2,\nwith a slope of - α represent the trade-off the owner-manager faces between non-pecuniary\nbenefits and his wealth after the sale.  Given that the owner has decided to sell a claim 1-α of the\nfirm, his welfare will be maximized when V2P2 is tangent to some indifference curve such as U3 in\nfig. 1.  A price for a claim of (1-α) on the firm that is satisfactory to both the buyer and the seller\nwill require that this tangency occur along \nV F , that is, that the value of the firm must be V’.  To\nshow this, assume that such is not the case—that the tangency occurs to the left of the point B on\nthe line V F .  Then, since the slope of V2P2 is negative, the value of the firm will be larger than V’.\n\n\nJensen and Meckling 20 1976\nThe owner-manager’s choice of this lower level of consumption of non-pecuniary benefits will\nimply a higher value both to the firm as a whole and to the fraction of the firm (1- α) which the\noutsider has acquired; that is, (1-α)V’ > So.  From the owner’s viewpoint, he has sold 1-α of the\nfirm for less than he could have, given the (assumed) lower level of non-pecuniary benefits he\nenjoys.  On the other hand, if the tangency point B is to the right of the line V F , the owner-\nmanager’s higher consumption of non-pecuniary benefits means the value of the firm is less than\nV’, and hence (1-α)V(F, α) < So = (1-α)V’.  The outside owner then has paid more for his share\nof the equity than it is worth.  So will be a mutually satisfactory price if and only if (1-α)V’ = So.\nBut this means that the owner’s post-sale wealth is equal to the (reduced) value of the firm V’,\nsince\nW = So + aV’ = (1-α)V’ + aV’ = V’.\nQ.E.D.\nThe requirement that V’ and F’ fall on V F  is thus equivalent to requiring that the value of\nthe claim acquired by the outside buyer be equal to the amount he pays for it, and conversely for\nthe owner.  This means that the decline in the total value of the firm (V*-V’) is entirely\nimposed on the owner-manager .  His total wealth after the sale of (1-α) of the equity is V’ and\nthe decline in his wealth is V*-V’.\nThe distance V*-V’ is the reduction in the market value of the firm engendered by the\nagency relationship and is a measure of the “residual loss” defined earlier.  In this simple example\nthe residual loss represents the total agency costs engendered by the sale of outside equity\nbecause monitoring and bonding activities have not been allowed.  The welfare loss the owner\nincurs is less than the residual loss by the value to him of the increase in non-pecuniary benefits\n(F’-F*).  In fig. 1 the difference between the intercepts on the Y axis of the two indifference\n\n\nJensen and Meckling 21 1976\ncurves U2 and U3 is a measure of the owner-manager’s welfare loss due to the incurrence of\nagency costs,22 and he would sell such a claim only if the increment in welfare he achieved by\nusing the cash amounting to (1-α)V’ for other things was worth more to him than this amount of\nwealth.\n2.3 Determination of the Optimal Scale of the Firm\nThe case of all equity financing .  Consider the problem faced by an entrepreneur with\ninitial pecuniary wealth, W, and monopoly access to a project requiring investment outlay, I, subject\nto diminishing returns to scale in I.  Fig. 2 portrays the solution to the optimal scale of the firm\ntaking into account the agency costs associated with the existence of outside equity.  The axes are\nas defined in fig. 1 except we now plot on the vertical axis the total wealth of the owner, that is,\nhis initial wealth, W, plus V(I)-I, the net increment in wealth he obtains from exploitation of his\ninvestment opportunities.  The market value of the firm, V = V(I,F), is now a function of the level\nof investment, I, and the current market value of the manager’s expenditures of the firm’s\nresources on non-pecuniary benefits, F.  Let \nV (I)  represent the value of the firm as a function of\nthe level of investment when the manager’s expenditures on non-pecuniary benefits, F, are zero.\nThe schedule with intercept labeled W  +  [V (I *) − I*)]  and slope equal to -1 in fig. 2 represents the\nlocus of combinations of post-investment wealth and dollar cost to thefirm of non-pecuniary\nbenefits which are available to the manager when investment is carried to the value maximizing\npoint, I*.  At this point Δ\nV (I ) − ΔI  =  0 .  If the manager’s wealth were large enough to cover the\ninvestment required to reach this scale of operation, I*, he would consume F* in non-pecuniary\n                                                                \n22 The distance V*-V’ is a measure of what we will define as the gross agency costs.  The distance V3-V4 is a\nmeasure of what we call net agency costs, and it is this measure of agency costs which will be minimized by\nthe manager in the general case where we allow investment to change.\n\n\nJensen and Meckling 22 1976\nbenefits and have pecuniary wealth with value W + V*-I*.  However, if outside financing is\nrequired to cover the investment he will not reach this point if monitoring costs are non-zero.23\nThe expansion path OZBC represents the equilibrium combinations of wealth and non-\npecuniary benefits, F, which the manager could obtain if he had enough personal wealth to finance\nall levels of investment up to I*.  It is the locus of points such as Z and C which present the\nequilibrium position for the 100 percent owner-manager at each possible level of investment, I.  As\nI increases we move up the expansion path to the point C where V(I)-I is at a maximum.\nAdditional investment beyond this point reduces the net value of the firm, and as it does the\nequilibrium path of the manager’s wealth and non-pecuniary benefits retraces (in the reverse\ndirection) the curve OZBC.  We draw the path as a smooth concave function only as a matter of\nconvenience.\nFig. 2. Determination of the optimal scale of the firm in the case where no monitoring takes place.  Point C\ndenotes optimum investment, I*, and non-pecuniary benefits, F*, when investment is 100% financed by\nentrepreneur.  Point D denotes optimum investment, I’, and non-pecuniary benefits, F, when outside equity\nfinancing is used to help finance the investment and the entrepreneur owns a fraction α‘ of the firm.  The\ndistance A measures the gross agency costs.\n                                                                \n23 I* is the value maximizing and Pareto Optimum investment level which results from the traditional analysis\nof the corporate investment decision if the firm operates in perfectly competitive capital and product markets\nand the agency cost problems discussed here are ignored.  See Debreu (1959, ch. 7), Jensen and Long\n(1972), Long (1972), Merton and Subrahmanyam (1974), Hirshleifer (1958, 1970), and Fama and Miller (1972).\n\n\nJensen and Meckling 23 1976\nIf the manager obtained outside financing and if there were zero costs to the agency\nrelationship (perhaps because monitoring costs were zero), the expansion path would also be\nrepresented by OZBC.  Therefore, this path represents what we might call the “idealized”\nsolutions, that is, those which would occur in the absence of agency costs.\nAssume the manager has sufficient personal wealth to completely finance the firm only up\nto investment level I1, which puts him at point Z.  At this point W = I1.  To increase the size of the\nfirm beyond this point he must obtain outside financing to cover the additional investment required,\nand this means reducing his fractional ownership. When he does this he incurs agency costs, and\nthe lower his ownership fraction, the larger are the agency costs he incurs.  However, if the\ninvestments requiring outside financing are sufficiently profitable his welfare will continue to\nincrease.\nThe expansion path ZEDHL in fig. 2 portrays one possible path of the equilibrium levels of\nthe owner’s non-pecuniary benefits and wealth at each possible level of investment higher than I1.\nThis path is the locus of points such as E or D where (1) the manager’s indifference curve is\ntangent to a line with slope equal to -α (his fractional claim on the firm at that level of investment),\nand (2) the tangency occurs on the “budget constraint” with slope = -1 for the firm value and non-\npecuniary benefit trade-off at the same level of investment. 24  As we move along ZEDHL his\n                                                                \n24 Each equilibrium point such as that at E is characterized by ( ˆ a , ˆ F ,\nc\nWt ) where \nc\nWt  is the entrepreneur’s\npost-investment financing wealth.  Such an equilibrium must satisfy each of the following four conditions:\n(1)\nc\nWt + F  =  V (I) + W − I  =  V (I) − K,\nwhere K ≡ I-W is the amount of outside financing required to make the investment I.  If this condition is not\nsatisfied there is an uncompensated wealth transfer (in one direction or the other) between the entrepreneur\nand outside equity buyers.\n(2) FU (\nc\nWt , ˆ F )/ WtU (\nc\nWt , ˆ F )  =  ˆ a ,\nwhere U is the entrepreneur’s utility function on wealth and perquisites, FU  and  WtU  are marginal utilities\nand ˆ a  is the manager’s share of the firm.\n\n\nJensen and Meckling 24 1976\nfractional claim on the firm continues to fall as he raises larger amounts of outside capital.  This\nexpansion path represents his complete opportunity set for combinations of wealth and non-\npecuniary benefits, given the existence of the costs of the agency relationship with the outside\nequity holders.  Point D, where this opportunity set is tangent to an indifference curve, represents\nthe solution which maximizes his welfare.  At this point, the level of investments is I’, his fractional\nownership share in the firm is a‘, his wealth is W+V’-I’, and he consumes a stream of non-\npecuniary benefits with current market value of F’.  The gross agency costs (denoted by A) are\nequal to (V*-I*)-(V’-I’).  Given that no monitoring is possible, I’ is the socially optimal level of\ninvestment as well as the privately optimal level.\nWe can characterize the optimal level of investment as that point, I’ which satisfies the\nfollowing condition for small changes:\nV - I + α‘F = 0 (1)\nV-I is the change in the net market value of the firm, and a‘F is the dollar value to\nthe manager of the incremental fringe benefits he consumes (which cost the firm F dollars).25\nFurthermore, recognizing that V  =  \nV − F,  where  V  is the value of the firm at any level of\ninvestment when F = 0, we can substitute into the optimum condition to get\n                                                                                                                                                                                                \n(3) (1 − ˆ a )V(I)  =  (1 − ˆ a )[V (I) − ˆ F ] ≥  K,\nwhich says the funds received from outsiders are at least equal to K, the minimum required outside\nfinancing.\n(4) Among all points ( ˆ a , ˆ F ,\nc\nWt ) satisfying conditions (1)-(3), (a, F, tW )  gives the manager highest utility.\nThis implies that ( ˆ a , ˆ F ,\nc\nWt ) satisfy condition (3) as an equality.\n25 Proof.  Note that the slope of the expansion path (or locus of equilibrium points) at any point is (V-\nI)/F and at the optimum level of investment this must be equal to the slope of the manager’s indifference\ncurve between wealth and market value of fringe benefits, F.  Furthermore, in the absence of monitoring, the\nslope of the indifference curve, W½F, at the equilibrium point, D, must be equal to -α‘.  Thus,\n(V-I)/F = -α‘ (2)\n\n\nJensen and Meckling 25 1976\n( ΔV − ΔI) −  1−a' )ΔF  =  0 (3)\nas an alternative expression for determining the optimum level of investment.\nThe idealized or zero agency cost solution, I*, is given by the condition ( ΔV − ΔI)  =  0 ,\nand since F is positive the actual welfare maximizing level of investment I’ will be less than I*,\nbecause ( ΔV − ΔI)   must be positive at I’ if (3) is to be satisfied.  Since - α‘ is the slope of the\nindifference curve at the optimum and therefore represents the manager’s demand price for\nincremental non-pecuniary benefits, F, we know that α‘F is the dollar value to him of an\nincrement of fringe benefits costing the firm F dollars.  The term (1- α‘)F thus measures the\ndollar “loss” to the firm (and himself) of an additional F dollars spent on non-pecuniary benefits.\nThe term Δ V − ΔI   is the gross increment in the value of the firm ignoring any changes in the\nconsumption of non-pecuniary benefits.  Thus, the manager stops increasing the size of the firm\nwhen the gross increment in value is just offset by the incremental “loss” involved in the\nconsumption of additional fringe benefits due to his declining fractional interest in the firm.26\n                                                                                                                                                                                                \nis the condition for the optimal scale of investment and this implies condition (1) holds for small changes at\nthe optimum level of investment, I’.\n26 Since the manager’s indifference curves are negatively sloped we know that the optimum scale of the firm,\npoint D, will occur in the region where the expansion path has negative slope, i.e., the market value of the\nfirm, will be declining and the gross agency costs, A, will be increasing and thus, the manager will not\nminimize them in making the investment decision (even though he will minimize them for any given  level of\ninvestment).  However, we define the net agency cost as the dollar equivalent of the welfare loss the\nmanager experiences because of the agency relationship evaluated at F = 0 (the vertical distance between\nthe intercepts on the Y axis of the two indifference curves on which points C and D lie).  The optimum\nsolution, I’, does satisfy the condition that net agency costs are minimized.  But this simply amounts to a\nrestatement of the assumption that the manager maximizes his welfare.\nFinally, it is possible for the solution point D to be a corner solution and in this case the value of the firm\nwill not be declining.  Such a corner solution can occur, for instance, if the manager’s marginal rate of\nsubstitution between F and wealth falls to zero fast enough as we move up the expansion path, or if the\ninvestment projects are “sufficiently” profitable.  In these cases the expansion path will have a corner which\nlies on the maximum value budget constraint with intercept V (I*) − I * , and the level of investment will be\nequal to the idealized optimum, I*.  However, the market value of the residual claims will be less than V*\nbecause the manager’s consumption of perquisites will be larger than F*, the zero agency cost level.\n\n\nJensen and Meckling 26 1976\n2.4 The Role of Monitoring and Bonding Activities in Reducing Agency Costs\nIn the above analysis we have ignored the potential for controlling the behavior of the\nowner-manager through monitoring and other control activities.  In practice, it is usually possible\nby expending resources to alter the opportunity the owner-manager has for capturing non-\npecuniary benefits. These methods include auditing, formal control systems, budget restrictions,\nthe establishment of incentive compensation systems which serve to identify the manager’s\ninterests more closely with those of the outside equity holders, and so forth.  Fig. 3 portrays the\neffects of monitoring and other control activities in the simple situation portrayed in fig. 1.  Figs. 1\nand 3 are identical except for the curve BCE in fig. 3 which depicts a “budget constraint” derived\nwhen monitoring possibilities are taken into account.  Without monitoring, and with outside equity\nof (1- α), the value of the firm will be V’ and non-pecuniary expenditures F’.  By incurring\nmonitoring costs, M, the equity holders can restrict the manager’s consumption of perquisites to\namounts less than F’.  Let F(M, α) denote the maximum perquisites the manager can consume\nfor alternative levels of monitoring expenditures, M, given his ownership share α.  We assume\nthat increases in monitoring reduce F, and reduce it at a decreasing rate, that is, ¶F/¶M < 0 and\n¶2F/¶M2 > 0.\nSince the current value of expected future monitoring expenditures by the outside equity\nholders reduce the value of any given claim on the firm to them dollar for dollar, the outside equity\nholders will take this into account in determining the maximum price they will pay for any given\nfraction of the firm’s equity.  Therefore, given positive monitoring activity the value of the firm is\ngiven by V  =  \nV − F(M,a) − M  and the locus of these points for various levels of M and for a\ngiven level of α lie on the line BCE in fig. 3.  The vertical difference between the V F  and BCE\ncurves is M, the current market value of the future monitoring expenditures.\n\n\nJensen and Meckling 27 1976\nIf it is possible for the outside equity holders to make these monitoring expenditures and\nthereby to impose the reductions in the owner-manager’s consumption of F, he will voluntarily\nenter into a contract with the outside equity holders which gives them the rights to restrict his\nconsumption of non-pecuniary items to F”.  He finds this desirable because it will cause the value\nof the firm to rise to V”  Given the contract, the optimal monitoring expenditure on the part of the\noutsiders, M, is the amount D-C.  The entire increase in the value of the firm that accrues will be\nreflected in the owner’s wealth, but his welfare will be increased by less than this because he\nforgoes some non-pecuniary benefits he previously enjoyed.\nFig. 3. The value of the firm (V) and level of non-pecuniary benefits (F) when outside equity is (1- α), U1,\nU2, U3 represent owner’s indifference curves between wealth and non-pecuniary benefits, and monitoring\n(or bonding) activities impose opportunity set BCE as the tradeoff constraint facing the owner .\n\n\nJensen and Meckling 28 1976\nIf the equity market is competitive and makes unbiased estimates of the effects of\nmonitoring expenditures on F and V, potential buyers will be indifferent between the following two\ncontracts:\nPurchase of a share (1-α) of the firm at a total price of (1- α)V’ and no rights to monitor\nor control the manager’s consumption of perquisites.\nPurchase of a share (1-α) of the firm at a total price of (1- α)V” and the right to expend\nresources up to an amount equal to D-C which will limit the owner-manager’s consumption of\nperquisites to F”.\nGiven the contract (ii) the outside shareholders would find it desirable to monitor to the full\nrights of their contract because it will pay them to do so.  However, if the equity market is\ncompetitive the total benefits (net of the monitoring costs) will be capitalized into the price of the\nclaims.  Thus, not surprisingly, the owner-manager reaps all the benefits of the opportunity to write\nand sell the monitoring contract.27\nAn analysis of bonding expenditures .  We can also see from the analysis of fig. 3 that\nit makes no difference who actually makes the monitoring expenditures—the owner bears the full\namount of these costs as a wealth reduction in all cases.  Suppose that the owner-manager could\nexpend resources to guarantee to the outside equity holders that he would limit his activities which\n                                                                \n27 The careful reader will note that point C will be the equilibrium point only if the contract between the\nmanager and outside equity holders specifies with no ambiguity that they have the right to monitor to limit\nhis consumption of perquisites to an amount no less than F”.  If any ambiguity regarding these rights exists\nin this contract then another source of agency costs arises which is symmetrical to our original problem.  If\nthey could do so the outside equity holders would monitor to the point where the net value of their\nholdings, (1-α)V-M, was maximized, and this would occur when (¶V/¶M)(1-α)-1 = 0 which would be at some\npoint between points C and E in fig. 3.  Point E denotes the point where the value of the firm net of the\nmonitoring costs is at a maximum, i.e., where ¶V/¶M-1 = 0.  But the manager would be worse off than in the\nzero monitoring solution if the point where (1- α)V-M was at a maximum were to the left of the intersection\nbetween BCE and the indifference curve U3 passing through point B (which denotes the zero monitoring\nlevel of welfare).  Thus if the manager could not eliminate enough of the ambiguity in the contract to push\nthe equilibrium to the right of the intersection of the curve BCE with indifference curve U3 he would not\nengage in any contract which allowed monitoring.\n\n\nJensen and Meckling 29 1976\ncost the firm F.  We call these expenditures “bonding costs,” and they would take such forms as\ncontractual guarantees to have the financial accounts audited by a public account, explicit bonding\nagainst malfeasance on the part of the manager, and contractual limitations on the manager’s\ndecision-making power (which impose costs on the firm because they limit his ability to take full\nadvantage of some profitable opportunities as well as limiting his ability to harm the stockholders\nwhile making himself better off).\nIf the incurrence of the bonding costs were entirely under the control of the manager and\nif they yielded the same opportunity set BCE for him in fig. 3, he would incur them in amount D-C.\nThis would limit his consumption of perquisites to F”from F’, and the solution is exactly the same\nas if the outside equity holders had performed the monitoring.  The manager finds it in his interest\nto incur these costs as long as the net increments in his wealth which they generate (by reducing\nthe agency costs and therefore increasing the value of the firm) are more valuable than the\nperquisites given up.  This optimum occurs at point C in both cases under our assumption that the\nbonding expenditures yield the same opportunity set as the monitoring expenditures.  In general, of\ncourse, it will pay the owner-manager to engage in bonding activities and to write contracts which\nallow monitoring as long as the marginal benefits of each are greater than their marginal cost.\nOptimal scale of the firm in the presence of monitoring and bonding activities .  If\nwe allow the outside owners to engage in (costly) monitoring activities to limit the manager’s\nexpenditures on non-pecuniary benefits and allow the manager to engage in bonding activities to\nguarantee to the outside owners that he will limit his consumption of F we get an expansion path\nsuch as that illustrated in fig. 4 on which Z and G lie.  We have assumed in drawing fig. 4 that the\ncost functions involved in monitoring and bonding are such that some positive levels of the\nactivities are desirable, i.e., yield benefits greater than their cost.  If this is not true the expansion\npath generated by the expenditure of resources on these activities would lie below ZD and no such\n\n\nJensen and Meckling 30 1976\nactivity would take place at any level of investment.  Points Z, C, and D and the two expansion\npaths they lie on are identical to those portrayed in fig. 2.  Points Z and C lie on the 100 percent\nownership expansion path, and points Z and D lie on the fractional ownership, zero monitoring and\nbonding activity expansion path.\nThe path on which points Z and G lie is the one given by the locus of equilibrium points for\nalternative levels of investment characterized by the point labeled C in fig. 3 which denotes the\noptimal level of monitoring and bonding activity and resulting values of the firm and non-pecuniary\nbenefits to the manager given a fixed level of investment.  If any monitoring or bonding is cost\neffective the expansion path on which Z and G lie must be above the non-monitoring expansion\npath over some range.  Furthermore, if it lies anywhere to the right of the indifference curve\npassing through point D (the zero monitoring-bonding solution) the final solution to the problem will\ninvolve positive amounts of monitoring and/or bonding activities.  Based on the discussion above\nwe know that as long as the contracts between the manager and outsiders are unambiguous\nregarding the rights of the respective parties the final solution will be at that point where the new\nexpansion path is just tangent to the highest indifference curve.  At this point the optimal level of\nmonitoring and bonding expenditures are M” and b”; the manager’s post-investment-financing\nwealth is given by W + V”-I”-M”-b” and his non-pecuniary benefits are F”.  The total gross\nagency costs, A, are given by A(M”, b”, α“, I”) = (V*-I*) - (V”-I”-M”-b”).\n2.5 Pareto Optimality and Agency Costs in Manager-Operated Firms\nIn general we expect to observe both bonding and external monitoring activities, and the\nincentives are such that the levels of these activities will satisfy the conditions of efficiency. They\nwill not, however, result in the firm being run in a manner so as to maximize its value.  The\ndifference between V*, the efficient solution under zero monitoring and bonding costs (and\n\n\nJensen and Meckling 31 1976\ntherefore zero agency costs), and V”, the value of the firm given positive monitoring costs, are the\ntotal gross agency costs defined earlier in the introduction.  These are the costs of the “separation\nof ownership and control” which Adam Smith focused on in the passage quoted at the beginning\nof this paper and which Berle and Means (1932) popularized 157 years later. The solutions\noutlined above to our highly simplified problem imply that agency costs will be positive as long as\nmonitoring costs are positive—which they certainly are.\nFig. 4. Determination of optimal scale of the firm allowing for monitoring and bonding activities.  Optimal\nmonitoring costs are M” and bonding costs are b” and the equilibrium scale of firm, manager’s wealth and\nconsumption of non-pecuniary benefits are at point G.\nThe reduced value of the firm caused by the manager’s consumption of perquisites\noutlined above is “non-optimal” or inefficient only in comparison to a world in which we could\nobtain compliance of the agent to the principal’s wishes at zero cost or in comparison to a\nhypothetical  world in which the agency costs were lower.  But these costs (monitoring and\nbonding costs and ‘residual loss’) are an unavoidable result of the agency relationship.\n\n\nJensen and Meckling 32 1976\nFurthermore, since they are borne entirely by the decision maker (in this case the original owner)\nresponsible for creating the relationship he has the incentives to see that they are minimized\n(because he captures the benefits from their reduction).  Furthermore, these agency costs will be\nincurred only if the benefits to the owner-manager from their creation are great enough to\noutweigh them.  In our current example these benefits arise from the availability of profitable\ninvestments requiring capital investment in excess of the original owner’s personal wealth.\nIn conclusion, finding that agency costs are non-zero (i.e., that there are costs associated\nwith the separation of ownership and control in the corporation) and concluding therefrom that the\nagency relationship is non-optimal, wasteful or inefficient is equivalent in every sense to comparing\na world in which iron ore is a scarce commodity (and therefore costly) to a world in which it is\nfreely available at zero resource costs, and concluding that the first world is “non-optimal”—a\nperfect example of the fallacy criticized by Coase (1964) and what Demsetz (1969) characterizes\nas the “Nirvana” form of analysis.28\n2.6 Factors Affecting the Size of the Divergence from Ideal Maximization\nThe magnitude of the agency costs discussed above will vary from firm to firm.  It will\ndepend on the tastes of managers, the ease with which they can exercise their own preferences\nas opposed to value maximization in decision making, and the costs of monitoring and bonding\nactivities.29  The agency costs will also depend upon the cost of measuring the manager’s\n(agent’s) performance and evaluating it, the cost of devising and applying an index for\n                                                                \n28 If we could establish the existence of a feasible set of alternative institutional arrangements which would\nyield net benefits from the reduction of these costs we could legitimately conclude the agency relationship\nengendered by the corporation was not Pareto optimal.  However, we would then be left with the problem of\nexplaining why these alternative institutional arrangements have not replaced the corporate form of\norganization.\n29 The monitoring and bonding costs will differ from firm to firm depending on such things as the inherent\ncomplexity and geographical dispersion of operations, the attractiveness of perquisites available in the firm\n(consider the mint), etc.\n\n\nJensen and Meckling 33 1976\ncompensating the manager which correlates with the owner’s (principal’s) welfare, and the cost\nof devising and enforcing specific behavioral rules or policies.  Where the manager has less than a\ncontrolling interest in the firm, it will also depend upon the market for managers.  Competition\nfrom other potential managers limits the costs of obtaining managerial services (including the\nextent to which a given manager can diverge from the idealized solution which would obtain if all\nmonitoring and bonding costs were zero).  The size of the divergence (the agency costs) will be\ndirectly related to the cost of replacing the manager.  If his responsibilities require very little\nknowledge specialized to the firm, if it is easy to evaluate his performance, and if replacement\nsearch costs are modest, the divergence from the ideal will be relatively small and vice versa.\nThe divergence will also be constrained by the market for the firm itself, i.e., by capital\nmarkets.  Owners always have the option of selling their firm, either as a unit or piecemeal.\nOwners of manager-operated firms can and do sample the capital market from time to time.  If\nthey discover that the value of the future earnings stream to others is higher than the value of the\nfirm to them given that it is to be manager-operated, they can exercise their right to sell.  It is\nconceivable that other owners could be more efficient at monitoring or even that a single individual\nwith appropriate managerial talents and with sufficiently large personal wealth would elect to buy\nthe firm.  In this latter case the purchase by such a single individual would completely eliminate\nthe agency costs.  If there were a number of such potential owner-manager purchasers (all with\ntalents and tastes identical to the current manager) the owners would receive in the sale price of\nthe firm the full value of the residual claimant rights including the capital value of the eliminated\nagency costs plus the value of the managerial rights.\nMonopoly, competition and managerial behavior .  It is frequently argued that the\nexistence of competition in product (and factor) markets will constrain the behavior of managers\nto idealized value maximization, i.e., that monopoly in product (or monopsony in factor) markets\n\n\nJensen and Meckling 34 1976\nwill permit larger divergences from value maximization.30  Our analysis does not support this\nhypothesis. The owners of a firm with monopoly power have the same incentives to limit\ndivergences of the manager from value maximization (i.e., the ability to increase their wealth) as\ndo the owners of competitive firms.  Furthermore, competition in the market for managers will\ngenerally make it unnecessary for the owners to share rents with the manager.  The owners of a\nmonopoly firm need only pay the supply price for a manager.\nSince the owner of a monopoly has the same wealth incentives to minimize managerial\ncosts as would the owner of a competitive firm, both will undertake that level of monitoring which\nequates the marginal cost of monitoring to the marginal wealth increment from reduced\nconsumption of perquisites by the manager.  Thus, the existence of monopoly will not increase\nagency costs.\nFurthermore the existence of competition in product and factor markets will not eliminate\nthe agency costs due to managerial control problems as has often been asserted (cf. Friedman,\n1970).  If my competitors all incur agency costs equal to or greater than mine I will not be\neliminated from the market by their competition.\nThe existence and size of the agency costs depends on the nature of the monitoring costs,\nthe tastes of managers for non-pecuniary benefits and the supply of potential managers who are\ncapable of financing the entire venture out of their personal wealth.  If monitoring costs are zero,\n                                                                \n30 Where competitors are numerous and entry is easy, persistent departures from profit maximizing behavior\ninexorably leads to extinction.  Economic natural selection holds the stage.  In these circumstances, the\nbehavior of the individual units that constitute the supply side of the product market is essentially routine\nand uninteresting and economists can confidently predict industry behavior without being explicitly\nconcerned with the behavior of these individual units.\nWhen the conditions of competition are relaxed, however, the opportunity set of the firm is expanded.  In\nthis case, the behavior of the firm as a distinct operating unit is of separate interest.  Both for purposes of\ninterpreting particular behavior within the firm as well as for predicting responses of the industry aggregate,\nit may be necessary to identify the factors that influence the firm’s choices within this expanded opportunity\nset and embed these in a formal model (Williamson, 1964, p. 2).\n\n\nJensen and Meckling 35 1976\nagency costs will be zero or if there are enough 100 percent owner-managers available to own\nand run all the firms in an industry (competitive or not) then agency costs in that industry will also\nbe zero.31\n3. Some unanswered questions regarding the existence of the corporate form\n3.1  The question\nThe analysis to this point has left us with a basic puzzle:  Why, given the existence of\npositive costs of the agency relationship, do we find the usual corporate form of organization with\nwidely diffuse ownership so widely prevalent?  If one takes seriously much of the literature\nregarding the “discretionary” power held by managers of large corporations, it is difficult to\nunderstand the historical fact of enormous growth in equity in such organizations, not only in the\nUnited States, but throughout the world.  Paraphrasing Alchian (1968):  How does it happen that\nmillions of individuals are willing to turn over a significant fraction of their wealth to organizations\nrun by managers who have so little interest in their welfare?  What is even more remarkable, why\nare they willing to make these commitments purely as residual claimants, i.e., on the anticipation\nthat managers will operate the firm so that there will be earnings which accrue to the\nstockholders?\nThere is certainly no lack of alternative ways that individuals might invest, including\nentirely different forms of organizations.  Even if consideration is limited to corporate\norganizations, there are clearly alternative ways capital might be raised, i.e., through fixed claims\nof various sorts, bonds, notes, mortgages, etc.  Moreover, the corporate income tax seems to favor\nthe use of fixed claims since interest is treated as a tax deductible expense.  Those who assert\n                                                                \n31 Assuming there are no special tax benefits to ownership nor utility of ownership other than that derived\nfrom the direct wealth effects of ownership such as might be true for professional sports teams, race horse\nstables, firms which carry the family name, etc.\n\n\nJensen and Meckling 36 1976\nthat managers do not behave in the interest of stockholders have generally not addressed a very\nimportant question:  Why, if non-manager-owned shares have such a serious deficiency, have they\nnot long since been driven out by fixed claims?32\n3.2 Some alternative explanations of the ownership structure of the firm\nThe role of limited liability .  Manne (1967) and Alchian and Demsetz (1972) argue that\none of the attractive features of the corporate form vis-à-vis individual proprietorships or\npartnerships is the limited liability feature of equity claims in corporations.  Without this provision\neach and every investor purchasing one or more shares of a corporation would be potentially liable\nto the full extent of his personal wealth for the debts of the corporation.  Few individuals would\nfind this a desirable risk to accept and the major benefits to be obtained from risk reduction\nthrough diversification would be to a large extent unobtainable.  This argument, however, is\nincomplete since limited liability does not eliminate the basic risk, it merely shifts it.  The argument\nmust rest ultimately on transaction costs.  If all stockholders of GM were liable for GM’s debts,\nthe maximum liability for an individual shareholder would be greater than it would be if his shares\nhad limited liability.  However, given that many other stockholders also existed and that each was\nliable for the unpaid claims in proportion to his ownership it is highly unlikely that the maximum\npayment each would have to make would be large in the event of GM’s bankruptcy since the total\nwealth of those stockholders would also be large.  However, the existence of unlimited liability\nwould impose incentives for each shareholder to keep track of both the liabilities of GM and the\nwealth of the other GM owners.  It is easily conceivable that the costs of so doing would, in the\naggregate, be much higher than simply paying a premium in the form of higher interest rates to the\ncreditors of GM in return for their acceptance of a contract which grants limited liability to the\n                                                                \n32 Marris (1964, pp. 7-9) is the exception, although he argues that there exists some ‘maximum leverage point’\n\n\nJensen and Meckling 37 1976\nshareholders.  The creditors would then bear the risk of any non-payment of debts in the event of\nGM’s bankruptcy.\nIt is also not generally recognized that limited liability is merely a necessary condition for\nexplaining the magnitude of the reliance on equities, not a sufficient condition.  Ordinary debt also\ncarries limited liability.33  If limited liability is all that is required, why don’t we observe large\ncorporations, individually owned, with a tiny fraction of the capital supplied by the entrepreneur,\nand the rest simply borrowed.34  At first this question seems silly to many people (as does the\nquestion regarding why firms would ever issue debt or preferred stock under conditions where\nthere are no tax benefits obtained from the treatment of interest or preferred dividend\npayments.35)  We have found that oftentimes this question is misinterpreted to be one regarding\nwhy firms obtain capital.  The issue is not why they obtain capital, but why they obtain it through\n                                                                                                                                                                                                \nbeyond which the chances of “insolvency” are in some undefined sense too high.\n33 By limited liability we mean the same conditions that apply to common stock.  Subordinated debt or\npreferred stock could be constructed which carried with it liability provisions; i.e., if the corporation’s assets\nwere insufficient at some point to pay off all prior claims (such as trade credit, accrued wages, senior debt,\netc.) and if the personal resources of the ‘equity’ holders were also insufficient to cover these claims the\nholders of this ‘debt’ would be subject to assessments beyond the face value of their claim (assessments\nwhich might be limited or unlimited in amount).\n34 Alchian-Demsetz (1972, p. 709) argue that one can explain the existence of both bonds and stock in the\nownership structure of firms as the result of differing expectations regarding the outcomes to the firm.  They\nargue that bonds are created and sold to ‘pessimists’ and stocks with a residual claim with no upper bound\nare sold to ‘optimists.’\nAs long as capital markets are perfect with no taxes or transactions costs and individual investors can\nissue claims on distributions of outcomes on the same terms as firms, such actions on the part of firms\ncannot affect their values. The reason is simple.  Suppose such ‘pessimists’ did exist and yet the firm issues\nonly equity claims.  The demand for those equity claims would reflect the fact that the individual purchaser\ncould on his own account issue ‘bonds’ with a limited and prior claim on the distribution of outcomes on the\nequity which is exactly the same as that which the firm could issue.  Similarly, investors could easily unlever\nany position by simply buying a proportional claim on both the bonds and stocks of a levered firm.\nTherefore, a levered firm could not sell at a different price than an unlevered firm solely because of the\nexistence of such differential expectations. See Fama and Miller (1972, ch. 4) for an excellent exposition of\nthese issues.\n35 Corporations did use both prior to the institution of the corporate income tax in the United States and\npreferred dividends have, with minor exceptions, never been tax deductible.\n\n\nJensen and Meckling 38 1976\nthe particular forms we have observed for such long periods of time.  The fact is that no well\narticulated answer to this question currently exists in the literature of either finance or economics.\nThe “irrelevance” of capital structure .  In their pathbreaking article on the cost of\ncapital, Modigliani and Miller (1958) demonstrated that in the absence of bankruptcy costs and tax\nsubsidies on the payment of interest the value of the firm is independent of the financial structure.\nThey later (1963) demonstrated that the existence of tax subsidies on interest payments would\ncause the value of the firm to rise with the amount of debt financing by the amount of the\ncapitalized value of the tax subsidy. But this line of argument implies that the firm should be\nfinanced almost entirely with debt.  Realizing the inconsistence with observed behavior, Modigliani\nand Miller (1963, p. 442) comment:\nIt may be useful to remind readers once again that the existence of a tax advantage for\ndebt financing . . . does not necessarily mean that corporations should at all times seek to use the\nmaximum amount of debt in their capital structures . . . there are as we pointed out, limitations\nimposed by lenders . . . as well as many other dimensions (and kinds of costs) in real-world\nproblems of financial strategy which are not fully comprehended within the framework of static\nequilibrium models, either our own or those of the traditional variety.  These additional\nconsiderations, which are typically grouped under the rubric of “the need for preserving\nflexibility”, will normally imply the maintenance by the corporation of a substantial reserve of\nuntapped borrowing power.\nModigliani and Miller are essentially left without a theory of the determination of the\noptimal capital structure, and Fama and Miller (1972, p. 173)  commenting on the same issue\nreiterate this conclusion:\n\n\nJensen and Meckling 39 1976\nAnd we must admit that at this point there is little in the way of convincing research,\neither theoretical or empirical, that explains the amounts of debt that firms do decide to have in\ntheir capital structure.\nThe Modigliani-Miller theorem is based on the assumption that the probability distribution\nof the cash flows to the firm is independent of the capital structure.  It is now recognized that the\nexistence of positive costs associated with bankruptcy and the presence of tax subsidies on\ncorporate interest payments will invalidate this irrelevance theorem precisely because the\nprobability distribution of future cash flows changes as the probability of the incurrence of the\nbankruptcy costs changes, i.e., as the ratio of debt to equity rises. We believe the existence of\nagency costs provide stronger reasons for arguing that the probability distribution of future cash\nflows is not independent of the capital or ownership structure.\nWhile the introduction of bankruptcy costs in the presence of tax subsidies leads to a\ntheory which defines an optimal capital structure, 36 we argue that this theory is seriously\nincomplete since it implies that no debt should ever be used in the absence of tax subsidies if\nbankruptcy costs are positive.  Since we know debt was commonly used prior to the existence of\nthe current tax subsidies on interest payments this theory does not capture what must be some\nimportant determinants of the corporate capital structure.\nIn addition, neither bankruptcy costs nor the existence of tax subsidies can explain the use\nof preferred stock or warrants which have no tax advantages, and there is no theory which tells us\nanything about what determines the fraction of equity claims held by insiders as opposed to\noutsiders which our analysis in section 2 indicates is so important.  We return to these issues later\nafter analyzing in detail the factors affecting the agency costs associated with debt.\n                                                                \n36 See Kraus and Litzenberger (1973) and Lloyd-Davies (1975).\n\n\nJensen and Meckling 40 1976\n4.    The Agency Costs of Debt\nIn general if the agency costs engendered by the existence of outside owners are positive\nit will pay the absentee owner (i.e., shareholders) to sell out to an owner-manager who can avoid\nthese costs.37  This could be accomplished in principle by having the manager become the sole\nequity holder by repurchasing all of the outside equity claims with funds obtained through the\nissuance of limited liability debt claims and the use of his own personal wealth.  This single-owner\ncorporation would not suffer the agency costs associated with outside equity.  Therefore there\nmust be some compelling reasons why we find the diffuse-owner corporate firm financed by\nequity claims so prevalent as an organizational form.\nAn ingenious entrepreneur eager to expand, has open to him the opportunity to design a\nwhole hierarchy of fixed claims on assets and earnings, with premiums paid for different levels of\nrisk.38  Why don’t we observe large corporations individually owned with a tiny fraction of the\ncapital supplied by the entrepreneur in return for 100 percent of the equity and the rest simply\nborrowed?  We believe there are a number of reasons:  (1) the incentive effects associated with\nhighly leveraged firms, (2) the monitoring costs these incentive effects engender, and (3)\nbankruptcy costs.  Furthermore, all of these costs are simply particular aspects of the agency\ncosts associated with the existence of debt claims on the firm.\n                                                                \n37 And if there is competitive bidding for the firm from potential owner-managers the absentee owner will\ncapture the capitalized value of these agency costs.\n38 The spectrum of claims which firms can issue is far more diverse than is suggested by our two-way\nclassification—fixed vs. residual.  There are convertible bonds, equipment trust certificates, debentures,\nrevenue bonds, warrants, etc.  Different bond issues can contain different subordination provisions with\nrespect to assets and interest. They can be callable or non-callable.  Preferred stocks can be ‘preferred’ in a\nvariety of dimensions and contain a variety of subordination stipulations.  In the abstract, we can imagine\nfirms issuing claims contingent on a literally infinite variety of states of the world such as those considered\nin the literature on the time-state-preference models of Arrow (1964b), Debreu (1959) and Hirshleifer (1970).\n\n\nJensen and Meckling 41 1976\n4.1  The Incentive Effects Associated with Debt\nWe don’t find many large firms financed almost entirely with debt-type claims (i.e., non-\nresidual claims) because of the effect such a financial structure would have on the owner-\nmanager’s behavior.  Potential creditors will not loan $100,000,000 to a firm in which the\nentrepreneur has an investment of $10,000.  With that financial structure the owner-manager will\nhave a strong incentive to engage in activities (investments) which promise very high payoffs if\nsuccessful even if they have a very low probability of success.  If they turn out well, he captures\nmost of the gains, if they turn out badly, the creditors bear most of the costs.39\nTo illustrate the incentive effects associated with the existence of debt and to provide a\nframework within which we can discuss the effects of monitoring and bonding costs, wealth\ntransfers, and the incidence of agency costs, we again consider a simple situation.  Assume we\nhave a manager-owned firm with no debt outstanding in a world in which there are no taxes.  The\nfirm has the opportunity to take one of two mutually exclusive equal cost investment opportunities,\neach of which yields a random payoff, \nX j ,T  periods in the future ( j = 1,2).  Production and\nmonitoring activities take place continuously between time 0 and time T, and markets in which the\nclaims on the firm can be traded are open continuously over this period.  After time T the firm has\nno productive activities so the payoff X j  includes the distribution of all remaining assets.  For\nsimplicity, we assume that the two distributions are log-normally distributed and have the same\nexpected total payoff, E(X ), where X  is defined as the logarithm of the final payoff.  The\ndistributions differ only by their variances with 12s < 22s .  The systematic or covariance risk of\neach of the distributions, bj, in the Sharpe (1964)-Lintner (1965) capital asset pricing model, is\n                                                                \n39 An apt analogy is the way one would play poker on money borrowed at a fixed interest rate, with one’s\nown liability limited to some very small stake.  Fama and Miller (1972, pp. 179-180) also discuss and provide a\nnumerical example of an investment decision which illustrates very nicely the potential inconsistency\nbetween the interests of bondholders and stockholders.\n\n\nJensen and Meckling 42 1976\nassumed to be identical.  Assuming that asset prices are determined according to the capital asset\npricing model, the preceding assumptions imply that the total market value of each of these\ndistributions is identical, and we represent this value by V.\nIf the owner-manager has the right to decide which investment program to take, and if\nafter he decides this he has the opportunity to sell part or all of his claims on the outcomes in the\nform of either debt or equity, he will be indifferent between the two investments.40\nHowever, if the owner has the opportunity to first issue debt, then to decide which of the\ninvestments to take, and then to sell all or part of his remaining equity claim on the market, he will\nnot be indifferent between the two investments. The reason is that by promising to take the low\nvariance project, selling bonds and then taking the high variance project he can transfer wealth\nfrom the (naive) bondholders to himself as equity holder.\nLet X* be the amount of the “fixed” claim in the form of a non-coupon bearing bond sold\nto the bondholders such that the total payoff to them Rj(j = 1, 2, denotes the distribution the\nmanager chooses), is\nj\nR  =  X*,   if   X j  ≥  X*,\n      =  jX ,   if jX  ≤  X *.\nLet B1 be the current market value of bondholder claims if investment 1 is taken, and let\nB2 be the current market value of bondholders claims if investment 2 is taken.  Since in this\nexample the total value of the firm, V, is independent of the investment choice and also of the\nfinancing decision we can use the Black-Scholes (1973) option pricing model to determine the\n                                                                \n40 The portfolio diversification issues facing the owner-manager are brought into the analysis in section 5\nbelow.\n\n\nJensen and Meckling 43 1976\nvalues of the debt, Bj, and equity, S j, under each of the choices. 41  Black-Scholes derive the\nsolution for the value of a European call option (one which can be exercised only at the maturity\ndate) and argue that the resulting option pricing equation can be used to determine the value of the\nequity claim on a leveraged firm.  That is the stockholders in such a firm can be viewed as holding\na European call option on the total value of the firm with exercise price equal to X* (the face\nvalue of the debt), exercisable at the maturity date of the debt issue.  More simply, the\nstockholders have the right to buy the firm back from the bondholders for a price of X* at time T.\nMerton (1973, 1974) shows that as the variance of the outcome distribution rises the value of the\nstock (i.e., call option) rises, and since our two distributions differ only in their variances, 2\n2s > 12s ,\nthe equity value S1 is less than S2.  This implies B1 > B2, since B1 = V-S1, and B2 = V-S2.\nNow if the owner-manager could sell bonds with face value X* under the conditions that\nthe potential bondholders believed this to be a claim on distribution 1, he would receive a price of\nB1.  After selling the bonds, his equity interest in distribution 1 would have value S1.  But we know\nS2 is greater than S1 and thus the manager can make himself better off by changing the investment\nto take the higher variance distribution 2, thereby redistributing wealth from the bondholders to\nhimself.  All this assumes of course that the bondholders could not prevent him from changing the\ninvestment program.  If the bondholders cannot do so, and if they perceive that the manager\nhas the opportunity to take distribution 2 they will pay the manager only B 2 for the claim\nX*, realizing that his maximizing behavior will lead him to choose distribution 2.  In this\nevent there is no redistribution of wealth between bondholders and stockholders (and in general\nwith rational expectations there never will be) and no welfare loss.  It is easy to construct a case,\nhowever, in which these incentive effects do generate real costs.\n                                                                \n41 See Smith (1976) for a review of this option pricing literature and its applications and Galai and Masulis\n(1976) who apply the option pricing model to mergers, and corporate investment decisions.\n\n\nJensen and Meckling 44 1976\nLet cash flow distribution 2 in the previous example have an expected value, E(X2), which\nis lower than that of distribution 1.  Then we know that V1 > V2, and if V, which is given by\nV = V1-V2 = (S1-S2) + (B1-B2),\nis sufficiently small relative to the reduction in the value of the bonds the value of the stock will\nincrease.42  Rearranging the expression for V we see that the difference between the equity\nvalues for the two investments is given by\nS2-S1 = (B1-B2) - (V1-V2),\nand the first term on the RHS, ( B1-B2), is the amount of wealth “transferred” from the\nbondholders and V1-V2 is the reduction in overall firm value.  Since we know B1 > B2), S2-S1  can\nbe positive even though the reduction in the value of the firm, V1-V2, is positive.43  Again, the\nbondholders will not actually lose as long as they accurately perceive the motivation of the equity\nowning manager and his opportunity to take project 2.  They will presume he will take investment\n2, and hence will pay no more than B2 for the bonds when they are issued.\n                                                                \n42 While we used the option pricing model above to motivate the discussion and provide some intuitive\nunderstanding of the incentives facing the equity holders, the option pricing solutions of Black and Scholes\n(1973) do not apply when incentive effects cause V to be a function of the debt/equity ratio as it is in general\nand in this example.  Long (1974) points out this difficulty with respect to the usefulness of the model in the\ncontext of tax subsidies on interest and bankruptcy cost. The results of Merton (1974) and Galai and\nMasulis (1976) must be interpreted with care since the solutions are strictly incorrect in the context of tax\nsubsidies and/or agency costs.\n43 The numerical example of Fama and Miller (1972, pp. 179-180) is a close representation of this case in a\ntwo-period state model. However, they go on to make the following statement on p. 180:\nFrom a practical viewpoint, however, situations of potential conflict between bondholders and\nshareholders in the application of the market value rule are probably unimportant.  In general, investment\nopportunities that increase a firm’s market value by more than their cost both increase the value of the firm’s\nshares and strengthen the firm’s future ability to meet its current bond commitments.\nThis first issue regarding the importance of the conflict of interest between bondholders and stockholders\nis an empirical one, and the last statement is incomplete—in some circumstances the equity holders could\nbenefit from projects whose net effect was to reduce the total value of the firm as they and we have\nillustrated.  The issue cannot be brushed aside so easily.\n\n\nJensen and Meckling 45 1976\nIn this simple example the reduced value of the firm, V1-V2, is the agency cost engendered\nby the issuance of debt44 and it is borne by the owner-manager.  If he could finance the project\nout of his personal wealth, he would clearly choose project 1 since its investment outlay was\nassumed equal to that of project 2 and its market value, V1, was greater.  This wealth loss, V1-V2,\nis the “residual loss” portion of what we have defined as agency costs and it is generated by the\ncooperation required to raise the funds to make the investment.  Another important part of the\nagency costs are monitoring and bonding costs and we now consider their role.\n4.2 The Role of Monitoring and Bonding Costs\nIn principle it would be possible for the bondholders, by the inclusion of various covenants\nin the indenture provisions, to limit the managerial behavior which results in reductions in the value\nof the bonds.  Provisions which impose constraints on management’s decisions regarding such\nthings as dividends, future debt issues,45 and maintenance of working capital are not uncommon in\nbond issues.46  To completely protect the bondholders from the incentive effects, these provisions\nwould have to be incredibly detailed and cover most operating aspects of the enterprise including\nlimitations on the riskiness of the projects undertaken.  The costs involved in writing such\nprovisions, the costs of enforcing them and the reduced profitability of the firm (induced because\nthe covenants occasionally limit management’s ability to take optimal actions on certain issues)\n                                                                \n44 Myers (1975) points out another serious incentive effect on managerial decisions of the existence of debt\nwhich does not occur in our simple single decision world.  He shows that if the firm has the option to take\nfuture investment opportunities the existence of debt which matures after the options must be taken will\ncause the firm (using an equity value maximizing investment rule) to refuse to take some otherwise profitable\nprojects because they would benefit only the bondholders and not the equity holders.  This will (in the\nabsence of tax subsidies to debt) cause the value of the firm to fall.  Thus (although he doesn’t use the term)\nthese incentive effects also contribute to the agency costs of debt in a manner perfectly consistent with the\nexamples discussed in the text.\n45 Black-Scholes (1973) discuss ways in which dividend and future financing policy can redistribute wealth\nbetween classes of claimants on the firm.\n\n\nJensen and Meckling 46 1976\nwould likely be non-trivial.  In fact, since management is a continuous decision-making process it\nwill be almost impossible to completely specify such conditions without having the bondholders\nactually perform the management function. All costs associated with such covenants are what we\nmean by monitoring costs.\nThe bondholders will have incentives to engage in the writing of such covenants and in\nmonitoring the actions of the manager to the point where the “nominal” marginal cost to them of\nsuch activities is just equal to the marginal benefits they perceive from engaging in them. We use\nthe word nominal here because debtholders will not in fact bear these costs. As long as they\nrecognize their existence, they will take them into account in deciding the price they will pay for\nany given debt claim,47 and therefore the seller of the claim (the owner) will bear the costs just as\nin the equity case discussed in section 2.\nIn addition the manager has incentives to take into account the costs imposed on the firm\nby covenants in the debt agreement which directly affect the future cash flows of the firm since\nthey reduce the market value of his claims.  Because both the external and internal monitoring\ncosts are imposed on the owner-manager it is in his interest to see that the monitoring is\nperformed in the lowest cost way.  Suppose, for example, that the bondholders (or outside equity\nholders) would find it worthwhile to produce detailed financial statements such as those contained\nin the usual published accounting reports as a means of monitoring the manager.  If the manager\nhimself can produce such information at lower costs than they (perhaps because he is already\ncollecting much of the data they desire for his own internal decision-making purposes), it would\n                                                                                                                                                                                                \n46 Black, Miller and Posner (1978) discuss many of these issues with particular reference to the government\nregulation of bank holding companies.\n47 In other words, these costs will be taken into account in determining the yield to maturity on the issue.\nFor an examination of the effects of such enforcement costs on the nominal interest rates in the consumer\nsmall loan market, see Benston (1977).\n\n\nJensen and Meckling 47 1976\npay him to agree in advance to incur the cost of providing such reports and to have their accuracy\ntestified to by an independent outside auditor.  This is an example of what we refer to as bonding\ncosts.48,49\n                                                                \n48 To illustrate the fact that it will sometimes pay the manager to incur ‘bonding’ costs to guarantee the\nbondholders that he will not deviate from his promised behavior let us suppose that for an expenditure of $ b\nof the firm’s resources he can guarantee that project 1 will be chosen.  If he spends these resources and\ntakes project 1 the value of the firm will be V1-b and clearly as long as ( V1-b) > V2, or alternatively ( V1-V2) > b\nhe will be better off, since his wealth will be equal to the value of the firm minus the required investment, I\n(which we assumed for simplicity to be identical for the two projects).\nOn the other hand, to prove that the owner-manager prefers the lowest cost solution to the conflict let us\nassume he can write a covenant into the bond issue which will allow the bondholders to prevent him from\ntaking project 2, if they incur monitoring costs of $ m, where m < b.  If he does this his wealth will be higher\nby the amount b-m.  To see this note that if the bond market is competitive and makes unbiased estimates,\npotential bondholders will be indifferent between:\n  (i) a claim X* with no covenant (and no guarantees from management) at a price of B2,\n (ii) a claim X* with no covenant (and guarantees from management, through bonding expenditures\nby the firm of $b, that project 1 will be taken) at a price of B1, and\n(iii) a claim X* with a covenant and the opportunity to spend m on monitoring (to guarantee\nproject 1 will be taken) at a price of B1-m.\nThe bondholders will realize that (i) represents in fact a claim on project 2 and that (ii) and (iii) represent a\nclaim on project 1 and are thus indifferent between the three options at the specified prices.  The owner-\nmanager, however, will not be indifferent between incurring the bonding costs, b, directly, or including the\ncovenant in the bond indenture and letting the bondholders spend m to guarantee that he take project 1.\nHis wealth in the two cases will be given by the value of his equity plus the proceeds of the bond issue less\nthe required investment, and if m < b < V1-V2, then his post-investment-financing wealth, W, for the three\noptions will be such that Wi < Wii < Wiii.  Therefore, since it would increase his wealth, he would voluntarily\ninclude the covenant in the bond issue and let the bondholders monitor.\n49 We mention, without going into the problem in detail, that similar to the case in which the outside equity\nholders are allowed to monitor the manager-owner, the agency relationship between the bondholders and\nstockholders has a symmetry if the rights of the bondholders to limit actions of the manager are not\nperfectly spelled out.  Suppose the bondholders, by spending sufficiently large amounts of resources, could\nforce management to take actions which would transfer wealth from the equity holder to the bondholders\n(by taking sufficiently less risky projects).  One can easily construct situations where such actions could\nmake the bondholders better off, hurt the equity holders and actually lower the total value of the firm.  Given\nthe nature of the debt contract the original owner-manager might maximize his wealth in such a situation by\nselling off the equity and keeping the bonds as his ‘owner’s’ interest.  If the nature of the bond contract is\ngiven, this may well be an inefficient solution since the total agency costs (i.e., the sum of monitoring and\nvalue loss) could easily be higher than the alternative solution.  However, if the owner-manager could\nstrictly limit the rights of the bondholders (perhaps by inclusion of a provision which expressly reserves all\nrights not specifically granted to the bondholder for the equity holder), he would find it in his interest to\nestablish the efficient contractual arrangement since by minimizing the agency costs he would be maximizing\nhis wealth.  These issues involve the fundamental nature of contracts and for now we simply assume that\nthe ‘bondholders’ rights are strictly limited and unambiguous and all rights not specifically granted them are\nreserved for the ‘stockholders’; a situation descriptive of actual institutional arrangements.  This allows us\nto avoid the incentive effects associated with “bondholders” potentially exploiting ‘stockholders.’\n\n\nJensen and Meckling 48 1976\n4.3 Bankruptcy and Reorganization Costs\nWe argue in section 5 that as the debt in the capital structure increases beyond some point\nthe marginal agency costs of debt begin to dominate the marginal agency costs of outside equity\nand the result of this is the generally observed phenomenon of the simultaneous use of both debt\nand outside equity.  Before considering these issues, however, we consider here the third major\ncomponent of the agency costs of debt which helps to explain why debt doesn’t completely\ndominate capital structures—the existence of bankruptcy and reorganization costs.\nIt is important to emphasize that bankruptcy and liquidation are very different events.  The\nlegal definition of bankruptcy is difficult to specify precisely.  In general, it occurs when the firm\ncannot meet a current payment on a debt obligation, 50 or one or more of the other indenture\nprovisions providing for bankruptcy is violated by the firm. In this event the stockholders have lost\nall claims on the firm,51 and the remaining loss, the difference between the face value of the fixed\nclaims and the market value of the firm, is borne by the debtholders.  Liquidation of the firm’s\nassets will occur only if the market value of the future cash flows generated by the firm is less\nthan the opportunity cost of the assets, i.e., the sum of the values which could be realized if the\nassets were sold piecemeal.\nIf there were no costs associated with the event called bankruptcy the total market value\nof the firm would not be affected by increasing the probability of its incurrence.  However, it is\ncostly, if not impossible, to write contracts representing claims on a firm which clearly delineate\nthe rights of holders for all possible contingencies.  Thus even if there were no adverse incentive\n                                                                \n50 If the firm were allowed to sell assets to meet a current debt obligation, bankruptcy would occur when the\ntotal market value of the future cash flows expected to be generated by the firm is less than the value of a\ncurrent payment on a debt obligation.  Many bond indentures do not, however, allow for the sale of assets\nto meet debt obligations. \n\nJensen and Meckling 49 1976\neffects in expanding fixed claims relative to equity in a firm, the use of such fixed claims would be\nconstrained by the costs inherent in defining and enforcing those claims.  Firms incur obligations\ndaily to suppliers, to employees, to different classes of investors, etc.  So long as the firm is\nprospering, the adjudication of claims is seldom a problem.  When the firm has difficulty meeting\nsome of its obligations, however, the issue of the priority of those claims can pose serious\nproblems. This is most obvious in the extreme case where the firm is forced into bankruptcy.  If\nbankruptcy were costless, the reorganization would be accompanied by an adjustment of the\nclaims of various parties and the business, could, if that proved to be in the interest of the\nclaimants, simply go on (although perhaps under new management).52\nIn practice, bankruptcy is not costless, but generally involves an adjudication process\nwhich itself consumes a fraction of the remaining value of the assets of the firm.  Thus the cost of\nbankruptcy will be of concern to potential buyers of fixed claims in the firm since their existence\nwill reduce the payoffs to them in the event of bankruptcy.  These are examples of the agency\ncosts of cooperative efforts among individuals (although in this case perhaps “non-cooperative”\nwould be a better term). The price buyers will be willing to pay for fixed claims will thus be\ninversely related to the probability of the incurrence of these costs i.e., to the probability of\nbankruptcy.  Using a variant of the argument employed above for monitoring costs, it can be\nshown that the total value of the firm will fall, and the owner-manager equity holder will bear the\n                                                                                                                                                                                                \n51 We have been told that while this is true in principle, the actual behavior of the courts appears to\nfrequently involve the provision of some settlement to the common stockholders even when the assets of\nthe company are not sufficient to cover the claims of the creditors.\n52 If under bankruptcy the bondholders have the right to fire the management, the management will have\nsome incentives to avoid taking actions which increase the probability of this event (even if it is in the best\ninterest of the equity holders) if they (the management) are earning rents or if they have human capital\nspecialized to this firm or if they face large adjustment costs in finding new employment.  A detailed\nexamination of this issue involves the value of the control rights (the rights to hire and fire the manager) and\nwe leave it to a subsequent paper.\n\n\nJensen and Meckling 50 1976\nentire wealth effect of the bankruptcy costs as long as potential bondholders make unbiased\nestimates of their magnitude at the time they initially purchase bonds.53\nEmpirical studies of the magnitude of bankruptcy costs are almost non-existent.  Warner\n(1977) in a study of 11 railroad bankruptcies between 1930 and 1955 estimates the average costs\nof bankruptcy54 as a fraction of the value of the firm three years prior to bankruptcy to be 2.5%\n(with a range of 0.4% to 5.9%).  The average dollar costs were $1.88 million.  Both of these\nmeasures seem remarkably small and are consistent with our belief that bankruptcy costs\nthemselves are unlikely to be the major determinant of corporate capital structures.  it is also\ninteresting to note that the annual amount of defaulted funds has fallen significantly since 1940.\n(See Atkinson, 1967.)  One possible explanation for this phenomena is that firms are using\nmergers to avoid the costs of bankruptcy.  This hypothesis seems even more reasonable, if, as is\nfrequently the case, reorganization costs represent only a fraction of the costs associated with\nbankruptcy.\nIn general the revenues or the operating costs of the firm are not independent of the\nprobability of bankruptcy and thus the capital structure of the firm.  As the probability of\nbankruptcy increases, both the operating costs and the revenues of the firm are adversely\naffected, and some of these costs can be avoided by merger.  For example, a firm with a high\nprobability of bankruptcy will also find that it must pay higher salaries to induce executives to\naccept the higher risk of unemployment.  Furthermore, in certain kinds of durable goods industries\nthe demand function for the firm’s product will not be independent of the probability of\nbankruptcy.  The computer industry is a good example. There, the buyer’s welfare is dependent to\n                                                                \n53 Kraus and Litzenberger (1973) and Lloyd-Davies (1975) demonstrate that the total value of the firm will be\nreduced by these costs.\n\n\nJensen and Meckling 51 1976\na significant extent on the ability to maintain the equipment, and on continuous hardware and\nsoftware development.  Furthermore, the owner of a large computer often receives benefits from\nthe software developments of other users.  Thus if the manufacturer leaves the business or loses\nhis software support and development experts because of financial difficulties, the value of the\nequipment to his users will decline. The buyers of such services have a continuing interest in the\nmanufacturer’s viability not unlike that of a bondholder, except that their benefits come in the form\nof continuing services at lower cost rather than principle and interest payments. Service facilities\nand spare parts for automobiles and machinery are other examples.\nIn summary then the agency costs associated with debt55 consist of:\n1. the opportunity wealth loss caused by the impact of debt on the investment decisions\nof the firm,\n2. the monitoring and bonding expenditures by the bondholders and the owner-manager\n(i.e., the firm),\n3. the bankruptcy and reorganization costs.\n4.4 Why Are the Agency Costs of Debt Incurred?\nWe have argued that the owner-manager bears the entire wealth effects of the agency\ncosts of debt and he captures the gains from reducing them.  Thus, the agency costs associated\nwith debt discussed above will tend, in the absence of other mitigating factors, to discourage the\nuse of corporate debt.  What are the factors that encourage its use?\nOne factor is the tax subsidy on interest payments.  (This will not explain preferred stock\nwhere dividends are not tax deductible.)56  Modigliani and Miller (1963) originally demonstrated\n                                                                                                                                                                                                \n54 These include only payments to all parties for legal fees, professional services, trustees’ fees and filing\nfees.  They do not include the costs of management time or changes in cash flows due to shifts in the firm’s\ndemand or cost functions discussed below.\n\n\nJensen and Meckling 52 1976\nthat the use of riskless perpetual debt will increase the total value of the firm (ignoring the agency\ncosts) by an amount equal to τB, where τ is the marginal and average corporate tax rate and B is\nthe market value of the debt.  Fama and Miller (1972, ch. 4) demonstrate that for the case of risky\ndebt the value of the firm will increase by the market value of the (uncertain) tax subsidy on the\ninterest payments.  Again, these gains will accrue entirely to the equity and will provide an\nincentive to utilize debt to the point where the marginal wealth benefits of the tax subsidy are just\nequal to the marginal wealth effects of the agency costs discussed above.\nHowever, even in the absence of these tax benefits, debt would be utilized if the ability to\nexploit potentially profitable investment opportunities is limited by the resources of the owner.  If\nthe owner of a project cannot raise capital he will suffer an opportunity loss represented by the\nincrement in value offered to him by the additional investment opportunities.  Thus even though he\nwill bear the agency costs from selling debt, he will find it desirable to incur them to obtain\nadditional capital as long as the marginal wealth increments from the new investments projects are\ngreater than the marginal agency costs of debt, and these agency costs are in turn less than those\ncaused by the sale of additional equity discussed in section 2. Furthermore, this solution is optimal\nfrom the social viewpoint.  However, in the absence of tax subsidies on debt these projects must\n                                                                                                                                                                                                \n55 Which, incidentally, exist only when the debt has some probability of default.\n56 Our theory is capable of explaining why in the absence of the tax subsidy on interest payments, we would\nexpect to find firms using both debt and preferred stocks—a problem which has long puzzled at least one of\nthe authors.  If preferred stock has all the characteristics of debt except for the fact that its holders cannot\nput the firm into bankruptcy in the event of nonpayment of the preferred dividends, then the agency costs\nassociated with the issuance of preferred stock will be lower than those associated with debt by the present\nvalue of the bankruptcy costs.\nHowever, these lower agency costs of preferred stock exist only over some range if as the amount of such\nstock rises the incentive effects caused by their existence impose value reductions which are larger than that\ncaused by debt (including the bankruptcy costs of debt).  There are two reasons for this.  First, the equity\nholder’s claims can be eliminated by the debtholders in the event of bankruptcy, and second, the\ndebtholders have the right to fire the management in the event of bankruptcy.  Both of these will tend to\nbecome more important as an advantage to the issuance of debt as we compare situations with large\namounts of preferred stock to equivalent situations with large amounts of debt because they will tend to\nreduce the incentive effects of large amounts of preferred stock.\n\n\nJensen and Meckling 53 1976\nbe unique to this firm57 or they would be taken by other competitive entrepreneurs (perhaps new\nones) who possessed the requisite personal wealth to fully finance the projects 58 and therefore\nable to avoid the existence of debt or outside equity.\n5.    A Theory of the Corporate Ownership Structure\nIn the previous sections we discussed the nature of agency costs associated with outside\nclaims on the firm—both debt and equity.  Our purpose here is to integrate these concepts into the\nbeginnings of a theory of the corporate ownership structure.  We use the term “ownership\nstructure” rather than “capital structure” to highlight the fact that the crucial variables to be\ndetermined are not just the relative amounts of debt and equity but also the fraction of the equity\nheld by the manager.  Thus, for a given size firm we want a theory to determine three variables:59\nSi : inside equity (held by the manager),\nSo : outside equity (held by anyone outside of the firm),\nB  : debt (held by anyone outside of the firm).\n                                                                \n57 One other condition also has to hold to justify the incurrence of the costs associated with the use of debt\nor outside equity in our firm.  If there are other individuals in the economy who have sufficiently large\namounts of personal capital to finance the entire firm, our capital constrained owner can realize the full\ncapital value of his current and prospective projects and avoid the agency costs by simply selling the firm\n(i.e., the right to take these projects) to one of these individuals.  He will then avoid the wealth losses\nassociated with the agency costs caused by the sale of debt or outside equity.  If no such individuals exist,\nit will pay him (and society) to obtain the additional capital in the debt market.  This implies, incidentally,\nthat it is somewhat misleading to speak of the owner-manager as the individual who bears the agency costs.\nOne could argue that it is the project which bears the costs since, if it is not sufficiently profitable to cover\nall the costs (including the agency costs), it will not be taken.  We continue to speak of the owner-manager\nbearing these costs to emphasize the more correct and important point that he has the incentive to reduce\nthem because, if he does, his wealth will be increased.\n58 We continue to ignore for the moment the additional complicating factor involved with the portfolio\ndecisions of the owner, and the implied acceptance of potentially diversifiable risk by such 100% owners in\nthis example.\n59 We continue to ignore such instruments as convertible bonds and warrants.\n\n\nJensen and Meckling 54 1976\nThe total market value of the equity is S = Si+So, and the total market value of the firm is\nV = S+B.  In addition, we also wish to have a theory which determines the optimal size of the\nfirm, i.e., its level of investment.\n5.1 Determination of the Optimal Ratio of Outside Equity to Debt\nConsider first the determination of the optimal ratio of outside equity to debt, So/B.  To do\nthis let us hold the size of the firm constant.  V, the actual value of the firm for a given size, will\ndepend on the agency costs incurred, hence we use as our index of size V*, the value of the firm\nat a given scale when agency costs are zero.  For the moment we also hold the amount of outside\nfinancing (B+So), constant.  Given that a specified amount of financing ( B+So) is to be obtained\nexternally our problem is to determine the optimal fraction E*  ≡  o*S /(B + So ) to be financed with\nequity.\nWe argued above that:  (1) as long as capital markets are efficient (i.e., characterized by\nrational expectations) the prices of assets such as debt and outside equity will reflect unbiased\nestimates of the monitoring costs and redistributions which the agency relationship will engender,\nand (2) the selling owner-manager will bear these agency costs.  Thus from the owner-manager’s\nstandpoint the optimal proportion of outside funds to be obtained from equity (versus debt) for a\ngiven level of internal equity  is that E which results in minimum total agency costs.\nFig. 5 presents a breakdown of the agency costs into two separate components:  Define\nASo(E) as the total agency costs (a function of E) associated with the ‘exploitation’ of the outside\nequity holders by the owner-manager, and AB(E) as the total agency costs associated with the\npresence of debt in the ownership structure. Aτ(E) = ASo(E) + AB(E) is the total agency cost.\n\n\nJensen and Meckling 55 1976\nFig. 5. Total agency costs, Aτ(E), as a function of the ratio of outside equity, to total outside financing, E\n≡ So/(B+So), for a given firm size V* and given total amounts of outside financing ( B+So).  ASo(E) ≡ agency\ncosts associated with outside equity.  AB(E) ≡ agency costs associated with debt, B. AT(E*)  = minimum total\nagency costs at optimal fraction of outside financing E*.\nConsider the function ASo(E).  When E ≡ So/(B+So) is zero, i.e., when there is no outside\nequity, the manager’s incentives to exploit the outside equity is at a minimum (zero) since the\nchanges in the value of the total equity are equal to the changes in his equity.60  As E increases to\n100 percent his incentives to exploit the outside equity holders increase and hence the agency\ncosts ASo(E) increase.\n                                                                \n60 Note, however, that even when outsiders own none of the equity the stockholder-manager still has some\nincentives to engage in activities which yield him non-pecuniary benefits but reduce the value of the firm by\nmore than he personally values the benefits if there is any risky debt outstanding.  Any such actions he\ntakes which reduce the value of the firm, V, tend to reduce the value of the bonds as well as the value of the\nequity.  Although the option pricing model does not in general apply exactly to the problem of valuing the\ndebt and equity of the firm, it can be useful in obtaining some qualitative insights into matters such as this.\nIn the option pricing model \n¶S/¶V indicates the rate at which the stock value changes per dollar change in\nthe value of the firm (and similarly for ¶S/¶V).  Both of these terms are less than unity (cf. Black and Scholes,\n1973).  Therefore, any action of the manager which reduces the value of the firm, V, tends to reduce the\nvalue of both the stock and the bonds, and the larger is the total debt/equity ratio the smaller is the impact\nof any given change in V on the value of the equity, and therefore, the lower is the cost to him of consuming\nnon-pecuniary benefits.\n\n\nJensen and Meckling 56 1976\nThe agency costs associated with the existence of debt, AB(E) are composed mainly of\nthe value reductions in the firm and monitoring costs caused by the manager’s incentive to\nreallocate wealth from the bondholders to himself by increasing the value of his equity claim.\nThey are at a maximum where all outside funds are obtained from debt, i.e., where So = E = 0.\nAs the amount of debt declines to zero these costs also go to zero because as E goes to 1, his\nincentive to reallocate wealth from the bondholders to himself falls.  These incentives fall for two\nreasons:  (1) the total amount of debt falls, and therefore it is more difficult to reallocate any given\namount away from the debtholders, and (2) his share of any reallocation which is accomplished is\nfalling since So is rising and therefore Si/(So+Si), his share of the total equity, is falling.\nThe curve Aτ(E) represents the sum of the agency costs from various combinations of\noutside equity and debt financing, and as long as ASo(E) and AB(E) are as we have drawn them\nthe minimum total agency cost for given size firm and outside financing will occur at some point\nsuch as Aτ(E*) with a mixture of both debt and equity.61\nA caveat .  Before proceeding further we point out that the issue regarding the exact\nshapes of the functions drawn in fig. 5 and several others discussed below is essentially an open\nquestion at this time.  In the end the shape of these functions is a question of fact and can only be\nsettled by empirical evidence.  We outline some a priori arguments which we believe lead to some\nplausible hypotheses about the behavior of the system, but confess that we are far from\nunderstanding the many conceptual subtleties of the problem. We are fairly confident of our\narguments regarding the signs of the first derivatives of the functions, but the second derivatives\nare also important to the final solution and much more work (both theoretical and empirical) is\nrequired before we can have much confidence regarding these parameters. We anticipate the\n                                                                \n61 This occurs, of course, not at the intersection of ASo(E) and AB(E), but at the point where the absolute\nvalue oft he slopes of the functions are equal, i.e., where A’So(E)+A’B(E) = 0.\n\n\nJensen and Meckling 57 1976\nwork of others as well as our own to cast more light on these issues.  Moreover, we suspect the\nresults of such efforts will generate revisions to the details of what follows. We believe it is\nworthwhile to delineate the overall framework in order to demonstrate, if only in a simplified\nfashion, how the major pieces of the puzzle fit together into a cohesive structure.\n5.2 Effects of the Scale of Outside Financing\nIn order to investigate the effects of increasing the amount of outside financing, B+So, and\ntherefore reducing the amount of equity held by the manager, Si, we continue to hold the scale of\nthe firm, V*, constant.  Fig. 6 presents a plot of the agency cost functions ASo(E), AB(E) and Aτ(E)\n= ASo(E)+AB(E), for two different levels of outside financing.  Define an index of the amount of\noutside financing to be\nK = (B + So)/V*,\nand consider two different possible levels of outside financing Ko and K1 for a given scale of the\nfirm such that Ko < K1.\nAs the amount of outside equity increases, the owner’s fractional claim on the firm, α,\nfalls.  He will be induced thereby to take additional non-pecuniary benefits out of the firm because\nhis share of the cost falls.  This also increases the marginal benefits from monitoring activities and\ntherefore will tend to increase the optimal level of monitoring.  Both of these factors will cause the\nlocus of agency costs ASo(E;K) to shift upward as the fraction of outside financing, K, increases.\nThis is depicted in fig. 6 by the two curves representing the agency costs of equity, one for the\nlow level of outside financing, ASo(E;Ko), the other for the high level of outside financing,\nASo(E;K1).  The locus of the latter lies above the former everywhere except at the origin where\nboth are 0.\n\n\nJensen and Meckling 58 1976\nFig. 6. Agency cost functions and optimal outside equity as a fraction of total outside financing, E*(K) ,\nfor two different levels of outside financing.  K, for a given size firm, V* : K1 > Ko.\nThe agency cost of debt will similarly rise as the amount of outside financing increases.\nThis means that the locus of AB(E;K1) for high outside financing, K1, will lie above the locus of\nAB(E;Ko) for low outside financing, Ko because the total amount of resources which can be\nreallocated from bondholders increases as the total amount of debt increases.  However, since\nthese costs are zero when the debt is zero for both Ko and K1 the intercepts of the AB(E;K)\ncurves coincide at the right axis.\nThe net effect of the increased use of outside financing given the cost functions assumed\nin fig. 6 is to:  (1) increase the total agency costs from Aτ(E*;Ko) to Aτ(E*;K1), and (2) to\nincrease the optimal fraction of outside funds obtained from the sale of outside equity.  We draw\nthese functions for illustration only and are unwilling to speculate at this time on the exact form of\n\n\nJensen and Meckling 59 1976\nE*(K) which gives the general effects of increasing outside financing on the relative quantities of\ndebt and equity.\nThe locus of points Aτ(E*;K) where agency costs are minimized (not drawn in fig. 6),\ndetermines E*(K), the optimal proportions of equity and debt to be used in obtaining outside funds\nas the fraction of outside funds, K, ranges from 0 to 100 percent.  The solid line in fig. 7 is a plot\nof the minimum total agency costs as a function of the amount of outside financing for a firm with\nscale o*V .  The dotted line shows the total agency costs for a larger firm with scale 1*V  >  o*V .\nThat is, we hypothesize that the larger the firm becomes the larger are the total agency costs\nbecause it is likely that the monitoring function is inherently more difficult and expensive in a larger\norganization.\nFig. 7. Total agency costs as a function of the fraction of t he firm financed by outside claims for two firm\nsizes, 1*V  >  o*V .\n\n\nJensen and Meckling 60 1976\n5.3 Risk and the Demand for Outside Financing\nThe model we have used to explain the existence of minority shareholders and debt in the\ncapital structure of corporations implies that the owner-manager, if he resorts to any outside\nfunding, will have his entire wealth invested in the firm.  The reason is that he can thereby avoid\nthe agency costs which additional outside funding impose. This suggests he would not resort to\noutside funding until he had invested 100 percent of his personal wealth in the firm—an implication\nwhich is not consistent with what we generally observe.  Most owner-managers hold personal\nwealth in a variety of forms, and some have only a relatively small fraction of their wealth\ninvested in the corporation they manage.62  Diversification on the part of owner-managers can be\nexplained by risk aversion and optimal portfolio selection.\nIf the returns from assets are not perfectly correlated an individual can reduce the\nriskiness of the returns on his portfolio by dividing his wealth among many different assets, i.e., by\ndiversifying.63   Thus a manager who invests all of his wealth in a single firm (his own) will\ngenerally bear a welfare loss (if he is risk averse) because he is bearing more risk than necessary.\nHe will, of course, be willing to pay something to avoid this risk, and the costs he must bear to\naccomplish this diversification will be the agency costs outlined above.  He will suffer a wealth\nloss as he reduces his fractional ownership because prospective shareholders and bondholders will\ntake into account the agency costs. Nevertheless, the manager’s desire to avoid risk will\ncontribute to his becoming a minority stockholder.\n                                                                \n62 On the average, however, top managers seem to have substantial holdings in absolute dollars.  A recent\nsurvey by Wytmar (1974, p. 1) reported that the median value of 826 chief executive officers’ stock holdings\nin their companies at year end 1973 and $557,000 and $1.3 million at year end 1972.\n63 These diversification effects can be substantial.  Evans and Archer (1968) show that on the average for\nNew York Stock Exchange securities approximately 55% of the total risk (as measured by standard deviation\nof portfolio returns) can be eliminated by following a naive strategy of dividing one’s assets equally among\n40 randomly selected securities.\n\n\nJensen and Meckling 61 1976\n5.4 Determination of the Optimal Amount of Outside Financing, K*\nAssume for the moment that the owner of a project (i.e., the owner of a prospective firm)\nhas enough wealth to finance the entire project himself.  The optimal scale of the corporation is\nthen determined by the condition that, DV-DI = 0.  In general if the returns to the firm are\nuncertain the owner-manager can increase his welfare by selling off part of the firm either as debt\nor equity and reinvesting the proceeds in other assets.  If he does this with the optimal combination\nof debt and equity (as in fig. 6) the total wealth reduction he will incur is given by the agency cost\nfunction, Aτ(E*,K;V*) in fig. 7. The functions Aτ(E*,K;V*) will be S shaped (as drawn) if total\nagency costs for a given scale of firm increase at an increasing rate at low levels of outside\nfinancing, and at a decreasing rate for high levels of outside financing as monitoring imposes more\nand more constraints on the manager’s actions.\nFigure 8 shows marginal agency costs as a function of K, the fraction of the firm financed\nwith outside funds assuming the total agency cost function is as plotted in fig. 7, and assuming the\nscale of the firm is fixed. The demand by the owner-manager for outside financing is shown by\nthe remaining curve in fig. 8.  This curve represents the marginal value of the increased\ndiversification which the manager can obtain by reducing his ownership claims and optimally\nconstructing a diversified portfolio.  It is measured by the amount he would pay to be allowed to\nreduce his ownership claims by a dollar in order to increase his diversification. If the liquidation of\nsome of his holdings also influences the owner-manager’s consumption set, the demand function\nplotted in fig. 8 also incorporates the marginal value of these effects. The intersection of these two\nschedules determines the optimal fraction of the firm to be held by outsiders and this in turn\ndetermines the total agency costs borne by the owner.  This solution is Pareto optimal; there is no\nway to reduce the agency costs without making someone worse off.\n\n\nJensen and Meckling 62 1976\nFig. 8. Determination of the optimal amount of outside financing, K*, for a given scale of firm.\n5.5 Determination of the Optimal Scale of the Firm\nWhile the details of the solution of the optimal scale of the firm are complicated when we\nallow for the issuance of debt, equity and monitoring and bonding, the general structure of the\nsolution is analogous to the case where monitoring and bonding are allowed for the outside equity\nexample (see fig. 4).\nIf it is optimal to issue any debt, the expansion path taking full account of such\nopportunities must lie above the curve ZG in fig. 4.  If this new expansion path lies anywhere to\nthe right of the indifference curve passing through point G debt will be used in the optimal\nfinancing package.  Furthermore, the optimal scale of the firm will be determined by the point at\nwhich this new expansion path touches the highest indifference curve.  In this situation the\nresulting level of the owner-manager’s welfare must therefore be higher.\n\n\nJensen and Meckling 63 1976\n6.    Qualifications and Extensions of the Analysis\n6.1 Multiperiod aspects of the agency problem\nWe have assumed throughout our analysis that we are dealing only with a single\ninvestment–financing decision by the entrepreneur and have ignored the issues associated with the\nincentives affecting future financing–investment decisions which might arise after the initial set of\ncontracts are consummated between the entrepreneur–manager, outside stockholders and\nbondholders.  These are important issues which are left for future analysis.64  Their solution will\nundoubtedly introduce some changes in the conclusions of the single decision analysis.  It seems\nclear, for instance, that the expectation of future sales of outside equity and debt will change the\ncosts and benefits facing the manager in making decisions which benefit himself at the (short–run)\nexpense of the current bondholders and stockholders.  If he develops a reputation for such\ndealings, he can expect this to unfavorably influence the terms at which he can obtain future\ncapital from outside sources.  This will tend to increase the benefits associated with “sainthood”\nand will tend to reduce the size of the agency costs.  Given the finite life of any individual,\nhowever, such an effect cannot reduce these costs to zero, because at some point these future\ncosts will begin to weigh more heavily on his successors and therefore the relative benefits to him\nof acting in his own best interests will rise.65  Furthermore, it will generally be impossible for him\nto fully guarantee the outside interests that his successor will continue to follow his policies.\n                                                                \n64 The recent work of Myers (1975) which views future investment opportunities as options and investigates\nthe incentive effects of the existence of debt in such a world where a sequence of investment decisions is\nmade is another important step in the investigation of the multiperiod aspects of the agency problem and the\ntheory of the firm.\n65 Becker and Stigler (1972) analyze a special case of this problem involving the use of nonvested pension\nrights to help correct for this end game play in the law enforcement area.\n\n\nJensen and Meckling 64 1976\n6.2 The Control Problem and Outside Owner’s Agency Costs\nThe careful reader will notice that nowhere in the analysis thus far have we taken into\naccount many of the details of the relationship between the part owner–manager and the outside\nstockholders and bondholders.  In particular, we have assumed that all outside equity is nonvoting.\nIf such equity does have voting rights, then the manager will be concerned about the effects on his\nlong–run welfare of reducing his fractional ownership below the point where he loses effective\ncontrol of the corporation.  That is, below the point where it becomes possible for the outside\nequity holders to fire him.  A complete analysis of this issue will require a careful specification of\nthe contractual rights involved on both sides, the role of the board of directors, and the\ncoordination (agency) costs borne by the stockholders in implementing policy changes.  This latter\npoint involves consideration of the distribution of the outside ownership claims.  Simply put, forces\nexist to determine an equilibrium distribution of outside ownership.  If the costs of reducing the\ndispersion of ownership are lower than the benefits to be obtained from reducing the agency costs,\nit will pay some individual or group of individuals to buy shares in the market to reduce the\ndispersion of ownership.  We occasionally witness these conflicts for control which involve\noutright market purchases, tender offers, and proxy fights.  Further analysis of these issues is left\nto the future.\n6.3 A Note on the Existence of Inside Debt and Some Conjectures on the Use of Convertible\nFinancial Instruments\nWe have been asked66 why debt held by the manager (i.e., “inside debt”) plays no role in\nour analysis.  We have as yet been unable to incorporate this dimension formally into our analysis\nin a satisfactory way.  The question is a good one and suggests some potentially important\n                                                                \n66  By our colleague David Henderson.\n\n\nJensen and Meckling 65 1976\nextensions of the analysis.  For instance, it suggests an inexpensive way for the owner–manager\nwith both equity and debt outstanding to eliminate a large part (perhaps all) of the agency costs of\ndebt.  If he binds himself contractually to hold a fraction of the total debt equal to his fractional\nownership of the total equity he would have no incentive whatsoever to reallocate wealth from the\ndebt holders to the stockholders.  Consider the case where\nBi/Si  =  Bo /So, (4)\nwhere Si and So are as defined earlier, Bi is the dollar value of the inside debt held by the owner–\nmanager, and Bo is the debt held by outsiders.  In this case, if the manager changes the investment\npolicy of the firm to reallocate wealth between the debt and equity holders, the net effect on the\ntotal value of his holdings in the firm will be zero.  Therefore, his incentives to perform such\nreallocations are zero.67\nWhy then don’t we observe practices or formal contracts which accomplish this\nelimination or reduction of the agency costs of debt?  Maybe we do for smaller privately held\nfirms (we haven’t attempted to obtain this data), but for large diffuse owner corporations the\npractice does not seem to be common.  One reason for this we believe is that in some respects\nthe claim that the manager holds on the firm in the form of his wage contract has some of the\ncharacteristics of debt.68  If true, this implies that even with zero holdings of formal debt claims he\nstill has positive holdings of a quasi–debt claim and this may accomplish the satisfaction of\ncondition (4).  The problem here is that any formal analysis of this issue requires a much deeper\n                                                                \n67 This also suggests that some outside debt holders can protect themselves from ‘exploitation’ by the\nmanager by purchasing a fraction of the total equity equal to their fractional ownership of the debt.  All debt\nholders, of course, cannot do this unless the manager does so also.  In addition, such an investment rule\nrestricts the portfolio choices of investors and therefore would impose costs if followed rigidly.  Thus the\nagency costs will not be eliminated this way either.\n68 Consider the situation in which the bondholders have the right in the event of bankruptcy to terminate his\nemployment and therefore to terminate the future returns to any specific human capital or rents he may be\nreceiving.\n\n\nJensen and Meckling 66 1976\nunderstanding of the relationship between formal debt holdings and the wage contract; i.e., how\nmuch debt is it equivalent to?\nThis line of thought also suggests some other interesting issues.  Suppose the implicit debt\ncharacteristics of the manager’s wage contract result in a situation equivalent to\nBi/Si  =>  Bo /So.\nThen he would have incentives to change the operating characteristics of the firm (i.e.,\nreduce the variance of the outcome distribution) to transfer wealth from the stockholders to the\ndebt holders which is the reverse of the situation we examined in section 4.  Furthermore, this\nseems to capture some of the concern often expressed regarding the fact that managers of large\npublicly held corporations seem to behave in a risk–averse way to the detriment of the equity\nholders.  One solution to this would be to establish incentive compensation systems for the\nmanager or to give him stock options which in effect give him a claim on the upper tail of the\noutcome distribution.  This also seems to be a commonly observed phenomenon.\nThis analysis also suggests some additional issues regarding the costs and benefits\nassociated with the use of more complicated financial claims such as warrants, convertible bonds,\nand convertible preferred stock which we have not formally analyzed as yet.  Warrants,\nconvertible bonds, and convertible preferred stock have some of the characteristics of non–voting\nshares although they can be converted into voting shares under some terms.  Alchian–Demsetz\n(1972) provide an interesting analysis regarding the use of non–voting shares.  They argue that\nsome shareholders with strong beliefs in the talents and judgments of the manager will want to be\nprotected against the possibility that some other shareholders will take over and limit the actions of\nthe manager (or fire him).  Given that the securities exchanges prohibit the use of non–voting\nshares by listed firms, the use of the option–type securities might be a substitute for these claims.\n\n\nJensen and Meckling 67 1976\nIn addition, warrants represent a claim on the upper tail of the distribution of outcomes,\nand convertible securities can be thought of as securities with non–detachable warrants.  It seems\nthat the incentive effect of warrants would tend to offset to some extent the incentive effects of\nthe existence of risky debt because the owner–manager would be sharing part of the proceeds\nassociated with a shift in the distribution of returns with the warrant holders.  Thus, we conjecture\nthat potential bondholders will find it attractive to have warrants attached to the risky debt of firms\nin which it is relatively easy to shift the distribution of outcomes to expand the upper tail of the\ndistribution to transfer wealth from bondholders.  It would also then be attractive to the owner–\nmanager because of the reduction in the agency costs which he would bear.  This argument also\nimplies that it would make little difference if the warrants were detachable (and therefore saleable\nseparately from the bonds) since their mere existence would reduce the incentives of the manager\n(or stockholders) to increase the riskiness of the firm (and therefore increase the probability of\nbankruptcy).  Furthermore, the addition of a conversion privilege to fixed claims such as debt or\npreferred stock would also tend to reduce the incentive effects of the existence of such fixed\nclaims and therefore lower the agency costs associated with them.  The theory predicts that these\nphenomena should be more frequently observed in cases where the incentive effects of such fixed\nclaims are high than when they are low.\n6.4 Monitoring and the Social Product of Security Analysts\nOne of the areas in which further analysis is likely to lead to high payoffs is that of\nmonitoring.  We currently have little which could be glorified by the title of a “Theory of\nMonitoring” and yet this is a crucial building block of the analysis.  We would expect monitoring\nactivities to become specialized to those institutions and individuals who possess comparative\nadvantages in these activities.  One of the groups who seem to play a large role in these activities\nis composed of the security analysts employed by institutional investors, brokers and investment\n\n\nJensen and Meckling 68 1976\nadvisory services as well as the analysis performed by individual investors in the normal course of\ninvestment decision making.\nA large body of evidence exists which indicates that security prices incorporate in an\nunbiased manner all publicly available information and much of what might be called “private\ninformation”.69  There is also a large body of evidence which indicates that the security analysis\nactivities of mutual funds and other institutional investors are not reflected in portfolio returns, i.e.,\nthey do not increase risk–adjusted portfolio returns over a naive random selection buy–and–hold\nstrategy.70  Therefore, some have been tempted to conclude that the resources expended on such\nresearch activities to find under– or over–valued securities is a social loss.  Jensen (1979) argues\nthat this conclusion cannot be unambiguously drawn because there is a large consumption element\nin the demand for these services.\nFurthermore, the analysis of this paper would seem to indicate that to the extent that\nsecurity analysis activities reduce the agency costs associated with the separation of ownership\nand control, they are indeed socially productive.  Moreover, if this is true, we expect the major\nbenefits of the security analysis activity to be reflected in the higher capitalized value of the\nownership claims to corporations and not in the period–to–period portfolio returns of the analyst.\nEquilibrium in the security analysis industry requires that the private returns to analysis (i.e.,\nportfolio returns) must be just equal to the private costs of such activity,71 and this will not reflect\nthe social product of this activity which will consist of larger output and higher levels of the capital\nvalue of ownership claims.  Therefore, the argument implies that if there is a non–optimal amount\n                                                                \n69  See Fama (1970a) for a survey of this ‘efficient markets’ literature.\n70  See Jensen (1969) for an example of this evidence and references.\n71 Ignoring any pure consumption elements in the demand for security analysis\n\n\nJensen and Meckling 69 1976\nof security analysis being performed, it is too much72 not too little (since the shareholders would be\nwilling to pay directly to have the “optimal” monitoring performed), and we don’t seem to observe\nsuch payments.\n6.5 Specialization in the Use of Debt and Equity\nOur previous analysis of agency costs suggests at least one other testable hypothesis:  i.e.,\nthat in those industries where the incentive effects of outside equity or debt are widely different,\nwe would expect to see specialization in the use of the low agency cost financing arrangement.  In\nindustries where it is relatively easy for managers to lower the mean value of the outcomes of the\nenterprise by outright theft, special treatment of favored customers, ease of consumption of leisure\non the job, etc. (for example, the bar and restaurant industry), we would expect to see the\nownership structure of firms characterized by relatively little outside equity (i.e., 100 percent\nownership of the equity by the manager) with almost all outside capital obtained through the use of\ndebt.\nThe theory predicts the opposite would be true where the incentive effects of debt are\nlarge relative to the incentive effects of equity.  Firms like conglomerates, in which it would be\neasy to shift outcome distributions adversely for bondholders (by changing the acquisition or\ndivestiture policy) should be characterized by relatively lower utilization of debt.  Conversely, in\nindustries where the freedom of management to take riskier projects is severely constrained (for\nexample, regulated industries such as public utilities), we should find more intensive use of debt\nfinancing.\nThe analysis suggests that in addition to the fairly well–understood role of uncertainty in\nthe determination of the quality of collateral, there is at least one other element of great\n                                                                \n72 Again ignoring the value of the pure consumption elements in the demand for security analysis.\n\n\nJensen and Meckling 70 1976\nimportance—the ability of the owner of the collateral to change the distribution of outcomes by\nshifting either the mean outcome or the variance of the outcomes.  A study of bank lending\npolicies should reveal these to be important aspects of the contractual practices observed there.\n6.6 Application of the Analysis to the Large Diffuse Ownership Corporation\nWhile we believe the structure outlined in the proceeding pages is applicable to a wide\nrange of corporations, it is still in an incomplete state.  One of the most serious limitations of the\nanalysis is that, as it stands, we have not worked out in this paper its application to the very large\nmodern corporation whose managers own little or no equity.  We believe our approach can be\napplied to this case, but space limitations preclude discussion of these issues here.  They remain to\nbe worked out in detail and will be included in a future paper.\n6.7  The Supply Side of the Incomplete Markets Question\nThe analysis of this paper is also relevant to the incomplete market issue considered by\nArrow (1964a), Diamond (1967), Hakansson (1974a, 1974b), Rubinstein (1974), Ross (1974b), and\nothers.  The problems addressed in this literature derive from the fact that whenever the available\nset of financial claims on outcomes in a market fails to span the underlying state space (see\nArrow, 1964a,  andDebreu, 1959)  the resulting allocation is Pareto inefficient.  A disturbing\nelement in this literature surrounds the fact that the inefficiency conclusion is generally drawn\nwithout explicit attention in the analysis to the costs of creating new claims or of maintaining the\nexpanded set of markets called for to bring about the welfare improvement.\nThe demonstration of a possible welfare improvement from the expansion of the set of\nclaims by the introduction of new basic contingent claims or options can be thought of as an\nanalysis of the demand conditions for new markets.  Viewed from this perspective, what is\nmissing in the literature on this problem is the formulation of a positive analysis of the supply of\n\n\nJensen and Meckling 71 1976\nmarkets (or the supply of contingent claims).  That is, what is it in the maximizing behavior of\nindividuals in the economy that causes them to create and sell contingent claims of various sorts?\nThe analysis in this paper can be viewed as a small first step in the direction of\nformulating an analysis of the supply of markets issue which is founded in the self–interested\nmaximizing behavior of individuals.  We have shown why it is in the interest of a wealth–\nmaximizing entrepreneur to create and sell claims such as debt and equity.  Furthermore, as we\nhave indicated above, it appears that extensions of these arguments will lead to a theory of the\nsupply of warrants, convertible bonds, and convertible preferred stock.  We are not suggesting\nthat the specific analysis offered above is likely to be sufficient to lead to a theory of the supply of\nthe wide range of contracts (both existing and merely potential) in the world at large.  However,\nwe do believe that framing the question of the completeness of markets in terms of the joining of\nboth the demand and supply conditions will be very fruitful instead of implicitly assuming that new\nclaims spring forth from some (costless) well head of creativity unaided or unsupported by human\neffort.\n7.    Conclusions\nThe publicly held business corporation is an awesome social invention.  Millions of\nindividuals voluntarily entrust billions of dollars, francs, pesos, etc. of personal wealth to the care\nof managers on the basis of a complex set of contracting relationships which delineate the rights\nof the parties involved.  The growth in the use of the corporate form as well as the growth in\nmarket value of established corporations suggests that at least, up to the present, creditors and\ninvestors have by and large not been disappointed with the results, despite the agency costs\ninherent in the corporate form.\n\n\nJensen and Meckling 72 1976\nAgency costs are as real as any other costs.  The level of agency costs depends, among\nother things, on statutory and common law and human ingenuity in devising contracts.  Both the\nlaw and the sophistication of contracts relevant to the modern corporation are the products of a\nhistorical process in which there were strong incentives for individuals to minimize agency costs.\nMoreover, there were alternative organizational forms available, and opportunities to invent new\nones.  Whatever its shortcomings, the corporation has thus far survived the market test against\npotential alternatives.\nReferences\nAlchian, Armen A. (1965). “The Basis of Some Recent Advances in the Theory of Management\nof the Firm.” Journal of Industrial Economics  (November): 30-44.\nAlchian, Armen A. (1968). Corporate Management and Property Rights . Economic Policy and\nthe Regulation of Securities, Washington, D.C., American Enterprise Institute.\nAlchian, Armen A. (1974). Some Implications of Recognition of Property Right Transaction\nCosts. First Interlaken Conference on Analysis and Ideology.\nAlchian, Armen A. and W.R. Allen (1969). Exchange and Production: Theory in Use .\nBelmont, CA, Wadsworth.\nAlchian, Armen A. and Harold Demsetz (1972). “Production, Information Costs, and Economic\nOrganization.” American Economic Review   LXII, no. 5 (December): 777-795.\nAlchian, Armen. A. and R.A. Kessel (1962). “Competition, Monopoly, and the Pursuit of\nPecuniary Gain”. Aspects of Labor Economics . Princeton, NJ, National Bureau of\nEconomic Research.\nArrow, Kenneth J. (1964a). “Control in Large Organizations.” Management Science  10 (April):\n397-408.\nArrow, Kenneth J. (1964b). “The Role of Securities in the Optimal Allocation of Risk Bearing.”\nReview of Economic Studies   31, no. 86 (January): 91-96.\nAtkinson, T.R. (1967). “Trends in Corporate Bond Quality”. Studies in Corporate Bond\nFinance 4 . New York:, National Bureau of Economic Research.\nBaumol, W.J. (1959). Business Behavior, Value and Growth . New York, Macmillan.\n\n\nJensen and Meckling 73 1976\nBecker, Gary S. (1957). The Economics of Discrimination . Chicago, IL, University of Chicago\nPress.\nBecker, Gary S. and George J. Stigler (1972). Law Enforcement, Corruption, and\nCompensation of Enforcers . Conference on Capitalism and Freedom.\nBenston, George J. (1977). “The Impact of Maturity Regulation on High Interest Rate Lenders\nand Borrowers.” Journal of Financial Economics   4 no. 1 .\nBerhold, M. (1971). “A Theory of Linear Profit Sharing Incentives.” Quarterly Journal of\nEconomics  LXXXV (August): 460-482.\nBerle, Adolf A. and Gardiner C. Means (1932). The Modern Corporation and Private\nProperty. New York, Macmillan Publishing Co.\nBlack, F., H. Merton, M.H. Miller, et al. (1978). “An Approach to the Regulation of Bank Holding\nCompanies.” Journal of Business  51 (3): 379-412.\nBlack, F. and M. Scholes (1973). “The Pricing of Options and Corporate Liabilities.” Journal of\nPolitical Economy  81, no. 3 : 637-654.\nBranch, B. (1973). “Corporate Objectives and Market Performance.” Financial Management\n(Summer): 24-29.\nCoase, Ronald H. (1937). “The Nature of the Firm”. Readings in Price Theory . Homewood, IL,\nIrwin. New Series, V: 331-351.\nCoase, Ronald H. (1959). “The Federal Communications Commission.” Journal of Law and\nEconomics  II (October): 1-40.\nCoase, Ronald H. (1960). “The Problem of Social Cost.” Journal of Law and Economics  III\n(October): 1-44.\nCoase, Ronald H. (1964). “Discussion.” American Economic Review   LIV, no. 3 : 194-197.\nCyert, R.M. and C.L. Hedrick (1972). “Theory of the Firm:  Past, Present and Future: An\nInterpretation.” Journal of Economic Literature   X (June): 398-412.\nCyert, R.M. and J.G. March (1963). A Behavioral Theory of The Firm . Englewood Cliffs, NJ,\nPrentice Hall.\nDeAlessi, L. (1973). “Private Property and Dispersion of Ownership in Large Corporations.”\nJournal of Finance  (September): 839-851.\nDebreu, Gerard (1959). Theory of Value . New York, John Wiley & Sons.\nDemsetz, Harold (1967). “Toward a Theory of Property Rights.” American Economic Review\nLVII (May): 347-359.\n\n\nJensen and Meckling 74 1976\nDemsetz, Harold (1969). “Information and Efficiency: Another Viewpoint.” Journal of Law and\nEconomics  XII (April): 1-22.\nDiamond, P.A. (1967). “The Role of Stock Market in a General Equilibrium Model with\nTechnological Uncertainty.” American Economic Review   LVII (September): 759-776.\nEvans, J.L. and S.H. Archer (1968). “Divesification and the Reduction of Dispersion: An\nEmpirical Analysis.” Journal of Finance  (December).\nFama, Eugene F. (1970a). “Efficient Capital Markets: A Review of Theory and Empirical Work.”\nJournal of Finance   XXV, no. 2 .\nFama, Eugene F. (1970b). “Multiperiod Consumption-investment Decisions.” American\nEconomic Review   LX (March).\nFama, Eugene F. (1972). “Ordinal and Measurable Utility”. Studies in the Theory of Capital\nMarkets . M. C. Jensen. New York, Praeger.\nFama, Eugene F.  and M. Miller (1972). The Theory of Finance . New York, Hold, Rhinehart,\nand Winston.\nFriedman, Milton (1970). The Social Responsibility of Business is to Increase its Profits. New\nYork Times Magazine : 32 FF.\nFurubotn, E.G. and S. Pejovich (1972). “Property Rights and Economic Theory: A Survey of\nRecent Literature.” Journal of Economic Literature   X (December): 1137-1162.\nGalai, D.  and R.W. Masulis (1976). “The Option Pricing Model and the Risk Factor of Stock.”\nJournal of Financial Economics   3 no. 1/2 : 53-82.\nHakansson, N.H. (1974a). Ordering Markets and the Capital Structures of Firms with\nIllustrations. Institute of Business and Economic Working Paper no. 24 . Berkley, CA,\nUniversity of California.\nHakansson, N.H. (1974b). The Superfund: Efficient Paths Toward a Complete Financial Market,\nUnpublished.\nHeckerman, D.G. (1975). “Motivating Managers to Make Investment Decision.” Journal of\nFinancial Economics   2 no. 3 : 273-292.\nHirschleifer, Jack (1958). “On the Theory of Optimal Investment Decisions.” Journal of\nPolitical Economy (August): 329-352.\nHirschleifer, Jack (1970). Investment, Interest, and Capital . Englewood Cliffs, NJ, Prentice-\nHall.\nJensen, Michael C. (1969). “Risk, the Pricing of Capital Assets, and the Evaluation of Investment\nPortfolios.” Journal of Business   42, no. 2 : 167-247.\n\n\nJensen and Meckling 75 1976\nJensen, Michael C. (1979). “Tests of Capital Market Theory and Implications of the Evidence.”\nHandbook of Financial Economics  .\nJensen, Michael C. and J.B. Long (1972). “Corporate Investment under Uncertainty and Pareto\nOptimality in the Capital Markets.” Bell Journal of Economics  (Spring): 151-174.\nJensen, Michael C. and William H. Meckling (1978). “Can the Corporation Survive?” Financial\nAnalysts Journal  (January-February).\nKlein, W.A. (1976). Legal and Economic Perspectives on the FIrm. Unpublished manuscript .\nLos Angeles, CA, University of California.\nKraus, A. and R. Litzenberger (1973). “A State Preference Model of Optimal Financial\nLeverage.” Journal of Finance  (September).\nLarner, R.J. (1970). Management Control and the Large Corporation . New York, Dunellen.\nLintner, J. (1965). “Security Prices, Risk, and Maximal Gains from Diversification.” Journal of\nFinance  XX (December): 587-616.\nLloyd-Davies, Peter R. (1975). “Optimal Financial Policy in Imperfect Markets.” Journal of\nFinancial and Quantitative Analysis  10 (3): 457ff.\nLong, J.B. (1972). “Wealth, Welfare, and the Price of Risk.” Journal of Finance  (May): 419-\n433.\nLong, J.B. (1974). “Discussion.” Journal of Finance   XXXIX, no. 12 : 485-188.\nMachlup, F. (1967). “Theories of the Firm: Marginalist, Behavioral, Managerial.” American\nEconomic Review  (March): 1-33.\nManne, H.G. (1962). “The ‘Higher Criticism’ of the Modern Corporation.” Columbia Law\nReview  62 (March): 259-284.\nManne, H.G. (1965). “Mergers and the Market for Corporate Control.” Journal of Political\nEconomy (April): 110-120.\nManne, H.G. (1967). “Our Two Corporate Systems: Law and Economics.” Virginia Law Review\n53 (March): 259-284.\nManne, H.G. (1972). “The Social Responsibility of Regulated Utilities.” Wisconsin Law Review\nV, no. 4 : 995-1009.\nMarris, R. (1964). The Economic Theory of Managerial Capitalism . Glencoe, IL, Free Press of\nGlencoe.\nMason, E.S. (1959). The Corporation in Modern Society . Cambridge, MA, Harvard University\nPress.\n\n\nJensen and Meckling 76 1976\nMcManus, J.C. (1975). “The Costs of Alternative Economic Organizations.” Canadian Journal\nof Economics   VII (August): 334-350.\nMeckling, William H. (1976). “Values and the Choice of the Model of the Individual in the Social\nSciences.” Schweizerische Zeitschrift fur Volkswirtschaft  (December).\nMerton, R.C. (1973). “The Theory of Rational Option Pricing.” Bell Journal of Economics and\nManagement Science   4, no. 1 : 141-183.\nMerton, R.C. (1974). “On the Pricing of Corporate Debt: The Risk Structure of Interest Rates.”\nJournal of Finance  XXIX, no. 2 : 449-470.\nMerton, R.C. and M.G. Subrahmanyam (1974). “The Optimality of a Competitive Stock Market.”\nBell Journal of Economics and Management Science  (Spring): 145-170.\nMiller, M.H. and F. Modigliani (1966). “Some Estimates of the Cost of Capital to the Electric\nUtility Industry, 1954-57.” American Economic Review   48 (June): 333-391.\nModigliani, F.  and M.H. Miller (1958). “The Costs of Capital, Corporate Finance, and the Theory\nof Investment.” American Economic Review   48 (June): 261-297.\nModigliani, F.  and M.H. Miller (1963). “Corporate Income Taxes and the Cost of Capital: A\nCorrection.” American Economic Review  53 (June): 433-443.\nMonson, R.J. and A. Downs (1965). “A Theory of Large Managerial Firms.” Journal of\nPolitical Economy (June): 221-236.\nMyers, Stewart C. (1975). A Note on the Determinents of Corporate Debt Capacity.\nUnpublished manuscript . London, London Graduate School of Business.\nPenrose, E. (1958). The Theory of the Growth of the Firm . New York, Wiley.\nPreston, L.E. (1975). “Corporation and Society: The Search for a Paradigm.” Journal of\nEconomic Literature   XIII (June): 434-453.\nRoss, Steven A. (1973). “The Economic Theory of Agency: The Principal’s Problems.”\nAmerican Economic Review   LXII (May): 134-139.\nRoss, S.A. (1974a). “The Economic Theory of Agency and the Principle of Similarity”. Essays\non Economic Behavior under Uncertainty . M. D. B. e. al. Amsterdam, North-Holland.\nRoss, S.A. (1974b). Options and Efficiency. Rodney L. White Center for Financial Research\nWorking Paper no. 3074 . Philadelphia, PA, University of Pennsylvania.\nRubenstein, M. (1974). A Discrete-Time Synthesis of Financial Theory, Parts I and II. Intstitute\nof Business and Economic Research . Berkeley, CA, University of California.\nScitovsky, T. (1943). “A Note of Profit Maximisation and its Implications.” Review of Economic\nStudies  XI : 57-60.\n\n\nJensen and Meckling 77 1976\nSharpe, W.F. (1964). “Capital Asset Prices: A Theory of Market Equilibrium under Conditions of\nRisk.” Journal of Finance   XIX (September): 425-442.\nShubik, M (1970). “A Curmudgeon’s Guide to Microeconomics.” Journal of Economic\nLiterature  VIII (June): 405-434.\nSilver, M. and R. Auster (1969). “Entrepreneurship, Profit, and Limits on Firm Size.” Journal of\nBusiness  42 (July): 277-281.\nSimon, Herbert A. (1955). “A Behavioral Model of Rational Choice.” Quarterly Journal of\nEconomics  69 : 99-118.\nSimon, Herbert A. (1959). “Theories of Decision Making in Economics and Behavioral Science.”\nAmerican Economic Review  (June): 253-283.\nSmith, Adam (1776). The Wealth of Nations . Edited by Edwin Cannan, 1904. Reprint edition\n1937. New York, Modern Library.\nSmith, Clifford W., Jr. (1976). “Option Pricing: A Review.” Journal of Financial Economics   2\nno. 1/2 : 3-52.\nWarner, Jerold B. (1977). “Bankruptcy, Absolute Priority, and the Pricing of Risky Debt Claims.”\nJournal of Financial Economics    4 : 239-76.\nWilliamson, Oliver E. (1964). The Economics of Discretionary Behavior: Managerial\nObjectives in a Theory of the Firm . Englewood Cliffs, NJ, Prentice-Hall.\nWilliamson, Oliver E. (1970). Corporate Control and Business Behavior . Englewood Cliffs, NJ,\nPrentice-Hall.\nWilliamson, Oliver E. (1975). Markets and Hierarchies: Analysis and Antitrust Implications .\nNew York, Free Press.\nWilson, R. (1968). “On the Theory of Syndicates.” Econometrica  36 (January): 119-132.\nWilson, R. (1969). “La Decision: Agregation et Dynamique des Orders de Preference”. Editions\ndu Centre National de la Recherche Scientifique . Paris, Centre National de la\nRecherche Scientifique: 288-307.\nWytmar (1974). . Wall Street Journal .\n	Electronic copy available at: http://ssrn.com/abstract=94043Electronic copy available at: http://ssrn.com/abstract=94043\nTheory of the Firm: Managerial Behavior,  Agency Costs and Ownership Structure Michael C. Jensen Harvard Business School MJensen@hbs.edu  And  William H. Meckling University of Rochester Abstract This paper integrates elements from the theory of agency, the theory of property rights and the theory of finance to develop a theory of the ownership structure of the firm. We define...	en	0.9	uploaded	27976	173996	2025-09-05 19:01:29.125517	2025-09-05 19:01:29.125529	\N	1	\N	\N
65	Anscombe-Intention-1956	file	document	\N	pdf	Anscombe-Intention-1956.pdf	uploads/14d1b5ead57648c3a6ea9ece65569cd8_Anscombe-Intention-1956.pdf	521606	\N	 \n \nIntention\nAuthor(s): G. E. M. Anscombe\nSource: Proceedings of the Aristotelian Society, New Series, Vol. 57 (1956 - 1957), pp. 321-\n332\nPublished by: Oxford University Press on behalf of The Aristotelian Society\nStable URL: https://www.jstor.org/stable/4544583\nAccessed: 05-09-2025 17:48 UTC\n \nJSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide\nrange of content in a trusted digital archive. We use information technology and tools to increase productivity and\nfacilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.\n \nYour use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at\nhttps://about.jstor.org/terms\n \nThe Aristotelian Society, Oxford University Press are collaborating with JSTOR to\ndigitize, preserve and extend access to Proceedings of the Aristotelian Society\nThis content downloaded from 132.174.234.36 on Fri, 05 Sep 2025 17:48:40 UTC\nAll use subject to https://about.jstor.org/terms\n\n Meeting of the Aristotelian Society at 21, Bedford Square, London,\n W.C.I, on 3rd June, 1957, at 7.30 p.m.\n XIV.-INTENTION \n By G. E. M. ANSCOMBE \n What distinguishes actions which are intentional from those \n which are not? The answer that suggests itself is that they \n are the actions to which a certain sense of the question \n 'Why? ' is given application; the sense is defined as that in \n which the answer, if positive, gives a reason for acting. But \n this hardly gets us any further, because the questions \n 'What is the relevant sense of the question " Why? " ' and \n 'What is meant by " reason for acting"?' are one and the \n same.\n To see the difficulties here, consider the question ' Why \n did you knock the cup off the table ? ' answered by' I thought \n I saw a face at the window and it made me jump.' Now \n we cannot say that since the answer mentions something \n previous to the action, this will be a cause as opposed to a \n reason; for if you ask 'Why did you kill him?' the answer \n ' he killed my father' is surely a reason rather than a cause, \n but what it mentions is previous to the action. It is true \n that we don't ordinarily think of a case like giving a sudden \n start when we speak of a reason for acting. ' Giving a sudden \n start ', someone might say, ' is not acting in the sense suggested \n by the expression "reason for acting".' Hence, though \n indeed we readily say e.g. ' What was the reason for your \n starting so violently? ' this is totally unlike ' What is your \n reason for excluding so-and-so from your will? ' or 'What \n is your reason for sending for a taxi? ' But what is the \n difference? Why is giving a start or gasp not an ' action', \n while sending for a taxi or crossing the road is one? The \n answer cannot be ' Because an answer to the question \n " why? " may give a reason in the latter cases ', for the \n answer may 'give a reason' in the former cases too; and \nThis content downloaded from 132.174.234.36 on Fri, 05 Sep 2025 17:48:40 UTC\nAll use subject to https://about.jstor.org/terms\n\n 322 G. E. M. ANSCOMBE \n we cannot say ' Ah, but not a reason for acting; ' we should \n be going round in circles. We need to find the difference \n between the two kinds of ' reason ' without talking about \n ' acting'; and if we do, perhaps we shall discover what \n is meant by 'acting' when it is said with this special \n emphasis. \n It will hardly be enlightening to say ' in the case of the \n sudden start the "reason" is a cause'; the topic of causality \n is in a state of too great confusion; all we know is that this \n is one of the places where we do use the word ' cause '. \n But we also know that this is rather a strange case of \n causality; the subject is able to give a cause of a thought \n or feeling or bodily movement in the same kind of \n way as he is able to state the place of his pain or the \n position of his limbs. Such statements are not based on \n observation. \n Nor can we say: 'Well, the " reason " for a movement \n is a cause, and not a reason in the sense of "reason for \n acting ", when the movement is involuntary; it is a reason \n as opposed to a cause, when the movement is voluntary and \n intentional.' This is partly because in any case the object \n of the whole enquiry is really to delineate such concepts \n as the voluntary and the intentional, and partly because \n one can also give a ' reason' which is only a ' cause' for \n what is voluntary and intentional. E.g. ' Why are you \n walking up and down like that? '-' It's that military band; \n it excites me.' Or ' What made you sign the document \n at last? '-' The thought: " It is my duty " kept hammering \n away in my mind until I said to myself" I can do no other", \n and so signed.' \n Now we can see that the cases where this difficulty \n arises are just those where the cause itself, qua cause, (or \n perhaps one should rather say the causation itself) is in the \n class of things known without observation. \n I will call the type of cause in question a ' mental cause'. \n Mental causes are possible, not only for actions (' The martial \n music excites me, that is why I walk up and down ') but \nThis content downloaded from 132.174.234.36 on Fri, 05 Sep 2025 17:48:40 UTC\nAll use subject to https://about.jstor.org/terms\n\n INTENTION 323 \n also for feelings and even thoughts. In considering actions, \n it is important to distinguish between mental causes and \n motives; in considering feelings, such as fear or anger, it \n is important to distinguish between mental causes and \n objects of feeling. To see this, consider the following \n cases: \n A child saw a bit of red stuff on a turn in a stairway and \n asked what it was. He thought his nurse told him it was a \n bit of Satan and felt dreadful fear of it. (No doubt she said \n it was a bit of satin.) What he was frightened of was the \n bit of stuff; the cause of his fright was his nurse's remark. \n The object of fear may be the cause of fear, but, as \n Wittgenstein' remarks, is not as such the cause of fear. (A \n hideous face appearing at the window would of course be \n both cause and object, and hence the two are easily confused.) \n Or again, you may be angry at someone's action, when \n what makes you angry is some reminder of it, or someone's \n telling you of it. \n This sort of cause of a feeling or reaction may be reported \n by the person himself, as well as recognised by someone \n else, even when it is not the same as the object. Note that \n this sort of causality or sense of ' causality ' is so far from \n accommodating itself to Hume's explanations that people \n who believe that Hume pretty well dealt with the topic of \n causality would entirely leave it out of their calculations; \n if their attention were drawn to it they might insist that the \n word ' cause' was inappropriate or was quite equivocal. \n Or conceivably they might try to give a Humeian \n account of the matter as far as concerned the outside \n observer's recognition of the cause; but hardly for the \n patient's. \n Now one might think that when the question 'Why?' \n is answered by giving the intention with which a person \n acts-a case of which I will here simply characterise by \n ' Philosophical Investigations, ? 476. \nThis content downloaded from 132.174.234.36 on Fri, 05 Sep 2025 17:48:40 UTC\nAll use subject to https://about.jstor.org/terms\n\n 324 G. E. M. ANSCOMBE \n saying that it mentions something future-this is also a case \n of a mental cause. For couldn't it be recast in the form: \n 'Because I wanted ... or ' Out of a desire that...' ? \n If a feeling of desire for an apple affects me and I get up and \n go to a cupboard where I think there are some, I might \n answer the question what led to this action by mentioning \n the desire as having made me . . . etc. But it is not in all \n cases that ' I did so and so in order to . . .' can be backed \n up by ' I felt a desire that . . . ' I may e.g. simply hear \n a knock on the door and go downstairs to open it without \n experiencing any such desire. Or suppose I feel an upsurge \n of spite against someone and destroy a message he has \n received so that he shall miss an appointment. If I describe \n this by saying' I wanted to make him miss that appointment', \n this does not necessarily mean that I had the thought ' If I \n do this, he will . . . ' and that it affected me with a desire \n of bringing that about which led up to my action. This may \n have happened, but need not. It could be that all that \n happened was this: I read the message, had the thought \n ' That unspeakable man ! ' with feelings of hatred, tore the \n message up, and laughed. Then if the question 'Why did \n you do that? ' is put by someone who makes it clear that \n he wants me to mention the mental causes-i.e., what went \n on in my mind and issued in the action-I should perhaps \n give this account; but normally the reply would be no such \n thing. That particular enquiry is not very often made. \n Nor do I wish to say that it always has an answer in cases \n where it can be made. One might shrug or say ' I don't \n know that there was any definite history of the kind you \n mean', or ' It merely occurred to me . . \n A 'mental cause', of course, need not be a mental \n event, i.e., a thought or feeling or image; it might be a \n knock on the door. But if it is not a mental event, it must \n be something perceived by the person affected-e.g. the \n knock on the door must be heard-so if in this sense anyone \n wishes to say it is always a mental event, I have no objection. \n A mental cause is what someone would describe if he were \n asked the specific question: what produced this action or \nThis content downloaded from 132.174.234.36 on Fri, 05 Sep 2025 17:48:40 UTC\nAll use subject to https://about.jstor.org/terms\n\n INTENTION 325 \n thought or feeling in you? i.e., what did you see or hear or \n feel, or what ideas or images cropped up in your mind, and \n led up to it? I have isolated this notion of a mental cause \n because there is such a thing as this question with this sort \n of answer, and because I want to distinguish it from the \n ordinary senses of ' motive ' and ' intention', rather than \n because it is in itself of very great importance; for I believe \n that it is of very little. But it is important to have a clear \n idea of it, partly because a very natural conception of \n motive ' is that it is what moves (the very word suggests \n that)-glossed as ' what causes' a man's actions etc. And \n ' what causes ' them is perhaps then thought of as an event \n that brings the effect about-though how-i.e. whether it \n should be thought of as a kind of pushing in another \n medium, or in some other way-is of course completely \n obscure. \n In philosophy a distinction has sometimes been drawn \n between ' motives' and 'intentions in acting' as referring \n to quite different things. A man's intention is what he \n aims at or chooses; his motive is what determines the aim \n or choice; and I suppose that ' determines ' must here be \n another word for ' causes '. \n Popularly, ' motive' and ' intention ' are not treated as \n so distinct in meaning. E.g. we hear of ' the motive of \n gain '; some philosophers have wanted to say that such an \n expression must be elliptical; gain must be the intention, and \n desire of gain the motive. Asked for a motive, a man might \n say' I wanted to . . . 'which would please such philosophers; \n or 'I did it in order to . . . ' which would not; and yet \n the meaning of the two phrases is here identical. When a \n man's motives are called good, this may be in no way distinct \n from calling his intentions good-e.g. ' he only wanted to \n make peace among his relations'. \n Nevertheless there is even popularly a distinction \n between the meaning of ' motive ' and the meaning of \n 'intention'. E.g. if a man kills someone, he may be said \n to have done it out of love and pity, or to have done it out \n of-hatred; these might indeed be cast in the forms "to release \nThis content downloaded from 132.174.234.36 on Fri, 05 Sep 2025 17:48:40 UTC\nAll use subject to https://about.jstor.org/terms\n\n 326 G. E. M. ANSCOMBE \n him from this awful suffering', or 'to get rid of the swine' \n but though these are forms of expression suggesting objectives, \n they are perhaps expressive of the spirit in which the man \n killed rather than descriptive of the end to which the killing \n was a means-a future state of affairs to be produced by the \n killing. And this shows us part of the distinction that there \n is between the popular senses of motive and intention. We \n should say: popularly, ' motive for an action' has a rather \n wider and more diverse application than ' intention with \n which the action was done '. \n When a man says what his motive was, speaking popu- \n larly, and in a sense in which ' motive 'is not interchangeable \n with 'intention', he is not giving a 'mental cause' in the \n sense that I have given to that phrase. The fact that the \n mental causes were such-and-such may indeed help to make \n his claim intelligible. And further, though he may say \n that his motive was this or that one straight off and without \n lying-i.e. without saying what he knows or even half knows \n to be untrue-yet a consideration of various things, which \n may include the mental causes, might possibly lead both \n him and other people to judge that his declaration of his \n own motive was false. But it appears to me that the mental \n causes are seldom more than a very trivial item among the \n things that it would be reasonable to consider. As for the \n importance of considering the motives of an action, as \n opposed to considering the intention, I am very glad not to \n be writing either ethics or literary criticism, to which this \n question belongs. \n Motives may explain actions to us; but that is not to say \n that they ' determine ', in the sense of causing, actions. We \n do say: ' His love of truth caused him to . . . ' and similar \n things, and no doubt such expressions help us to think that \n a motive must be what produces or brings about a choice. \n But this means rather ' He did this in that he loved the \n truth '; it interprets his action. \n Someone who sees the confusions involved in radically \n distinguishing between motives and intentions and in \n defining motives, so distinct, as the determinants of choice, \nThis content downloaded from 132.174.234.36 on Fri, 05 Sep 2025 17:48:40 UTC\nAll use subject to https://about.jstor.org/terms\n\n INTENTION 327 \n may easily be inclined to deny both that there is any such \n thing as mental causality, and that ' motive ' means anything \n but intention. But both of these inclinations are mistaken. \n We shall create confusion if we do not notice (a) that \n phenomena deserving the name of mental causality exist, \n for we can make the question 'Why?' into a request for \n the sort of answer that I considered under that head; \n (b) that mental causality is not restricted to choices or \n voluntary or intentional actions but is of wider application; \n it is restricted to the wider field of things the agent knows \n about not as an observer, so that it includes some involuntary \n actions; (c) that motives are not mental causes; and (d) that \n there is application for ' motive ' other than the applications \n of ' the intention with which a man acts '. \n Revenge and gratitude are motives; if I kill a man as an \n act of revenge I may say I do it in order to be revenged, \n or that revenge is my object; but revenge is not some further \n thing obtained by killing him, it is rather that killing him is \n revenge. Asked why I killed him, I reply 'Because he \n killed my brother.' We might compare this answer, which \n describes a concrete past event, to the answer describing a \n concrete future state of affairs which we sometimes get in \n statements of objectives. It is the same with gratitude, \n and remorse, and pity for something specific. These motives \n differ from, say, love or curiosity or despair in just this way: \n something that has happened (or is at present happening) is \n given as the ground of an action or abstention that is good \n or bad for the person (it may be oneself, as with remorse) at \n whom it is aimed. And if we wanted to explain e.g. revenge, \n we should say it was harming someone because he had done \n one some harm; we should not need to add some description \n of the feelings prompting the action or of the thoughts that \n had gone with it. Whereas saying that someone does \n something out of, say, friendship cannot be explained in any \n such way. I will call revenge and gratitude and remorse \n and pity backward-looking motives, and contrast them with \n motive-in-general. \n Motive-in-general is a very difficult topic which I do \nThis content downloaded from 132.174.234.36 on Fri, 05 Sep 2025 17:48:40 UTC\nAll use subject to https://about.jstor.org/terms\n\n 328 G. E. M. ANSCOMBE \n not want to discuss at any length. Consider the statement \n that one motive for my signing a petition was admiration \n for its promoter, X. Asked ' Why did you sign it? ' I \n might well say ' Well, for one thing, X, who is promoting it, \n did . . . ' and describe what he did in an admiring way. \n I might add ' Of course, I know that is not a ground for \n signing it, but I am sure it was one of the things that most \n influenced me '-which need not mean: ' I thought explicitly \n of this before signing.' I say ' Consider this' really with a \n view to saying ' let us not consider it here.' It is too \n complicated. The account of motive popularised by \n Professor Ryle does not appear satisfactory. He recommends \n construing ' he boasted from vanity' as saying ' he boasted \n * . . and his doing so satisfies the law-like proposition that \n whenever he finds a chance of securing the admiration and \n envy of others, he does whatever he thinks will produce this \n admiration and envy.'2 This passage is rather curious and \n roundabout in its way of putting what it seems to say, but \n I can't understand it unless it implies that a man could not \n be said to have boasted from vanity unless he always behaved \n vainly, or at least very often did so. But this does not seem \n to be true. \n To give a motive (of the sort I have labelled ' motive-in- \n general', as opposed to backward-looking motives and \n intentions) is to say something like ' See the action in this \n light.' To explain one's own actions by an account indica- \n ting a motive is to put them in a certain light. This sort of \n explanation is often elicited by the question 'Why? ' The \n question whether the light in which one so puts one's action \n is a true light is a notoriously difficult one. \n The motives admiration, curiosity, spite, friendship, fear, \n love of truth, despair and a host of others are either of this \n extremely complicated kind, or are forward-looking or \n mixed. I call a motive forward-looking if it is an intention. \n For example, to say that someone did something for fear \n 2 The Concept of Mind, p. 89. \nThis content downloaded from 132.174.234.36 on Fri, 05 Sep 2025 17:48:40 UTC\nAll use subject to https://about.jstor.org/terms\n\n INTENTION 329 \n of . . . often comes to the same as saying he did so lest \n or in order that . . . should not happen. \n Leaving then, the topic of motive-in-general or ' inter- \n pretative' motive, let us return to backward-looking \n motives. Why is it that in revenge and gratitude, pity and \n remorse, the past event (or present situation) is a reason \n for acting, not just a mental cause? \n Now the most striking thing about these four is the way \n in which good and evil are involved in them. E.g. if I am \n grateful to someone, it is because he has done me some \n good, or at least I think he has, and I cannot show gratitude \n by something that I intend to harm him. In remorse, I \n hate some good things for myself; I could not express remorse \n by getting myself plenty of enjoyments, or for something that \n I did not find bad. If I do something out of revenge which \n is in fact advantageous rather than harmful to my enemy, \n my action, in its description of being advantageous to him, \n is involuntary. \n These facts are the clue to our present problem. If an \n action has to be thought of by the agent as doing good or \n harm of some sort, and the thing in the past as good or bad, \n in order for the thing in the past to be the reason for the \n action, then this reason shows not a mental cause but a \n motive. This will come out in the agent's elaborations on \n his answer to the question ' Why?' \n It might seem that this is not the most important point, \n but that the important point is that a proposed action can be \n questioned and the answer be a mention of something past. \n I I am going to kill him.'-' Why? '-' He killed my father.' \n But do we yet know what a proposal to act is; other than a \n prediction which the predictor justifies, if he does justify it, \n by mentioning a reason for acting? and the meaning of the \n expression ' reason for acting ' is precisely what we are at \n present trying to elucidate. Might one not predict mental \n causes and their effects? Or even their effects after the \n causes have occurred? E.g. 'This is going to make me \n angry.' Here it may be worth while to remark that it is a \n mistake to think one cannot choose whether to act from a \n 2L \nThis content downloaded from 132.174.234.36 on Fri, 05 Sep 2025 17:48:40 UTC\nAll use subject to https://about.jstor.org/terms\n\n 330 G. E. M. ANSCOMBE \n motive. Plato saying to a slave ' I should beat you if I were \n not angry' would be a case. Or a man might have a \n policy of never making remarks about a certain person \n because he could not speak about that man unenviously \n or unadmiringly. \n We have now distinguished between a backward-looking \n motive and a mental cause, and found that here at any rate \n what the agent reports in answer to the question ' Why? ' is \n a reason-for-acting if, in treating it as a reason, he conceives \n it as something good or bad, and his own action as doing \n good or harm. If you could e.g. show that either the action \n for which he has revenged himself, or that in which he.has \n revenged himself, was quite harmless or beneficial, he ceases \n to offer a reason, except prefaced by ' I thought '. If \n it is a proposed revenge he either gives it up or changes his \n reasons. No such discovery would affect an assertion of \n mental causality. Whether in general good and harm \n play an essential part in the concept of intention is something \n it still remains to find out. So far good and harm have only \n been introduced as making a clear difference between a \n backward-looking motive and a mental cause. When the \n question ' Why? ' about a present action is answered by \n description of a future state of affairs, this is already \n distinguished from a mental cause just by being future. \n Here there does not so far seem to be any need to characterise \n intention as being essentially of good or of harm. \n Now, however, let us consider this case: \n Why did you do it? \n Because he told me to. \n Is this a cause or a reason.? It appears to depend very much \n on what the action was or what the circumstances were. \n And we should often refuse to make any distinction at all \n between something's being a reason and its being a cause \n of the kind in question; for that was explained as what one \n is after if one asks the agent what led up to and issued in an \n action, but being given a, reason and accepting it might be \nThis content downloaded from 132.174.234.36 on Fri, 05 Sep 2025 17:48:40 UTC\nAll use subject to https://about.jstor.org/terms\n\n INTENTION 331 \n such a thing. And how would one distinguish between cause \n and reason in such a case as having hung one's hat on a peg \n because one's host said ' Hang up your hat on that peg ' ? \n Nor, I think, would it be correct to say that this is a reason \n and not a mental cause because of the understanding of the \n words that went into obeying the suggestion. Here one \n would be attempting a contrast between this case and, say, \n turning round at hearing someone say Boo But this case \n would not in fact be decisively on one side or the other; \n forced to say whether the noise was a reason or a cause, \n one would probably decide by how sudden one's reaction \n was. Further, there is no question of understanding a \n sentence in the following case: 'Why did you waggle your \n two fore-fingers by your temples? '-' Becasue he was doing \n it; ' but this is not particularly different from hanging one's \n hat up because one's host said ' Hang your hat up.' \n Roughly speaking, if one were forced to go on with the \n distinction, the more the action is described as a mere \n response, the more inclined one would be to the word \n ' cause '; while the more it is described as a response to \n something as having a significance that is dwelt on by the \n agent, or as a response surrounded with thoughts and \n questions, the more inclined one would be to use the word \n reason'. But in very many cases the distinction would have \n no point. \n This, however, does not mean that it never has a point. \n The cases on which we first grounded the distinction might \n be called ' full-blown ': that is to say, the case of e.g. revenge \n on the one hand, and of the thing that made me jump and \n knock a cup off a table on the other. Roughly speaking, \n it establishes something as a reason to object to it, not as \n when one says 'Noises should not make you jump like that: \n hadn't you better see a doctor? ' but in such a way as to \n link it up with motives and intentions. 'You did it because \n he told you to? But why do what he says? ' Answers like \n ' he has done a lot for me '; ' he is my father '; ' it would \n have been the worse for me if I hadn't ' give the original \n answer a place among reasons. Thus the full-blown \nThis content downloaded from 132.174.234.36 on Fri, 05 Sep 2025 17:48:40 UTC\nAll use subject to https://about.jstor.org/terms\n\n 332 G. E. M. ANSCOMBE \n cases are the right ones to consider in order to see the distinc- \n tion between reason and cause. But it is worth noticing that \n what is so commonly said, that reason and cause are every- \n where sharply distinct notions, is not true. \nThis content downloaded from 132.174.234.36 on Fri, 05 Sep 2025 17:48:40 UTC\nAll use subject to https://about.jstor.org/terms	 \n \nIntention\nAuthor(s): G. E. M. Anscombe\nSource: Proceedings of the Aristotelian Society, New Series, Vol. 57 (1956 - 1957), pp. 321-\n332\nPublished by: Oxford University Press on behalf of The Aristotelian Society\nStable URL: https://www.jstor.org/stable/4544583\nAccessed: 05-09-2025 17:48 UTC\n \nJSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide\nrange of content in a trusted digital archive. We use information technology and too...	en	0.9	uploaded	4747	26441	2025-09-05 19:01:50.765783	2025-09-05 19:01:50.765787	\N	1	\N	\N
66	Sutton and Barto - 2020 Reinforcement learning an introduction	file	document	\N	pdf	Sutton and Barto - 2020 Reinforcement learning an introduction.pdf	uploads/1f11908680af4eb58cc95b8716a28113_Sutton_and_Barto_-_2020_Reinforcement_learning_an_introduction.pdf	668963	\N	Chapter 1\nIntroduction\nThe idea that we learn by interacting with our environment is probably the ﬁrst to occur\nto us when we think about the nature of learning. When an infant plays, waves its arms,\nor looks about, it has no explicit teacher, but it does have a direct sensorimotor connection\nto its environment. Exercising this connection produces a wealth of information about\ncause and eﬀect, about the consequences of actions, and about what to do in order to\nachieve goals. Throughout our lives, such interactions are undoubtedly a major source\nof knowledge about our environment and ourselves. Whether we are learning to drive\na car or to hold a conversation, we are acutely aware of how our environment responds\nto what we do, and we seek to inﬂuence what happens through our behavior. Learning\nfrom interaction is a foundational idea underlying nearly all theories of learning and\nintelligence.\nIn this book we explore acomputational approach to learning from interaction. Rather\nthan directly theorizing about how people or animals learn, we primarily explore idealized\nlearning situations and evaluate the eﬀectiveness of various learning methods. That\nis, we adopt the perspective of an artiﬁcial intelligence researcher or engineer. We\nexplore designs for machines that are eﬀective in solving learning problems of scientiﬁc or\neconomic interest, evaluating the designs through mathematical analysis or computational\nexperiments. The approach we explore, calledreinforcement learning, is much more\nfocused on goal-directed learning from interaction than are other approaches to machine\nlearning.\n1.1 Reinforcement Learning\nReinforcement learning is learning what to do—how to map situations to actions—so\nas to maximize a numerical reward signal. The learner is not told which actions to\ntake, but instead must discover which actions yield the most reward by trying them. In\nthe most interesting and challenging cases, actions may aﬀect not only the immediate\nreward but also the next situation and, through that, all subsequent rewards. These two\ncharacteristics—trial-and-error search and delayed reward—are the two most important\ndistinguishing features of reinforcement learning.\n\n2 Chapter 1: Introduction\nReinforcement learning, like many topics whose names end with “ing,” such as machine\nlearning and mountaineering, is simultaneously a problem, a class of solution methods\nthat work well on the problem, and the ﬁeld that studies this problem and its solution\nmethods. It is convenient to use a single name for all three things, but at the same time\nessential to keep the three conceptually separate. In particular, the distinction between\nproblems and solution methods is very important in reinforcement learning; failing to\nmake this distinction is the source of many confusions.\nWe formalize the problem of reinforcement learning using ideas from dynamical sys-\ntems theory, speciﬁcally, as the optimal control of incompletely-known Markov decision\nprocesses. The details of this formalization must wait until Chapter 3, but the basic idea\nis simply to capture the most important aspects of the real problem facing a learning\nagent interacting over time with its environment to achieve a goal. A learning agent\nmust be able to sense the state of its environment to some extent and must be able to\ntake actions that aﬀect the state. The agent also must have a goal or goals relating to\nthe state of the environment. Markov decision processes are intended to include just\nthese three aspects—sensation, action, and goal—in their simplest possible forms without\ntrivializing any of them. Any method that is well suited to solving such problems we\nconsider to be a reinforcement learning method.\nReinforcement learning is diﬀerent fromsupervised learning, the kind of learning studied\nin most current research in the ﬁeld of machine learning. Supervised learning is learning\nfrom a training set of labeled examples provided by a knowledgable external supervisor.\nEach example is a description of a situation together with a speciﬁcation—the label—of\nthe correct action the system should take in that situation, which is often to identify a\ncategory to which the situation belongs. The object of this kind of learning is for the\nsystem to extrapolate, or generalize, its responses so that it acts correctly in situations\nnot present in the training set. This is an important kind of learning, but alone it is not\nadequate for learning from interaction. In interactive problems it is often impractical to\nobtain examples of desired behavior that are both correct and representative of all the\nsituations in which the agent has to act. In uncharted territory—where one would expect\nlearning to be most beneﬁcial—an agent must be able to learn from its own experience.\nReinforcement learning is also diﬀerent from what machine learning researchers call\nunsupervised learning, which is typically about ﬁnding structure hidden in collections of\nunlabeled data. The terms supervised learning and unsupervised learning would seem\nto exhaustively classify machine learning paradigms, but they do not. Although one\nmight be tempted to think of reinforcement learning as a kind of unsupervised learning\nbecause it does not rely on examples of correct behavior, reinforcement learning is trying\nto maximize a reward signal instead of trying to ﬁnd hidden structure. Uncovering\nstructure in an agent’s experience can certainly be useful in reinforcement learning, but by\nitself does not address the reinforcement learning problem of maximizing a reward signal.\nWe therefore consider reinforcement learning to be a third machine learning paradigm,\nalongside supervised learning and unsupervised learning and perhaps other paradigms.\n\n1.1. Reinforcement Learning 3\nOne of the challenges that arise in reinforcement learning, and not in other kinds\nof learning, is the trade-oﬀ between exploration and exploitation. To obtain a lot of\nreward, a reinforcement learning agent must prefer actions that it has tried in the past\nand found to be eﬀective in producing reward. But to discover such actions, it has to\ntry actions that it has not selected before. The agent has toexploit what it has already\nexperienced in order to obtain reward, but it also has toexplore in order to make better\naction selections in the future. The dilemma is that neither exploration nor exploitation\ncan be pursued exclusively without failing at the task. The agent must try a variety of\nactions and progressively favor those that appear to be best. On a stochastic task, each\naction must be tried many times to gain a reliable estimate of its expected reward. The\nexploration–exploitation dilemma has been intensively studied by mathematicians for\nmany decades, yet remains unresolved. For now, we simply note that the entire issue of\nbalancing exploration and exploitation does not even arise in supervised and unsupervised\nlearning, at least in the purest forms of these paradigms.\nAnother key feature of reinforcement learning is that it explicitly considers thewhole\nproblem of a goal-directed agent interacting with an uncertain environment. This is in\ncontrast to many approaches that consider subproblems without addressing how they\nmight ﬁt into a larger picture. For example, we have mentioned that many machine\nlearning researchers have studied supervised learning without specifying how such an\nability would ultimately be useful. Other researchers have developed theories of planning\nwith general goals, but without considering planning’s role in real-time decision making,\nor the question of where the predictive models necessary for planning would come from.\nAlthough these approaches have yielded many useful results, their focus on isolated\nsubproblems is a signiﬁcant limitation.\nReinforcement learning takes the opposite tack, starting with a complete, interactive,\ngoal-seeking agent. All reinforcement learning agents have explicit goals, can sense\naspects of their environments, and can choose actions to inﬂuence their environments.\nMoreover, it is usually assumed from the beginning that the agent has to operate despite\nsigniﬁcant uncertainty about the environment it faces. When reinforcement learning\ninvolves planning, it has to address the interplay between planning and real-time action\nselection, as well as the question of how environment models are acquired and improved.\nWhen reinforcement learning involves supervised learning, it does so for speciﬁc reasons\nthat determine which capabilities are critical and which are not. For learning research to\nmake progress, important subproblems have to be isolated and studied, but they should\nbe subproblems that play clear roles in complete, interactive, goal-seeking agents, even if\nall the details of the complete agent cannot yet be ﬁlled in.\nBy a complete, interactive, goal-seeking agent we do not always mean something like\na complete organism or robot. These are clearly examples, but a complete, interactive,\ngoal-seeking agent can also be a component of a larger behaving system. In this case, the\nagent directly interacts with the rest of the larger system and indirectly interacts with\nthe larger system’s environment. A simple example is an agent that monitors the charge\nlevel of robot’s battery and sends commands to the robot’s control architecture. This\nagent’s environment is the rest of the robot together with the robot’s environment. It is\n\n4 Chapter 1: Introduction\nimportant to look beyond the most obvious examples of agents and their environments\nto appreciate the generality of the reinforcement learning framework.\nOne of the most exciting aspects of modern reinforcement learning is its substantive\nand fruitful interactions with other engineering and scientiﬁc disciplines. Reinforcement\nlearning is part of a decades-long trend within artiﬁcial intelligence and machine learning\ntoward greater integration with statistics, optimization, and other mathematical subjects.\nFor example, the ability of some reinforcement learning methods to learn with parameter-\nized approximators addresses the classical “curse of dimensionality” in operations research\nand control theory. More distinctively, reinforcement learning has also interacted strongly\nwith psychology and neuroscience, with substantial beneﬁts going both ways. Of all the\nforms of machine learning, reinforcement learning is the closest to the kind of learning\nthat humans and other animals do, and many of the core algorithms of reinforcement\nlearning were originally inspired by biological learning systems. Reinforcement learning\nhas also given back, both through a psychological model of animal learning that better\nmatches some of the empirical data, and through an inﬂuential model of parts of the\nbrain’s reward system. The body of this book develops the ideas of reinforcement learning\nthat pertain to engineering and artiﬁcial intelligence, with connections to psychology and\nneuroscience summarized in Chapters 14 and 15.\nFinally, reinforcement learning is also part of a larger trend in artiﬁcial intelligence\nback toward simple general principles. Since the late 1960s, many artiﬁcial intelligence re-\nsearchers presumed that there are no general principles to be discovered, that intelligence is\ninstead due to the possession of a vast number of special purpose tricks, procedures, and\nheuristics. It was sometimes said that if we could just get enough relevant facts into a\nmachine, say one million, or one billion, then it would become intelligent. Methods based\non general principles, such as search or learning, were characterized as “weak methods,”\nwhereas those based on speciﬁc knowledge were called “strong methods.” This view is\nuncommon today. From our point of view, it was premature: too little eﬀort had been\nput into the search for general principles to conclude that there were none. Modern\nartiﬁcial intelligence now includes much research looking for general principles of learning,\nsearch, and decision making. It is not clear how far back the pendulum will swing, but\nreinforcement learning research is certainly part of the swing back toward simpler and\nfewer general principles of artiﬁcial intelligence.\n1.2 Examples\nA good way to understand reinforcement learning is to consider some of the examples\nand possible applications that have guided its development.\n• A master chess player makes a move. The choice is informed both by planning—\nanticipating possible replies and counterreplies—and by immediate, intuitive judg-\nments of the desirability of particular positions and moves.\n• An adaptive controller adjusts parameters of a petroleum reﬁnery’s operation in\nreal time. The controller optimizes the yield/cost/quality trade-oﬀ on the basis\n\n1.2. Examples 5\nof speciﬁed marginal costs without sticking strictly to the set points originally\nsuggested by engineers.\n• A gazelle calf struggles to its feet minutes after being born. Half an hour later it is\nrunning at 20 miles per hour.\n• A mobile robot decides whether it should enter a new room in search of more trash\nto collect or start trying to ﬁnd its way back to its battery recharging station. It\nmakes its decision based on the current charge level of its battery and how quickly\nand easily it has been able to ﬁnd the recharger in the past.\n• Phil prepares his breakfast. Closely examined, even this apparently mundane\nactivity reveals a complex web of conditional behavior and interlocking goal–subgoal\nrelationships: walking to the cupboard, opening it, selecting a cereal box, then\nreaching for, grasping, and retrieving the box. Other complex, tuned, interactive\nsequences of behavior are required to obtain a bowl, spoon, and milk carton. Each\nstep involves a series of eye movements to obtain information and to guide reaching\nand locomotion. Rapid judgments are continually made about how to carry the\nobjects or whether it is better to ferry some of them to the dining table before\nobtaining others. Each step is guided by goals, such as grasping a spoon or getting\nto the refrigerator, and is in service of other goals, such as having the spoon to eat\nwith once the cereal is prepared and ultimately obtaining nourishment. Whether\nhe is aware of it or not, Phil is accessing information about the state of his body\nthat determines his nutritional needs, level of hunger, and food preferences.\nThese examples share features that are so basic that they are easy to overlook. All\ninvolve interaction between an active decision-making agent and its environment, within\nwhich the agent seeks to achieve agoal despite uncertainty about its environment. The\nagent’s actions are permitted to aﬀect the future state of the environment (e.g., the\nnext chess position, the level of reservoirs of the reﬁnery, the robot’s next location and\nthe future charge level of its battery), thereby aﬀecting the actions and opportunities\navailable to the agent at later times. Correct choice requires taking into account indirect,\ndelayed consequences of actions, and thus may require foresight or planning.\nAt the same time, in all of these examples the eﬀects of actions cannot be fully predicted;\nthus the agent must monitor its environment frequently and react appropriately. For\nexample, Phil must watch the milk he pours into his cereal bowl to keep it from overﬂowing.\nAll these examples involve goals that are explicit in the sense that the agent can judge\nprogress toward its goal based on what it can sense directly. The chess player knows\nwhether or not he wins, the reﬁnery controller knows how much petroleum is being\nproduced, the gazelle calf knows when it falls, the mobile robot knows when its batteries\nrun down, and Phil knows whether or not he is enjoying his breakfast.\nIn all of these examples the agent can use its experience to improve its performance\nover time. The chess player reﬁnes the intuition he uses to evaluate positions, thereby\nimproving his play; the gazelle calf improves the eﬃciency with which it can run; Phil\nlearns to streamline making his breakfast. The knowledge the agent brings to the task at\nthe start—either from previous experience with related tasks or built into it by design or\n\n6 Chapter 1: Introduction\nevolution—inﬂuences what is useful or easy to learn, but interaction with the environment\nis essential for adjusting behavior to exploit speciﬁc features of the task.\n1.3 Elements of Reinforcement Learning\nBeyond the agent and the environment, one can identify four main subelements of a\nreinforcement learning system: apolicy,a reward signal,a value function, and, optionally,\na model of the environment.\nA policy deﬁnes the learning agent’s way of behaving at a given time. Roughly speaking,\na policy is a mapping from perceived states of the environment to actions to be taken\nwhen in those states. It corresponds to what in psychology would be called a set of\nstimulus–response rules or associations. In some cases the policy may be a simple function\nor lookup table, whereas in others it may involve extensive computation such as a search\nprocess. The policy is the core of a reinforcement learning agent in the sense that it alone\nis suﬃcient to determine behavior. In general, policies may be stochastic, specifying\nprobabilities for each action.\nA reward signaldeﬁnes the goal of a reinforcement learning problem. On each time\nstep, the environment sends to the reinforcement learning agent a single number called\nthe reward. The agent’s sole objective is to maximize the total reward it receives over\nthe long run. The reward signal thus deﬁnes what are the good and bad events for the\nagent. In a biological system, we might think of rewards as analogous to the experiences\nof pleasure or pain. They are the immediate and deﬁning features of the problem faced\nby the agent. The reward signal is the primary basis for altering the policy; if an action\nselected by the policy is followed by low reward, then the policy may be changed to\nselect some other action in that situation in the future. In general, reward signals may\nbe stochastic functions of the state of the environment and the actions taken.\nWhereas the reward signal indicates what is good in an immediate sense, avalue\nfunction speciﬁes what is good in the long run. Roughly speaking, thevalue of a state is\nthe total amount of reward an agent can expect to accumulate over the future, starting\nfrom that state. Whereas rewards determine the immediate, intrinsic desirability of\nenvironmental states, values indicate thelong-term desirability of states after taking into\naccount the states that are likely to follow and the rewards available in those states. For\nexample, a state might always yield a low immediate reward but still have a high value\nbecause it is regularly followed by other states that yield high rewards. Or the reverse\ncould be true. To make a human analogy, rewards are somewhat like pleasure (if high)\nand pain (if low), whereas values correspond to a more reﬁned and farsighted judgment\nof how pleased or displeased we are that our environment is in a particular state.\nRewards are in a sense primary, whereas values, as predictions of rewards, are secondary.\nWithout rewards there could be no values, and the only purpose of estimating values is to\nachieve more reward. Nevertheless, it is values with which we are most concerned when\nmaking and evaluating decisions. Action choices are made based on value judgments. We\nseek actions that bring about states of highest value, not highest reward, because these\nactions obtain the greatest amount of reward for us over the long run. Unfortunately, it\nis much harder to determine values than it is to determine rewards. Rewards are basically\ngiven directly by the environment, but values must be estimated and re-estimated from\n\n1.4. Limitations and Scope 7\nthe sequences of observations an agent makes over its entire lifetime. In fact, the most\nimportant component of almost all reinforcement learning algorithms we consider is a\nmethod for eﬃciently estimating values. The central role of value estimation is arguably\nthe most important thing that has been learned about reinforcement learning over the\nlast six decades.\nThe fourth and ﬁnal element of some reinforcement learning systems is amodel of\nthe environment. This is something that mimics the behavior of the environment, or\nmore generally, that allows inferences to be made about how the environment will behave.\nFor example, given a state and action, the model might predict the resultant next state\nand next reward. Models are used forplanning, by which we mean any way of deciding\non a course of action by considering possible future situations before they are actually\nexperienced. Methods for solving reinforcement learning problems that use models and\nplanning are calledmodel-based methods, as opposed to simplermodel-free methods that\nare explicitly trial-and-error learners—viewed as almost theopposite of planning. In\nChapter 8 we explore reinforcement learning systems that simultaneously learn by trial\nand error, learn a model of the environment, and use the model for planning. Modern\nreinforcement learning spans the spectrum from low-level, trial-and-error learning to\nhigh-level, deliberative planning.\n1.4 Limitations and Scope\nReinforcement learning relies heavily on the concept of state—as input to the policy and\nvalue function, and as both input to and output from the model. Informally, we can\nthink of the state as a signal conveying to the agent some sense of “how the environment\nis” at a particular time. The formal deﬁnition of state as we use it here is given by\nthe framework of Markov decision processes presented in Chapter 3. More generally,\nhowever, we encourage the reader to follow the informal meaning and think of the state\nas whatever information is available to the agent about its environment. In eﬀect, we\nassume that the state signal is produced by some preprocessing system that is nominally\npart of the agent’s environment. We do not address the issues of constructing, changing,\nor learning the state signal in this book (other than brieﬂy in Section 17.3). We take this\napproach not because we consider state representation to be unimportant, but in order\nto focus fully on the decision-making issues. In other words, our concern in this book is\nnot with designing the state signal, but with deciding what action to take as a function\nof whatever state signal is available.\nMost of the reinforcement learning methods we consider in this book are structured\naround estimating value functions, but it is not strictly necessary to do this to solve\nreinforcement learning problems. For example, solution methods such as genetic algo-\nrithms, genetic programming, simulated annealing, and other optimization methods never\nestimate value functions. These methods apply multiple static policies each interacting\nover an extended period of time with a separate instance of the environment. The policies\nthat obtain the most reward, and random variations of them, are carried over to the\nnext generation of policies, and the process repeats. We call theseevolutionary methods\nbecause their operation is analogous to the way biological evolution produces organisms\n\n8 Chapter 1: Introduction\nwith skilled behavior even if they do not learn during their individual lifetimes. If the\nspace of policies is suﬃciently small, or can be structured so that good policies are\ncommon or easy to ﬁnd—or if a lot of time is available for the search—then evolutionary\nmethods can be eﬀective. In addition, evolutionary methods have advantages on problems\nin which the learning agent cannot sense the complete state of its environment.\nOur focus is on reinforcement learning methods that learn while interacting with the\nenvironment, which evolutionary methods do not do. Methods able to take advantage\nof the details of individual behavioral interactions can be much more eﬃcient than\nevolutionary methods in many cases. Evolutionary methods ignore much of the useful\nstructure of the reinforcement learning problem: they do not use the fact that the policy\nthey are searching for is a function from states to actions; they do not notice which states\nan individual passes through during its lifetime, or which actions it selects. In some cases\nsuch information can be misleading (e.g., when states are misperceived), but more often it\nshould enable more eﬃcient search. Although evolution and learning share many features\nand naturally work together, we do not consider evolutionary methods by themselves to\nbe especially well suited to reinforcement learning problems and, accordingly, we do not\ncover them in this book.\n1.5 An Extended Example: Tic-Tac-Toe\nTo illustrate the general idea of reinforcement learning and contrast it with other ap-\nproaches, we next consider a single example in more detail.\n\n\n\n\n\nConsider the familiar child’s game of tic-tac-toe. Two players\ntake turns playing on a three-by-three board. One player plays\nXs and the other Os until one player wins by placing three marks\nin a row, horizontally, vertically, or diagonally, as the X player\nhas in the game shown to the right. If the board ﬁlls up with\nneither player getting three in a row, then the game is a draw.\nBecause a skilled player can play so as never to lose, let us assume\nthat we are playing against an imperfect player, one whose play\nis sometimes incorrect and allows us to win. For the moment, in\nfact, let us consider draws and losses to be equally bad for us. How might we construct a\nplayer that will ﬁnd the imperfections in its opponent’s play and learn to maximize its\nchances of winning?\nAlthough this is a simple problem, it cannot readily be solved in a satisfactory way\nthrough classical techniques. For example, the classical “minimax” solution from game\ntheory is not correct here because it assumes a particular way of playing by the opponent.\nFor example, a minimax player would never reach a game state from which it could\nlose, even if in fact it always won from that state because of incorrect play by the\nopponent. Classical optimization methods for sequential decision problems, such as\ndynamic programming, cancompute an optimal solution for any opponent, but require\nas input a complete speciﬁcation of that opponent, including the probabilities with which\nthe opponent makes each move in each board state. Let us assume that this information\nis not available a priori for this problem, as it is not for the vast majority of problems of\n\n1.5. An Extended Example: Tic-Tac-Toe 9\npractical interest. On the other hand, such information can be estimated from experience,\nin this case by playing many games against the opponent. About the best one can do\non this problem is ﬁrst to learn a model of the opponent’s behavior, up to some level of\nconﬁdence, and then apply dynamic programming to compute an optimal solution given\nthe approximate opponent model. In the end, this is not that diﬀerent from some of the\nreinforcement learning methods we examine later in this book.\nAn evolutionary method applied to this problem would directly search the space\nof possible policies for one with a high probability of winning against the opponent.\nHere, a policy is a rule that tells the player what move to make for every state of the\ngame—every possible conﬁguration of Xs and Os on the three-by-three board. For each\npolicy considered, an estimate of its winning probability would be obtained by playing\nsome number of games against the opponent. This evaluation would then direct which\npolicy or policies were considered next. A typical evolutionary method would hill-climb\nin policy space, successively generating and evaluating policies in an attempt to obtain\nincremental improvements. Or, perhaps, a genetic-style algorithm could be used that\nwould maintain and evaluate a population of policies. Literally hundreds of diﬀerent\noptimization methods could be applied.\nHere is how the tic-tac-toe problem would be approached with a method making use\nof a value function. First we would set up a table of numbers, one for each possible state\nof the game. Each number will be the latest estimate of the probability of our winning\nfrom that state. We treat this estimate as the state’svalue, and the whole table is the\nlearned value function. StateA has higher value than stateB, or is considered “better”\nthan stateB, if the current estimate of the probability of our winning fromA is higher\nthan it is fromB. Assuming we always play Xs, then for all states with three Xs in a row\nthe probability of winning is 1, because we have already won. Similarly, for all states\nwith three Os in a row, or that are ﬁlled up, the correct probability is 0, as we cannot\nwin from them. We set the initial values of all the other states to 0.5, representing a\nguess that we have a 50% chance of winning.\nWe then play many games against the opponent. To select our moves we examine the\nstates that would result from each of our possible moves (one for each blank space on the\nboard) and look up their current values in the table. Most of the time we movegreedily,\nselecting the move that leads to the state with greatest value, that is, with the highest\nestimated probability of winning. Occasionally, however, we select randomly from among\nthe other moves instead. These are calledexploratory moves because they cause us to\nexperience states that we might otherwise never see. A sequence of moves made and\nconsidered during a game can be diagrammed as in Figure 1.1.\nWhile we are playing, we change the values of the states in which we ﬁnd ourselves\nduring the game. We attempt to make them more accurate estimates of the probabilities\nof winning. To do this, we “back up” the value of the state after each greedy move to\nthe state before the move, as suggested by the arrows in Figure 1.1. More precisely, the\ncurrent value of the earlier state is updated to be closer to the value of the later state.\nThis can be done by moving the earlier state’s value a fraction of the way toward the\nvalue of the later state. If we letSt denote the state before the greedy move, andSt+1\nthe state after that move, then the update to the estimated value ofSt, denotedV(St),\n\n10 Chapter 1: Introduction\n\n\n\t\v\t \n\t\n\n\t\b\b\r\f\t \n\t\v\t \n\f\r\v\r\b\n\t\f\r\t\b\n\n\n\n\n\n\n\n\n\t\n\n\t\b\b\r\f\t \n\n\b\n\t\t\n\t\n\n\t\b\b\r\f\t \n\t\v\t \n\n\n\n\b\n\n\t\n\b\n\t\n\n\n\n\n \n\n\b\n\v\nFigure 1.1: A sequence of tic-tac-toe moves. The solid black lines represent the moves taken\nduring a game; the dashed lines represent moves that we (our reinforcement learning player)\nconsidered but did not make. The * indicates the move currently estimated to be the best. Our\nsecond move was an exploratory move, meaning that it was taken even though another sibling\nmove, the one leading toe∗, was ranked higher. Exploratory moves do not result in any learning,\nbut each of our other moves does, causing updates as suggested by the red arrows in which\nestimated values are moved up the tree from later nodes to earlier nodes as detailed in the text.\ncan be written as\nV(St) ← V(St)+ α\n[\nV(St+1)−V(St)\n]\n,\nwhere α is a small positive fraction called thestep-size parameter, which inﬂuences\nthe rate of learning. This update rule is an example of atemporal-diﬀerencelearning\nmethod, so called because its changes are based on a diﬀerence,V(St+1)−V(St), between\nestimates at two successive times.\nThe method described above performs quite well on this task. For example, if the\nstep-size parameter is reduced properly over time, then this method converges, for any\nﬁxed opponent, to the true probabilities of winning from each state given optimal play\nby our player. Furthermore, the moves then taken (except on exploratory moves) are in\nfact the optimal moves against this (imperfect) opponent. In other words, the method\nconverges to an optimal policy for playing the game against this opponent. If the step-size\nparameter is not reduced all the way to zero over time, then this player also plays well\nagainst opponents that slowly change their way of playing.\n\n1.5. An Extended Example: Tic-Tac-Toe 11\nThis example illustrates the diﬀerences between evolutionary methods and methods\nthat learn value functions. To evaluate a policy, an evolutionary method holds the policy\nﬁxed and plays many games against the opponent or simulates many games using a model\nof the opponent. The frequency of wins gives an unbiased estimate of the probability\nof winning with that policy, and can be used to direct the next policy selection. But\neach policy change is made only after many games, and only the ﬁnal outcome of each\ngame is used: what happensduring the games is ignored. For example, if the player wins,\nthen all of its behavior in the game is given credit, independently of how speciﬁc moves\nmight have been critical to the win. Credit is even given to moves that never occurred!\nValue function methods, in contrast, allow individual states to be evaluated. In the end,\nevolutionary and value function methods both search the space of policies, but learning a\nvalue function takes advantage of information available during the course of play.\nThis simple example illustrates some of the key features of reinforcement learning\nmethods. First, there is the emphasis on learning while interacting with an environment,\nin this case with an opponent player. Second, there is a clear goal, and correct behavior\nrequires planning or foresight that takes into account delayed eﬀects of one’s choices. For\nexample, the simple reinforcement learning player would learn to set up multi-move traps\nfor a shortsighted opponent. It is a striking feature of the reinforcement learning solution\nthat it can achieve the eﬀects of planning and lookahead without using a model of the\nopponent and without conducting an explicit search over possible sequences of future\nstates and actions.\nWhile this example illustrates some of the key features of reinforcement learning, it is\nso simple that it might give the impression that reinforcement learning is more limited\nthan it really is. Although tic-tac-toe is a two-person game, reinforcement learning\nalso applies in the case in which there is no external adversary, that is, in the case of\na “game against nature.” Reinforcement learning also is not restricted to problems in\nwhich behavior breaks down into separate episodes, like the separate games of tic-tac-toe,\nwith reward only at the end of each episode. It is just as applicable when behavior\ncontinues indeﬁnitely and when rewards of various magnitudes can be received at any\ntime. Reinforcement learning is also applicable to problems that do not even break down\ninto discrete time steps like the plays of tic-tac-toe. The general principles apply to\ncontinuous-time problems as well, although the theory gets more complicated and we\nomit it from this introductory treatment.\nTic-tac-toe has a relatively small, ﬁnite state set, whereas reinforcement learning can\nbe used when the state set is very large, or even inﬁnite. For example, Gerry Tesauro\n(1992, 1995) combined the algorithm described above with an artiﬁcial neural network to\nlearn to play backgammon, which has approximately 1020 states. With this many states\nit is impossible ever to experience more than a small fraction of them. Tesauro’s program\nlearned to play far better than any previous program and eventually better than the\nworld’s best human players (see Section 16.1). The artiﬁcial neural network provides the\nprogram with the ability to generalize from its experience, so that in new states it selects\nmoves based on information saved from similar states faced in the past, as determined\nby the network. How well a reinforcement learning system can work in problems with\nsuch large state sets is intimately tied to how appropriately it can generalize from past\n\n12 Chapter 1: Introduction\nexperience. It is in this role that we have the greatest need for supervised learning\nmethods within reinforcement learning. Artiﬁcial neural networks and deep learning\n(Section 9.7) are not the only, or necessarily the best, way to do this.\nIn this tic-tac-toe example, learning started with no prior knowledge beyond the\nrules of the game, but reinforcement learning by no means entails a tabula rasa view of\nlearning and intelligence. On the contrary, prior information can be incorporated into\nreinforcement learning in a variety of ways that can be critical for eﬃcient learning (e.g.,\nsee Sections 9.5, 17.4, and 13.1). We also have access to the true state in the tic-tac-toe\nexample, whereas reinforcement learning can also be applied when part of the state is\nhidden, or when diﬀerent states appear to the learner to be the same.\nFinally, the tic-tac-toe player was able to look ahead and know the states that would\nresult from each of its possible moves. To do this, it had to have a model of the game\nthat allowed it to foresee how its environment would change in response to moves that it\nmight never make. Many problems are like this, but in others even a short-term model\nof the eﬀects of actions is lacking. Reinforcement learning can be applied in either case.\nA model is not required, but models can easily be used if they are available or can be\nlearned (Chapter 8).\nOn the other hand, there are reinforcement learning methods that do not need any\nkind of environment model at all. Model-free systems cannot even think about how\ntheir environments will change in response to a single action. The tic-tac-toe player is\nmodel-free in this sense with respect to its opponent: it has no model of its opponent\nof any kind. Because models have to be reasonably accurate to be useful, model-free\nmethods can have advantages over more complex methods when the real bottleneck in\nsolving a problem is the diﬃculty of constructing a suﬃciently accurate environment\nmodel. Model-free methods are also important building blocks for model-based methods.\nIn this book we devote several chapters to model-free methods before we discuss how\nthey can be used as components of more complex model-based methods.\nReinforcement learning can be used at both high and low levels in a system. Although\nthe tic-tac-toe player learned only about the basic moves of the game, nothing prevents\nreinforcement learning from working at higher levels where each of the “actions” may\nitself be the application of a possibly elaborate problem-solving method. In hierarchical\nlearning systems, reinforcement learning can work simultaneously on several levels.\nExercise 1.1: Self-Play Suppose, instead of playing against a random opponent, the\nreinforcement learning algorithm described above played against itself, with both sides\nlearning. What do you think would happen in this case? Would it learn a diﬀerent policy\nfor selecting moves? □\nExercise 1.2: Symmetries Many tic-tac-toe positions appear diﬀerent but are really\nthe same because of symmetries. How might we amend the learning process described\nabove to take advantage of this? In what ways would this change improve the learning\nprocess? Now think again. Suppose the opponent did not take advantage of symmetries.\nIn that case, should we? Is it true, then, that symmetrically equivalent positions should\nnecessarily have the same value? □\nExercise 1.3: Greedy Play Suppose the reinforcement learning player wasgreedy,t h a ti s ,\nit always played the move that brought it to the position that it rated the best. Might it\n\n1.7. Early History of Reinforcement Learning 13\nlearn to play better, or worse, than a nongreedy player? What problems might occur?□\nExercise 1.4: Learning from Exploration Suppose learning updates occurred afterall\nmoves, including exploratory moves. If the step-size parameter is appropriately reduced\nover time (but not the tendency to explore), then the state values would converge to\na diﬀerent set of probabilities. What (conceptually) are the two sets of probabilities\ncomputed when we do, and when we do not, learn from exploratory moves? Assuming\nthat we do continue to make exploratory moves, which set of probabilities might be better\nto learn? Which would result in more wins? □\nExercise 1.5: Other Improvements Can you think of other ways to improve the reinforce-\nment learning player? Can you think of any better way to solve the tic-tac-toe problem\nas posed? □\n1.6 Summary\nReinforcement learning is a computational approach to understanding and automating\ngoal-directed learning and decision making. It is distinguished from other computational\napproaches by its emphasis on learning by an agent from direct interaction with its\nenvironment, without requiring exemplary supervision or complete models of the envi-\nronment. In our opinion, reinforcement learning is the ﬁrst ﬁeld to seriously address the\ncomputational issues that arise when learning from interaction with an environment in\norder to achieve long-term goals.\nReinforcement learning uses the formal framework of Markov decision processes to\ndeﬁne the interaction between a learning agent and its environment in terms of states,\nactions, and rewards. This framework is intended to be a simple way of representing\nessential features of the artiﬁcial intelligence problem. These features include a sense of\ncause and eﬀect, a sense of uncertainty and nondeterminism, and the existence of explicit\ngoals.\nThe concepts of value and value function are key to most of the reinforcement learning\nmethods that we consider in this book. We take the position that value functions\nare important for eﬃcient search in the space of policies. The use of value functions\ndistinguishes reinforcement learning methods from evolutionary methods that search\ndirectly in policy space guided by evaluations of entire policies.\n1.7 Early History of Reinforcement Learning\nThe early history of reinforcement learning has two main threads, both long and rich, that\nwere pursued independently before intertwining in modern reinforcement learning. One\nthread concerns learning by trial and error, and originated in the psychology of animal\nlearning. This thread runs through some of the earliest work in artiﬁcial intelligence\nand led to the revival of reinforcement learning in the early 1980s. The second thread\nconcerns the problem of optimal control and its solution using value functions and\ndynamic programming. For the most part, this thread did not involve learning. The\ntwo threads were mostly independent, but became interrelated to some extent around a\n\n14 Chapter 1: Introduction\nthird, less distinct thread concerning temporal-diﬀerence methods such as that used in\nthe tic-tac-toe example in this chapter. All three threads came together in the late 1980s\nto produce the modern ﬁeld of reinforcement learning as we present it in this book.\nThe thread focusing on trial-and-error learning is the one with which we are most\nfamiliar and about which we have the most to say in this brief history. Before doing that,\nhowever, we brieﬂy discuss the optimal control thread.\nThe term “optimal control” came into use in the late 1950s to describe the problem of\ndesigning a controller to minimize or maximize a measure of a dynamical system’s behavior\nover time. One of the approaches to this problem was developed in the mid-1950s by\nRichard Bellman and others through extending a nineteenth century theory of Hamilton\nand Jacobi. This approach uses the concepts of a dynamical system’s state and of a\nvalue function, or “optimal return function,” to deﬁne a functional equation, now often\ncalled the Bellman equation. The class of methods for solving optimal control problems\nby solving this equation came to be known as dynamic programming (Bellman, 1957a).\nBellman (1957b) also introduced the discrete stochastic version of the optimal control\nproblem known as Markov decision processes (MDPs). Ronald Howard (1960) devised\nthe policy iteration method for MDPs. All of these are essential elements underlying the\ntheory and algorithms of modern reinforcement learning.\nDynamic programming is widely considered the only feasible way of solving general\nstochastic optimal control problems. It suﬀers from what Bellman called “the curse of\ndimensionality,” meaning that its computational requirements grow exponentially with\nthe number of state variables, but it is still far more eﬃcient and more widely applicable\nthan any other general method. Dynamic programming has been extensively developed\nsince the late 1950s, including extensions to partially observable MDPs (surveyed by\nLovejoy, 1991), many applications (surveyed by White, 1985, 1988, 1993), approximation\nmethods (surveyed by Rust, 1996), and asynchronous methods (Bertsekas, 1982, 1983).\nMany excellent modern treatments of dynamic programming are available (e.g., Bertsekas,\n2005, 2012; Puterman, 1994; Ross, 1983; Whittle, 1982, 1983). Bryson (1996) provides\nan authoritative history of optimal control.\nConnections between optimal control and dynamic programming, on the one hand,\nand learning, on the other, were slow to be recognized. We cannot be sure about what\naccounted for this separation, but its main cause was likely the separation between\nthe disciplines involved and their diﬀerent goals. Also contributing may have been the\nprevalent view of dynamic programming as an oﬀ-line computation depending essentially\non accurate system models and analytic solutions to the Bellman equation. Further,\nthe simplest form of dynamic programming is a computation that proceeds backwards\nin time, making it diﬃcult to see how it could be involved in a learning process that\nmust proceed in a forward direction. Some of the earliest work in dynamic programming,\nsuch as that by Bellman and Dreyfus (1959), might now be classiﬁed as following\na learning approach. Witten’s (1977) work (discussed below) certainly qualiﬁes as a\ncombination of learning and dynamic-programming ideas. Werbos (1987) argued explicitly\nfor greater interrelation of dynamic programming and learning methods and for dynamic\nprogramming’s relevance to understanding neural and cognitive mechanisms. For us the\nfull integration of dynamic programming methods with online learning did not occur\n\n1.7. Early History of Reinforcement Learning 15\nuntil the work of Chris Watkins in 1989, whose treatment of reinforcement learning using\nthe MDP formalism has been widely adopted. Since then these relationships have been\nextensively developed by many researchers, most particularly by Dimitri Bertsekas and\nJohn Tsitsiklis (1996), who coined the term “neurodynamic programming” to refer to\nthe combination of dynamic programming and artiﬁcial neural networks. Another term\ncurrently in use is “approximate dynamic programming.” These various approaches\nemphasize diﬀerent aspects of the subject, but they all share with reinforcement learning\nan interest in circumventing the classical shortcomings of dynamic programming.\nWe consider all of the work in optimal control also to be, in a sense, work in reinforce-\nment learning. We deﬁne a reinforcement learning method as any eﬀective way of solving\nreinforcement learning problems, and it is now clear that these problems are closely\nrelated to optimal control problems, particularly stochastic optimal control problems\nsuch as those formulated as MDPs. Accordingly, we must consider the solution methods\nof optimal control, such as dynamic programming, also to be reinforcement learning\nmethods. Because almost all of the conventional methods require complete knowledge\nof the system to be controlled, it feels a little unnatural to say that they are part of\nreinforcement learning. On the other hand, many dynamic programming algorithms are\nincremental and iterative. Like learning methods, they gradually reach the correct answer\nthrough successive approximations. As we show in the rest of this book, these similarities\nare far more than superﬁcial. The theories and solution methods for the cases of complete\nand incomplete knowledge are so closely related that we feel they must be considered\ntogether as part of the same subject matter.\nLet us return now to the other major thread leading to the modern ﬁeld of reinforcement\nlearning, the thread centered on the idea of trial-and-error learning. We only touch on\nthe major points of contact here, taking up this topic in more detail in Section 14.3.\nAccording to American psychologist R. S. Woodworth (1938) the idea of trial-and-error\nlearning goes as far back as the 1850s to Alexander Bain’s discussion of learning by\n“groping and experiment” and more explicitly to the British ethologist and psychologist\nConway Lloyd Morgan’s 1894 use of the term to describe his observations of animal\nbehavior. Perhaps the ﬁrst to succinctly express the essence of trial-and-error learning as\na principle of learning was Edward Thorndike:\nOf several responses made to the same situation, those which are accompanied\nor closely followed by satisfaction to the animal will, other things being\nequal, be more ﬁrmly connected with the situation, so that, when it recurs,\nthey will be more likely to recur; those which are accompanied or closely\nfollowed by discomfort to the animal will, other things being equal, have their\nconnections with that situation weakened, so that, when it recurs, they will\nbe less likely to occur. The greater the satisfaction or discomfort, the greater\nthe strengthening or weakening of the bond. (Thorndike, 1911, p. 244)\nThorndike called this the “Law of Eﬀect” because it describes the eﬀect of reinforcing\nevents on the tendency to select actions. Thorndike later modiﬁed the law to better\naccount for subsequent data on animal learning (such as diﬀerences between the eﬀects\nof reward and punishment), and the law in its various forms has generated considerable\ncontroversy among learning theorists (e.g., see Gallistel, 2005; Herrnstein, 1970; Kimble,\n\n16 Chapter 1: Introduction\n1961, 1967; Mazur, 1994). Despite this, the Law of Eﬀect—in one form or another—is\nwidely regarded as a basic principle underlying much behavior (e.g., Hilgard and Bower,\n1975; Dennett, 1978; Campbell, 1960; Cziko, 1995). It is the basis of the inﬂuential\nlearning theories of Clark Hull (1943, 1952) and the inﬂuential experimental methods of\nB. F. Skinner (1938).\nThe term “reinforcement” in the context of animal learning came into use well after\nThorndike’s expression of the Law of Eﬀect, ﬁrst appearing in this context (to the best of\nour knowledge) in the 1927 English translation of Pavlov’s monograph on conditioned\nreﬂexes. Pavlov described reinforcement as the strengthening of a pattern of behavior due\nto an animal receiving a stimulus—a reinforcer—in an appropriate temporal relationship\nwith another stimulus or with a response. Some psychologists extended the idea of\nreinforcement to include weakening as well as strengthening of behavior, and extended\nthe idea of a reinforcer to include possibly the omission or termination of stimulus. To be\nconsidered a reinforcer, the strengthening or weakening must persist after the reinforcer\nis withdrawn; a stimulus that merely attracts an animal’s attention or that energizes its\nbehavior without producing lasting changes would not be considered a reinforcer.\nThe idea of implementing trial-and-error learning in a computer appeared among the\nearliest thoughts about the possibility of artiﬁcial intelligence. In a 1948 report, Alan\nTuring described a design for a “pleasure-pain system” that worked along the lines of the\nLaw of Eﬀect:\nWhen a conﬁguration is reached for which the action is undetermined, a\nrandom choice for the missing data is made and the appropriate entry is made\nin the description, tentatively, and is applied. When a pain stimulus occurs\nall tentative entries are cancelled, and when a pleasure stimulus occurs they\nare all made permanent. (Turing, 1948)\nMany ingenious electro-mechanical machines were constructed that demonstrated trial-\nand-error learning. The earliest may have been a machine built by Thomas Ross (1933)\nthat was able to ﬁnd its way through a simple maze and remember the path through\nthe settings of switches. In 1951 W. Grey Walter built a version of his “mechanical\ntortoise” (Walter, 1950) capable of a simple form of learning. In 1952 Claude Shannon\ndemonstrated a maze-running mouse named Theseus that used trial and error to ﬁnd\nits way through a maze, with the maze itself remembering the successful directions\nvia magnets and relays under its ﬂoor (see also Shannon, 1951). J. A. Deutsch (1954)\ndescribed a maze-solving machine based on his behavior theory (Deutsch, 1953) that\nhas some properties in common with model-based reinforcement learning (Chapter 8).\nIn his PhD dissertation, Marvin Minsky (1954) discussed computational models of\nreinforcement learning and described his construction of an analog machine composed of\ncomponents he called SNARCs (Stochastic Neural-Analog Reinforcement Calculators)\nmeant to resemble modiﬁable synaptic connections in the brain (Chapter 15). The\nweb sitecyberneticzoo.com contains a wealth of information on these and many other\nelectro-mechanical learning machines.\nBuilding electro-mechanical learning machines gave way to programming digital com-\nputers to perform various types of learning, some of which implemented trial-and-error\nlearning. Farley and Clark (1954) described a digital simulation of a neural-network\n\n1.7. Early History of Reinforcement Learning 17\nlearning machine that learned by trial and error. But their interests soon shifted from\ntrial-and-error learning to generalization and pattern recognition, that is, from reinforce-\nment learning to supervised learning (Clark and Farley, 1955). This began a pattern\nof confusion about the relationship between these types of learning. Many researchers\nseemed to believe that they were studying reinforcement learning when they were actually\nstudying supervised learning. For example, artiﬁcial neural network pioneers such as\nRosenblatt (1962) and Widrow and Hoﬀ (1960) were clearly motivated by reinforcement\nlearning—they used the language of rewards and punishments—but the systems they\nstudied were supervised learning systems suitable for pattern recognition and perceptual\nlearning. Even today, some researchers and textbooks minimize or blur the distinction\nbetween these types of learning. For example, some textbooks have used the term “trial-\nand-error” to describe artiﬁcial neural networks that learn from training examples. This\nis an understandable confusion because these networks use error information to update\nconnection weights, but this misses the essential character of trial-and-error learning as\nselecting actions on the basis of evaluative feedback that does not rely on knowledge of\nwhat the correct action should be.\nPartly as a result of these confusions, research into genuine trial-and-error learning\nbecame rare in the 1960s and 1970s, although there were notable exceptions. In the 1960s\nthe terms “reinforcement” and “reinforcement learning” were used in the engineering\nliterature for the ﬁrst time to describe engineering uses of trial-and-error learning (e.g.,\nWaltz and Fu, 1965; Mendel, 1966; Fu, 1970; Mendel and McClaren, 1970). Particularly\ninﬂuential was Minsky’s paper “Steps Toward Artiﬁcial Intelligence” (Minsky, 1961),\nwhich discussed several issues relevant to trial-and-error learning, including prediction,\nexpectation, and what he called thebasic credit-assignment problem for complex rein-\nforcement learning systems: How do you distribute credit for success among the many\ndecisions that may have been involved in producing it? All of the methods we discuss in\nthis book are, in a sense, directed toward solving this problem. Minsky’s paper is well\nworth reading today.\nIn the next few paragraphs we discuss some of the other exceptions and partial\nexceptions to the relative neglect of computational and theoretical study of genuine\ntrial-and-error learning in the 1960s and 1970s.\nOne exception was the work of the New Zealand researcher John Andreae, who\ndeveloped a system called STeLLA that learned by trial and error in interaction with\nits environment. This system included an internal model of the world and, later, an\n“internal monologue” to deal with problems of hidden state (Andreae, 1963, 1969; Andreae\nand Cashin, 1969). Andreae’s later work (1977) placed more emphasis on learning\nfrom a teacher, but still included learning by trial and error, with the generation of\nnovel events being one of the system’s goals. A feature of this work was a “leakback\nprocess,” elaborated more fully in Andreae (1998), that implemented a credit-assignment\nmechanism similar to the backing-up update operations that we describe. Unfortunately,\nhis pioneering research was not well known and did not greatly impact subsequent\nreinforcement learning research. Recent summaries are available (Andreae, 2017a,b).\nMore inﬂuential was the work of Donald Michie. In 1961 and 1963 he described a\nsimple trial-and-error learning system for learning how to play tic-tac-toe (or naughts\n\n18 Chapter 1: Introduction\nand crosses) called MENACE (for Matchbox Educable Naughts and Crosses Engine). It\nconsisted of a matchbox for each possible game position, each matchbox containing a\nnumber of colored beads, a diﬀerent color for each possible move from that position. By\ndrawing a bead at random from the matchbox corresponding to the current game position,\none could determine MENACE’s move. When a game was over, beads were added to\nor removed from the boxes used during play to reward or punish MENACE’s decisions.\nMichie and Chambers (1968) described another tic-tac-toe reinforcement learner called\nGLEE (Game Learning Expectimaxing Engine) and a reinforcement learning controller\ncalled BOXES. They applied BOXES to the task of learning to balance a pole hinged to\na movable cart on the basis of a failure signal occurring only when the pole fell or the\ncart reached the end of a track. This task was adapted from the earlier work of Widrow\nand Smith (1964), who used supervised learning methods, assuming instruction from a\nteacher already able to balance the pole. Michie and Chambers’s version of pole-balancing\nis one of the best early examples of a reinforcement learning task under conditions of\nincomplete knowledge. It inﬂuenced much later work in reinforcement learning, beginning\nwith some of our own studies (Barto, Sutton, and Anderson, 1983; Sutton, 1984). Michie\n(1974) consistently emphasized trial and error and learning as essential aspects of artiﬁcial\nintelligence.\nWidrow, Gupta, and Maitra (1973) modiﬁed the Least-Mean-Square (LMS) algorithm\nof Widrow and Hoﬀ (1960) to produce a reinforcement learning rule that could learn\nfrom success and failure signals instead of from training examples. They called this form\nof learning “selective bootstrap adaptation” and described it as “learning with a critic”\ninstead of “learning with a teacher.” They analyzed this rule and showed how it could\nlearn to play blackjack. This was an isolated foray into reinforcement learning by Widrow,\nwhose contributions to supervised learning were much more inﬂuential. Our use of the\nterm “critic” is derived from Widrow, Gupta, and Maitra’s paper. Buchanan, Mitchell,\nSmith, and Johnson (1978) independently used the term critic in the context of machine\nlearning (see also Dietterich and Buchanan, 1984), but for them a critic was an expert\nsystem able to do more than evaluate performance.\nResearch on learning automata had a more direct inﬂuence on the trial-and-error\nthread leading to modern reinforcement learning research. These are methods for solving\na nonassociative, purely selectional learning problem known as thek-armed banditby\nanalogy to a slot machine, or “one-armed bandit,” except withk levers (see Chapter 2).\nLearning automata are simple, low-memory machines for improving the probability\nof reward in these problems. Learning automata originated with work in the 1960s\nof the Russian mathematician and physicist M. L. Tsetlin and colleagues (published\nposthumously in Tsetlin, 1973) and has been extensively developed since then within\nengineering (see Narendra and Thathachar, 1974, 1989). These developments included the\nstudy ofstochastic learning automata, which are methods for updating action probabilities\non the basis of reward signals. Although not developed in the tradition of stochastic\nlearning automata, Harth and Tzanakou’s (1974) Alopex algorithm (forAlgorithm of\npattern extraction) is a stochastic method for detecting correlations between actions and\nreinforcement that inﬂuenced some of our early research (Barto, Sutton, and Brouwer,\n1981). Stochastic learning automata were foreshadowed by earlier work in psychology,\nbeginning with William Estes’ (1950) eﬀort toward a statistical theory of learning and\nfurther developed by others (e.g., Bush and Mosteller, 1955; Sternberg, 1963).\n\n1.7. Early History of Reinforcement Learning 19\nThe statistical learning theories developed in psychology were adopted by researchers in\neconomics, leading to a thread of research in that ﬁeld devoted to reinforcement learning.\nThis work began in 1973 with the application of Bush and Mosteller’s learning theory to\na collection of classical economic models (Cross, 1973). One goal of this research was to\nstudy artiﬁcial agents that act more like real people than do traditional idealized economic\nagents (Arthur, 1991). This approach expanded to the study of reinforcement learning\nin the context of game theory. Reinforcement learning in economics developed largely\nindependently of the early work in reinforcement learning in artiﬁcial intelligence, though\ngame theory remains a topic of interest in both ﬁelds (beyond the scope of this book).\nCamerer (2011) discusses the reinforcement learning tradition in economics, and Now´e,\nVrancx, and De Hauwere (2012) provide an overview of the subject from the point of view\nof multi-agent extensions to the approach that we introduce in this book. Reinforcement\nlearning in the context of game theory is a much diﬀerent subject than reinforcement\nlearning used in programs to play tic-tac-toe, checkers, and other recreational games. See,\nfor example, Szita (2012) for an overview of this aspect of reinforcement learning and\ngames.\nJohn Holland (1975) outlined a general theory of adaptive systems based on selectional\nprinciples. His early work concerned trial and error primarily in its nonassociative\nform, as in evolutionary methods and thek-armed bandit. In 1976 and more fully in\n1986, he introducedclassiﬁer systems, true reinforcement learning systems including\nassociation and value functions. A key component of Holland’s classiﬁer systems was\nthe “bucket-brigade algorithm” for credit assignment, which is closely related to the\ntemporal diﬀerence algorithm used in our tic-tac-toe example and discussed in Chapter 6.\nAnother key component was agenetic algorithm, an evolutionary method whose role was\nto evolve useful representations. Classiﬁer systems have been extensively developed by\nmany researchers to form a major branch of reinforcement learning research (reviewed by\nUrbanowicz and Moore, 2009), but genetic algorithms—which we do not consider to be\nreinforcement learning systems by themselves—have received much more attention, as\nhave other approaches to evolutionary computation (e.g., Fogel, Owens and Walsh, 1966;\nKoza, 1992).\nThe individual most responsible for reviving the trial-and-error thread of reinforcement\nlearning within artiﬁcial intelligence was Harry Klopf (1972, 1975, 1982). Klopf recognized\nthat essential aspects of adaptive behavior were being lost as learning researchers came\nto focus almost exclusively on supervised learning. What was missing, according to\nKlopf, were the hedonic aspects of behavior: the drive to achieve some result from the\nenvironment, to control the environment toward desired ends and away from undesired\nends (see Section 15.9). This is the essential idea of trial-and-error learning. Klopf’s\nideas were especially inﬂuential on the authors because our assessment of them (Barto\nand Sutton, 1981a) led to our appreciation of the distinction between supervised and\nreinforcement learning, and to our eventual focus on reinforcement learning. Much of\nthe early work that we and colleagues accomplished was directed toward showing that\nreinforcement learning and supervised learning were indeed diﬀerent (Barto, Sutton, and\nBrouwer, 1981; Barto and Sutton, 1981b; Barto and Anandan, 1985). Other studies\nshowed how reinforcement learning could address important problems in artiﬁcial neural\n\n20 Chapter 1: Introduction\nnetwork learning, in particular, how it could produce learning algorithms for multilayer\nnetworks (Barto, Anderson, and Sutton, 1982; Barto and Anderson, 1985; Barto, 1985,\n1986; Barto and Jordan, 1987; see Section 15.10).\nWeturnnowtothethirdthreadtothehistoryofreinforcementlearning, thatconcerning\ntemporal-diﬀerence learning. Temporal-diﬀerence learning methods are distinctive in\nbeing driven by the diﬀerence between temporally successive estimates of the same\nquantity—for example, of the probability of winning in the tic-tac-toe example. This\nthread is smaller and less distinct than the other two, but it has played a particularly\nimportant role in the ﬁeld, in part because temporal-diﬀerence methods seem to be new\nand unique to reinforcement learning.\nThe origins of temporal-diﬀerence learning are in part in animal learning psychology,\nin particular, in the notion ofsecondary reinforcers. A secondary reinforcer is a stimulus\nthat has been paired with a primary reinforcer such as food or pain and, as a result, has\ncome to take on similar reinforcing properties. Minsky (1954) may have been the ﬁrst to\nrealize that this psychological principle could be important for artiﬁcial learning systems.\nArthur Samuel (1959) was the ﬁrst to propose and implement a learning method that\nincluded temporal-diﬀerence ideas, as part of his celebrated checkers-playing program\n(Section 16.2).\nSamuel made no reference to Minsky’s work or to possible connections to animal\nlearning. His inspiration apparently came from Claude Shannon’s (1950) suggestion that\na computer could be programmed to use an evaluation function to play chess, and that it\nmight be able to improve its play by modifying this function online. (It is possible that\nthese ideas of Shannon’s also inﬂuenced Bellman, but we know of no evidence for this.)\nMinsky (1961) extensively discussed Samuel’s work in his “Steps” paper, suggesting the\nconnection to secondary reinforcement theories, both natural and artiﬁcial.\nAs we have discussed, in the decade following the work of Minsky and Samuel, little\ncomputationalworkwasdoneontrial-and-errorlearning, andapparentlynocomputational\nwork at all was done on temporal-diﬀerence learning. In 1972, Klopf brought trial-and-\nerror learning together with an important component of temporal-diﬀerence learning.\nKlopf was interested in principles that would scale to learning in large systems, and thus\nwas intrigued by notions of local reinforcement, whereby subcomponents of an overall\nlearning system could reinforce one another. He developed the idea of “generalized\nreinforcement,” whereby every component (nominally, every neuron) views all of its\ninputs in reinforcement terms: excitatory inputs as rewards and inhibitory inputs as\npunishments. This is not the same idea as what we now know as temporal-diﬀerence\nlearning, and in retrospect it is farther from it than was Samuel’s work. On the other\nhand, Klopf linked the idea with trial-and-error learning and related it to the massive\nempirical database of animal learning psychology.\nSutton (1978a,b,c) developed Klopf’s ideas further, particularly the links to animal\nlearning theories, describing learning rules driven by changes in temporally successive\npredictions. He and Barto reﬁned these ideas and developed a psychological model of\nclassical conditioning based on temporal-diﬀerence learning (Sutton and Barto, 1981a;\nBarto and Sutton, 1982). There followed several other inﬂuential psychological models of\nclassical conditioning based on temporal-diﬀerence learning (e.g., Klopf, 1988; Moore et al.,\n\n1.7. Early History of Reinforcement Learning 21\n1986; Sutton and Barto, 1987, 1990). Some neuroscience models developed at this time\nare well interpreted in terms of temporal-diﬀerence learning (Hawkins and Kandel, 1984;\nByrne, Gingrich, and Baxter, 1990; Gelperin, Hopﬁeld, and Tank, 1985; Tesauro, 1986;\nFriston et al., 1994), although in most cases there was no historical connection.\nOur early work on temporal-diﬀerence learning was strongly inﬂuenced by animal\nlearning theories and by Klopf’s work. Relationships to Minsky’s “Steps” paper and to\nSamuel’s checkers players were recognized only afterward. By 1981, however, we were\nfully aware of all the prior work mentioned above as part of the temporal-diﬀerence and\ntrial-and-error threads. At this time we developed a method for using temporal-diﬀerence\nlearning combined with trial-and-error learning, known as theactor–critic architecture,\nand applied this method to Michie and Chambers’s pole-balancing problem (Barto,\nSutton, and Anderson, 1983). This method was extensively studied in Sutton’s (1984)\nPhD dissertation and extended to use backpropagation neural networks in Anderson’s\n(1986) PhD dissertation. Around this time, Holland (1986) incorporated temporal-\ndiﬀerence ideas explicitly into his classiﬁer systems in the form of his bucket-brigade\nalgorithm. A key step was taken by Sutton (1988) by separating temporal-diﬀerence\nlearning from control, treating it as a general prediction method. That paper also\nintroduced the TD(λ) algorithm and proved some of its convergence properties.\nAs we were ﬁnalizing our work on the actor–critic architecture in 1981, we discovered\na paper by Ian Witten (1977, 1976a) which appears to be the earliest publication of a\ntemporal-diﬀerence learning rule. He proposed the method that we now call tabular TD(0)\nfor use as part of an adaptive controller for solving MDPs. This work was ﬁrst submitted\nfor journal publication in 1974 and also appeared in Witten’s 1976 PhD dissertation.\nWitten’s work was a descendant of Andreae’s early experiments with STeLLA and other\ntrial-and-error learning systems. Thus, Witten’s 1977 paper spanned both major threads\nof reinforcement learning research—trial-and-error learning and optimal control—while\nmaking a distinct early contribution to temporal-diﬀerence learning.\nThe temporal-diﬀerence and optimal control threads were fully brought together\nin 1989 with Chris Watkins’s development of Q-learning. This work extended and\nintegrated prior work in all three threads of reinforcement learning research. Paul Werbos\n(1987) contributed to this integration by arguing for the convergence of trial-and-error\nlearning and dynamic programming since 1977. By the time of Watkins’s work there had\nbeen tremendous growth in reinforcement learning research, primarily in the machine\nlearning subﬁeld of artiﬁcial intelligence, but also in artiﬁcial neural networks and artiﬁcial\nintelligence more broadly. In 1992, the remarkable success of Gerry Tesauro’s backgammon\nplaying program, TD-Gammon, brought additional attention to the ﬁeld.\nIn the time since publication of the ﬁrst edition of this book, a ﬂourishing subﬁeld of\nneuroscience developed that focuses on the relationship between reinforcement learning\nalgorithms and reinforcement learning in the nervous system. Most responsible for this is\nan uncanny similarity between the behavior of temporal-diﬀerence algorithms and the\nactivity of dopamine producing neurons in the brain, as pointed out by a number of\nresearchers (Friston et al., 1994; Barto, 1995a; Houk, Adams, and Barto, 1995; Montague,\nDayan, and Sejnowski, 1996; and Schultz, Dayan, and Montague, 1997). Chapter 15\nprovides an introduction to this exciting aspect of reinforcement learning. Other important\n\n22 Chapter 1: Introduction\ncontributions made in the recent history of reinforcement learning are too numerous to\nmention in this brief account; we cite many of these at the end of the individual chapters\nin which they arise.\nBibliographical Remarks\nFor additional general coverage of reinforcement learning, we refer the reader to the\nbooks by Szepesv´ari (2010), Bertsekas and Tsitsiklis (1996), Kaelbling (1993a), and\nSugiyama, Hachiya, andMorimura(2013). Booksthattakeacontroloroperationsresearch\nperspective include those of Si, Barto, Powell, and Wunsch (2004), Powell (2011), Lewis\nand Liu (2012), and Bertsekas (2012). Cao’s (2009) review places reinforcement learning\nin the context of other approaches to learning and optimization of stochastic dynamic\nsystems. Three special issues of the journalMachine Learningfocus on reinforcement\nlearning: Sutton (1992a), Kaelbling (1996), and Singh (2002). Useful surveys are provided\nby Barto (1995b); Kaelbling, Littman, and Moore (1996); and Keerthi and Ravindran\n(1997). The volume edited by Weiring and van Otterlo (2012) provides an excellent\noverview of recent developments.\n1.2 The example of Phil’s breakfast in this chapter was inspired by Agre (1988).\n1.5 The temporal-diﬀerence method used in the tic-tac-toe example is developed in\nChapter 6.	Chapter 1\nIntroduction\nThe idea that we learn by interacting with our environment is probably the ﬁrst to occur\nto us when we think about the nature of learning. When an infant plays, waves its arms,\nor looks about, it has no explicit teacher, but it does have a direct sensorimotor connection\nto its environment. Exercising this connection produces a wealth of information about\ncause and eﬀect, about the consequences of actions, and about what to do in order to\nachieve goals. Throughout our lives...	en	0.9	uploaded	11370	73256	2025-09-05 19:23:59.406631	2025-09-05 19:23:59.406634	\N	1	\N	\N
\.


--
-- Data for Name: domains; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.domains (id, uuid, name, display_name, namespace_uri, description, metadata, is_active, created_at, updated_at) FROM stdin;
\.


--
-- Data for Name: experiment_documents; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.experiment_documents (experiment_id, document_id, added_at) FROM stdin;
31	66	2025-09-05 21:05:09.665165
31	65	2025-09-05 21:05:09.667186
31	64	2025-09-05 21:05:09.669358
31	63	2025-09-05 21:05:09.671296
\.


--
-- Data for Name: experiment_references; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.experiment_references (experiment_id, reference_id, include_in_analysis, added_at, notes) FROM stdin;
\.


--
-- Data for Name: experiments; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.experiments (id, name, description, experiment_type, configuration, status, results, results_summary, created_at, updated_at, started_at, completed_at, user_id) FROM stdin;
31	Agent Temporal Analysis		temporal_evolution	{"period_generation_method": "manual", "start_year": 1950, "end_year": 2024, "period": 10, "time_periods": [1950, 1960, 1970, 1980, 1990, 2000, 2010, 2020, 2024], "track_entities": true, "track_frequency": true, "target_terms": ["agent"], "temporal_data": {}}	completed	{"document_count": 4, "total_words": 68750, "experiment_type": "temporal_evolution", "timestamp": "2025-09-05T23:25:57.148993"}	Analyzed 4 documents with 68750 total words.	2025-09-05 21:05:09.65586	2025-09-05 23:25:57.151577	2025-09-05 23:25:57.140984	2025-09-05 23:25:57.15119	1
\.


--
-- Data for Name: extracted_entities; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.extracted_entities (id, entity_text, entity_type, entity_subtype, context_before, context_after, sentence, start_position, end_position, paragraph_number, sentence_number, confidence_score, extraction_method, properties, language, normalized_form, created_at, updated_at, processing_job_id, text_segment_id) FROM stdin;
\.


--
-- Data for Name: fuzziness_adjustments; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.fuzziness_adjustments (id, term_version_id, original_score, adjusted_score, adjustment_reason, adjusted_by, created_at) FROM stdin;
\.


--
-- Data for Name: ontologies; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.ontologies (id, uuid, domain_id, name, base_uri, description, is_base, is_editable, parent_ontology_id, ontology_type, metadata, created_at, updated_at) FROM stdin;
\.


--
-- Data for Name: ontology_entities; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.ontology_entities (id, ontology_id, entity_type, uri, label, comment, parent_uri, domain, range, properties, embedding, created_at) FROM stdin;
\.


--
-- Data for Name: ontology_mappings; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.ontology_mappings (id, ontology_uri, concept_label, concept_definition, parent_concepts, child_concepts, related_concepts, mapping_confidence, mapping_method, mapping_source, semantic_type, domain, properties, is_verified, verified_by, verification_notes, alternative_mappings, created_at, updated_at, verified_at, extracted_entity_id) FROM stdin;
\.


--
-- Data for Name: ontology_versions; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.ontology_versions (id, ontology_id, version_number, version_tag, content, content_hash, change_summary, created_by, created_at, is_current, is_draft, workflow_status, metadata) FROM stdin;
\.


--
-- Data for Name: processing_jobs; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.processing_jobs (id, job_type, job_name, provider, model, parameters, status, progress_percent, current_step, total_steps, result_data, result_summary, error_message, error_details, retry_count, max_retries, tokens_used, processing_time, cost_estimate, created_at, started_at, completed_at, updated_at, user_id, document_id, parent_job_id) FROM stdin;
\.


--
-- Data for Name: provenance_chains; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.provenance_chains (id, entity_id, entity_type, was_derived_from, derivation_activity, derivation_metadata, created_at) FROM stdin;
\.


--
-- Data for Name: search_history; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.search_history (id, query, query_type, results_count, execution_time, user_id, ip_address, created_at) FROM stdin;
\.


--
-- Data for Name: semantic_drift_activities; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.semantic_drift_activities (id, activity_type, start_period, end_period, temporal_scope_years, used_entity, generated_entity, was_associated_with, drift_metrics, detection_algorithm, algorithm_parameters, started_at_time, ended_at_time, activity_status, drift_detected, drift_magnitude, drift_type, evidence_summary, created_by, created_at) FROM stdin;
\.


--
-- Data for Name: term_version_anchors; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.term_version_anchors (id, term_version_id, context_anchor_id, similarity_score, rank_in_neighborhood, created_at) FROM stdin;
5d96a7a6-5998-4cf8-bb0f-89b35192b251	703b355f-e602-40ad-a470-33a89d008dd8	e52c0e9a-c7b1-43c1-9db8-1e2be5f7af1a	\N	\N	\N
3d23291a-f7b7-498c-b7a1-d7635177ab4a	703b355f-e602-40ad-a470-33a89d008dd8	99267944-387f-4c8d-a0ba-62bbc69d2f4d	\N	\N	\N
359ba9c5-69ec-401c-a200-48e52b65bede	703b355f-e602-40ad-a470-33a89d008dd8	0bd2bb6f-36f3-46cf-9b60-d7ab53b77fa5	\N	\N	\N
6d216341-693d-4cde-b0a0-c613ee9a75f3	703b355f-e602-40ad-a470-33a89d008dd8	03406a50-523c-43c3-aeb4-e151c60f442a	\N	\N	\N
61102e2b-4bfb-47ac-8e71-13e412bca382	9bb70015-82d9-4393-b3ad-da13b1702378	ded8a9b4-6329-4289-8481-f3d3a580821b	\N	\N	\N
bb7eabfb-6475-4da4-b3d7-ce8c060619f5	9bb70015-82d9-4393-b3ad-da13b1702378	e75826c3-2f6a-447e-b584-70fd0278bea9	\N	\N	\N
55ed757d-3beb-4c66-98be-5f8d08376955	9bb70015-82d9-4393-b3ad-da13b1702378	acc4e4ad-04fc-42b6-9f37-b70e132b26f4	\N	\N	\N
d8e2e9df-9904-430c-8cf3-71ffa40ac399	9bb70015-82d9-4393-b3ad-da13b1702378	14c18a05-b195-46a3-a5b0-bc3381db0894	\N	\N	\N
\.


--
-- Data for Name: term_versions; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.term_versions (id, term_id, temporal_period, temporal_start_year, temporal_end_year, meaning_description, context_anchor, original_context_anchor, fuzziness_score, confidence_level, certainty_notes, corpus_source, source_documents, extraction_method, generated_at_time, was_derived_from, derivation_type, version_number, is_current, created_by, created_at, neighborhood_overlap, positional_change, similarity_reduction, source_citation) FROM stdin;
703b355f-e602-40ad-a470-33a89d008dd8	8ca94d22-caec-4b8c-81e1-205d8cd5a0bb	2025	\N	\N	a usually young man who engages in rowdy or violent behavior especially as part of a group or gang : ruffian, hoodlum	["hooligan", "usually", "young", "engages"]	\N	\N	medium	\N	\N	\N	manual	2025-08-24 14:03:22.101585-04	\N	\N	1	t	1	2025-08-24 14:03:22.102248-04	\N	\N	\N	Merriam-Webster.com Dictionary, s.v. "hooligan," accessed August 24, 2025, https://www.merriam-webster.com/dictionary/hooligan.
9bb70015-82d9-4393-b3ad-da13b1702378	dc997a45-6713-4bb7-b37c-d2ac0da6f764	2025	\N	\N	a unit of language that native speakers can identify	["word", "language unit", "charade", "meronym"]	\N	\N	medium	\N	\N	\N	manual	2025-09-02 14:12:42.807819-04	\N	\N	1	t	1	2025-09-02 14:12:42.808746-04	\N	\N	\N	WordNet: A Lexical Database for English. Princeton University. Synset: word.n.01
377cf87f-c01f-4504-b5a1-5528d1a0171a	d36546b4-c1d1-4faa-aa7a-aec75e906917	1957_philosophy	1957	1967	Entity capable of deliberate action with moral responsibility. Anscombe establishes agency through intentional action, where agents bear capacity for purposeful acts distinguished from mere occurrences.	["intentional action", "moral responsibility", "deliberate choice", "purposeful acts", "philosophical agency"]	["intentionality", "moral_responsibility", "deliberate_action", "purposeful_behavior"]	\N	high	\N	\N	\N	manual_academic_curation	2025-09-06 00:24:36.30907-04	\N	\N	1	t	\N	2025-09-06 00:24:36.307794-04	\N	\N	\N	Anscombe, G.E.M. (1957). Intention. Oxford: Basil Blackwell.
41efa2ae-dcf1-4a7a-8d75-f0fccc4240da	d36546b4-c1d1-4faa-aa7a-aec75e906917	1976_economics	1976	1986	Party in contractual relationship who acts on behalf of a principal with potential conflicts of interest. Introduces principal-agent framework revolutionizing organizational theory.	["principal-agent relationship", "contractual authority", "information asymmetry", "moral hazard", "incentive alignment", "organizational theory"]	["contractual_authority", "information_asymmetry", "moral_hazard", "incentive_alignment"]	\N	high	\N	\N	\N	manual_academic_curation	2025-09-06 00:24:36.31156-04	\N	\N	1	t	\N	2025-09-06 00:24:36.311015-04	\N	\N	\N	Jensen, M.C. & Meckling, W.H. (1976). Theory of the firm: Managerial behavior, agency costs and ownership structure. Journal of Financial Economics, 3(4), 305-360.
397f8cfd-6a8a-4a67-b39f-8ea9ba086496	d36546b4-c1d1-4faa-aa7a-aec75e906917	1995_computer_science	1995	2005	Autonomous computational entity capable of independent action in dynamic environments. Marks transition from human role to independent computational artifact with properties of autonomy, reactivity, and social ability.	["artificial intelligence", "autonomous systems", "computational entities", "multi-agent systems", "intelligent behavior", "software agents"]	["autonomy", "reactivity", "social_ability", "computational_independence"]	\N	high	\N	\N	\N	manual_academic_curation	2025-09-06 00:24:36.312734-04	\N	\N	1	t	\N	2025-09-06 00:24:36.312371-04	\N	\N	\N	Wooldridge, M. & Jennings, N.R. (1995). Intelligent agents: Theory and practice. Knowledge Engineering Review, 10(2), 115-152.
2772abf8-6947-4aa7-a91e-2b0c7f40487f	d36546b4-c1d1-4faa-aa7a-aec75e906917	2018_machine_learning	2018	2028	Learning optimization entity that maximizes cumulative reward through environmental interaction. Agents defined by ability to learn from interaction, maintain state representations, and execute policies.	["reinforcement learning", "optimization", "reward maximization", "policy functions", "state representation", "value estimation", "learning systems"]	["reward_optimization", "environmental_interaction", "policy_execution", "state_representation"]	\N	high	\N	\N	\N	manual_academic_curation	2025-09-06 00:24:36.313767-04	\N	\N	1	t	\N	2025-09-06 00:24:36.31352-04	\N	\N	\N	Sutton, R.S. & Barto, A.G. (2018). Reinforcement Learning: An Introduction, 2nd Edition. MIT Press.
\.


--
-- Data for Name: terms; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.terms (id, term_text, entry_date, status, created_by, updated_by, created_at, updated_at, description, etymology, notes, research_domain, selection_rationale, historical_significance) FROM stdin;
8ca94d22-caec-4b8c-81e1-205d8cd5a0bb	hooligan	2025-08-24 14:03:22.10073-04	active	1	\N	2025-08-24 14:03:22.100732-04	2025-08-24 14:03:22.100733-04	\N	\N			\N	\N
dc997a45-6713-4bb7-b37c-d2ac0da6f764	word	2025-09-02 14:12:42.804564-04	active	1	\N	2025-09-02 14:12:42.804566-04	2025-09-02 14:12:42.804567-04	\N	\N		Linguistics	\N	\N
d36546b4-c1d1-4faa-aa7a-aec75e906917	agent	2025-09-05 20:47:09.074219-04	active	4	\N	2025-09-05 20:47:09.074221-04	2025-09-05 20:24:36.302508-04	Semantic evolution across philosophy, economics, and computer science as documented in foundational academic works	\N	\N	interdisciplinary_academic	Key term showing semantic evolution across multiple disciplines	Evolves from philosophical concept to economic framework to AI systems
\.


--
-- Data for Name: text_segments; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.text_segments (id, content, segment_type, segment_number, start_position, end_position, parent_segment_id, level, word_count, character_count, sentence_count, language, language_confidence, embedding, embedding_model, processed, processing_notes, topics, keywords, sentiment_score, complexity_score, created_at, updated_at, processed_at, document_id) FROM stdin;
\.


--
-- Data for Name: users; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.users (id, username, email, password_hash, first_name, last_name, organization, is_active, is_admin, created_at, updated_at, last_login) FROM stdin;
2	test_user	test@example.com	scrypt:32768:8:1$7co0CsFaL4Ci2PCP$dbfb3c2a20be1aa55e7802d17fdc3ca43ae8fbf01b1602c3fc1a1a8f76106a38a34f945ea282d3b8e1f6fbfc30348a923141a097f9087c961bdf9595466590e9	\N	\N	\N	t	f	2025-08-11 14:05:08.556545	2025-08-11 14:05:08.55655	\N
3	wook	wook@admin.local	scrypt:32768:8:1$TyCbt0YoZdyh6QjK$8651d05519b7732a13c23a115d1c660bf6e8d9b54565e81309f9263b7df5336956abb457801c820bbf9c79bb682ba1a9ee0267bf22d657c356e84ac867d92df6	Wook	Admin	\N	t	t	2025-08-20 09:18:26.897552	2025-08-23 21:26:13.020418	2025-08-20 09:22:56.630383
1	chris	chris@example.com	scrypt:32768:8:1$5ddzASQg1QEDwpAd$e33b4b605c3483beb213c4e1ba292cf3a88da93223adc390c79b71b90219c313eb2dd0149951cdafd489c13f406ee2b068b6464878893a61b7a1d1c3f01e3053	\N	\N	\N	t	t	2025-08-11 04:55:29.584732	2025-09-05 18:46:33.035377	2025-09-05 18:46:33.034277
4	system	system@ontextract.local	pbkdf2:sha256:600000$LfsdHmtQBeqlEewU$541edbd26b7797ee14b4e65cd8e75ac30e2e320bb2dc40cfeb6ce166c86ee088	\N	\N	\N	t	f	2025-09-05 20:47:09.065109	2025-09-05 20:47:09.065112	\N
\.


--
-- Name: document_embeddings_id_seq; Type: SEQUENCE SET; Schema: public; Owner: ontextract_user
--

SELECT pg_catalog.setval('public.document_embeddings_id_seq', 4, true);


--
-- Name: documents_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.documents_id_seq', 66, true);


--
-- Name: domains_id_seq; Type: SEQUENCE SET; Schema: public; Owner: ontextract_user
--

SELECT pg_catalog.setval('public.domains_id_seq', 1, false);


--
-- Name: experiments_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.experiments_id_seq', 31, true);


--
-- Name: extracted_entities_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.extracted_entities_id_seq', 1, false);


--
-- Name: ontologies_id_seq; Type: SEQUENCE SET; Schema: public; Owner: ontextract_user
--

SELECT pg_catalog.setval('public.ontologies_id_seq', 1, false);


--
-- Name: ontology_entities_id_seq; Type: SEQUENCE SET; Schema: public; Owner: ontextract_user
--

SELECT pg_catalog.setval('public.ontology_entities_id_seq', 1, false);


--
-- Name: ontology_mappings_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.ontology_mappings_id_seq', 1, false);


--
-- Name: ontology_versions_id_seq; Type: SEQUENCE SET; Schema: public; Owner: ontextract_user
--

SELECT pg_catalog.setval('public.ontology_versions_id_seq', 1, false);


--
-- Name: processing_jobs_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.processing_jobs_id_seq', 7, true);


--
-- Name: search_history_id_seq; Type: SEQUENCE SET; Schema: public; Owner: ontextract_user
--

SELECT pg_catalog.setval('public.search_history_id_seq', 1, false);


--
-- Name: text_segments_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.text_segments_id_seq', 2333, true);


--
-- Name: users_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.users_id_seq', 4, true);


--
-- Name: analysis_agents analysis_agents_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.analysis_agents
    ADD CONSTRAINT analysis_agents_pkey PRIMARY KEY (id);


--
-- Name: context_anchors context_anchors_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.context_anchors
    ADD CONSTRAINT context_anchors_pkey PRIMARY KEY (id);


--
-- Name: document_embeddings document_embeddings_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.document_embeddings
    ADD CONSTRAINT document_embeddings_pkey PRIMARY KEY (id);


--
-- Name: documents documents_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.documents
    ADD CONSTRAINT documents_pkey PRIMARY KEY (id);


--
-- Name: domains domains_name_key; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.domains
    ADD CONSTRAINT domains_name_key UNIQUE (name);


--
-- Name: domains domains_namespace_uri_key; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.domains
    ADD CONSTRAINT domains_namespace_uri_key UNIQUE (namespace_uri);


--
-- Name: domains domains_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.domains
    ADD CONSTRAINT domains_pkey PRIMARY KEY (id);


--
-- Name: domains domains_uuid_key; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.domains
    ADD CONSTRAINT domains_uuid_key UNIQUE (uuid);


--
-- Name: experiment_documents experiment_documents_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.experiment_documents
    ADD CONSTRAINT experiment_documents_pkey PRIMARY KEY (experiment_id, document_id);


--
-- Name: experiment_references experiment_references_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.experiment_references
    ADD CONSTRAINT experiment_references_pkey PRIMARY KEY (experiment_id, reference_id);


--
-- Name: experiments experiments_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.experiments
    ADD CONSTRAINT experiments_pkey PRIMARY KEY (id);


--
-- Name: extracted_entities extracted_entities_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.extracted_entities
    ADD CONSTRAINT extracted_entities_pkey PRIMARY KEY (id);


--
-- Name: fuzziness_adjustments fuzziness_adjustments_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.fuzziness_adjustments
    ADD CONSTRAINT fuzziness_adjustments_pkey PRIMARY KEY (id);


--
-- Name: ontologies ontologies_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.ontologies
    ADD CONSTRAINT ontologies_pkey PRIMARY KEY (id);


--
-- Name: ontologies ontologies_uuid_key; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.ontologies
    ADD CONSTRAINT ontologies_uuid_key UNIQUE (uuid);


--
-- Name: ontology_entities ontology_entities_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.ontology_entities
    ADD CONSTRAINT ontology_entities_pkey PRIMARY KEY (id);


--
-- Name: ontology_mappings ontology_mappings_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.ontology_mappings
    ADD CONSTRAINT ontology_mappings_pkey PRIMARY KEY (id);


--
-- Name: ontology_versions ontology_versions_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.ontology_versions
    ADD CONSTRAINT ontology_versions_pkey PRIMARY KEY (id);


--
-- Name: processing_jobs processing_jobs_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.processing_jobs
    ADD CONSTRAINT processing_jobs_pkey PRIMARY KEY (id);


--
-- Name: provenance_chains provenance_chains_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.provenance_chains
    ADD CONSTRAINT provenance_chains_pkey PRIMARY KEY (id);


--
-- Name: search_history search_history_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.search_history
    ADD CONSTRAINT search_history_pkey PRIMARY KEY (id);


--
-- Name: semantic_drift_activities semantic_drift_activities_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.semantic_drift_activities
    ADD CONSTRAINT semantic_drift_activities_pkey PRIMARY KEY (id);


--
-- Name: term_version_anchors term_version_anchors_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.term_version_anchors
    ADD CONSTRAINT term_version_anchors_pkey PRIMARY KEY (id);


--
-- Name: term_version_anchors term_version_anchors_term_version_id_context_anchor_id_key; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.term_version_anchors
    ADD CONSTRAINT term_version_anchors_term_version_id_context_anchor_id_key UNIQUE (term_version_id, context_anchor_id);


--
-- Name: term_versions term_versions_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.term_versions
    ADD CONSTRAINT term_versions_pkey PRIMARY KEY (id);


--
-- Name: terms terms_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.terms
    ADD CONSTRAINT terms_pkey PRIMARY KEY (id);


--
-- Name: terms terms_term_text_created_by_key; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.terms
    ADD CONSTRAINT terms_term_text_created_by_key UNIQUE (term_text, created_by);


--
-- Name: text_segments text_segments_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.text_segments
    ADD CONSTRAINT text_segments_pkey PRIMARY KEY (id);


--
-- Name: ontology_versions uq_ontology_version; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.ontology_versions
    ADD CONSTRAINT uq_ontology_version UNIQUE (ontology_id, version_number);


--
-- Name: users users_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.users
    ADD CONSTRAINT users_pkey PRIMARY KEY (id);


--
-- Name: idx_analysis_agents_active; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_analysis_agents_active ON public.analysis_agents USING btree (is_active) WHERE (is_active = true);


--
-- Name: idx_analysis_agents_type; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_analysis_agents_type ON public.analysis_agents USING btree (agent_type);


--
-- Name: idx_context_anchors_frequency; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_context_anchors_frequency ON public.context_anchors USING btree (frequency DESC);


--
-- Name: idx_context_anchors_term; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_context_anchors_term ON public.context_anchors USING btree (anchor_term);


--
-- Name: idx_documents_parent; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_documents_parent ON public.documents USING btree (parent_document_id);


--
-- Name: idx_documents_type; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_documents_type ON public.documents USING btree (document_type);


--
-- Name: idx_drift_activities_agent; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_drift_activities_agent ON public.semantic_drift_activities USING btree (was_associated_with);


--
-- Name: idx_drift_activities_generated_entity; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_drift_activities_generated_entity ON public.semantic_drift_activities USING btree (generated_entity);


--
-- Name: idx_drift_activities_periods; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_drift_activities_periods ON public.semantic_drift_activities USING btree (start_period, end_period);


--
-- Name: idx_drift_activities_status; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_drift_activities_status ON public.semantic_drift_activities USING btree (activity_status);


--
-- Name: idx_drift_activities_used_entity; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_drift_activities_used_entity ON public.semantic_drift_activities USING btree (used_entity);


--
-- Name: idx_embeddings_document; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_embeddings_document ON public.document_embeddings USING btree (document_id);


--
-- Name: idx_embeddings_model; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_embeddings_model ON public.document_embeddings USING btree (model_name);


--
-- Name: idx_embeddings_term_period; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_embeddings_term_period ON public.document_embeddings USING btree (term, period);


--
-- Name: idx_embeddings_vector; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_embeddings_vector ON public.document_embeddings USING hnsw (embedding public.vector_cosine_ops);


--
-- Name: idx_entity_label; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_entity_label ON public.ontology_entities USING btree (label);


--
-- Name: idx_entity_type; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_entity_type ON public.ontology_entities USING btree (entity_type);


--
-- Name: idx_experiment_references_experiment; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_experiment_references_experiment ON public.experiment_references USING btree (experiment_id);


--
-- Name: idx_experiment_references_reference; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_experiment_references_reference ON public.experiment_references USING btree (reference_id);


--
-- Name: idx_fuzziness_adjustments_user; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_fuzziness_adjustments_user ON public.fuzziness_adjustments USING btree (adjusted_by);


--
-- Name: idx_fuzziness_adjustments_version; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_fuzziness_adjustments_version ON public.fuzziness_adjustments USING btree (term_version_id);


--
-- Name: idx_ontology_entity; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_ontology_entity ON public.ontology_entities USING btree (ontology_id, entity_type);


--
-- Name: idx_term_version_anchors_anchor; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_term_version_anchors_anchor ON public.term_version_anchors USING btree (context_anchor_id);


--
-- Name: idx_term_version_anchors_similarity; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_term_version_anchors_similarity ON public.term_version_anchors USING btree (similarity_score DESC);


--
-- Name: idx_term_version_anchors_version; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_term_version_anchors_version ON public.term_version_anchors USING btree (term_version_id);


--
-- Name: idx_term_versions_corpus; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_term_versions_corpus ON public.term_versions USING btree (corpus_source);


--
-- Name: idx_term_versions_current; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_term_versions_current ON public.term_versions USING btree (is_current) WHERE (is_current = true);


--
-- Name: idx_term_versions_fuzziness; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_term_versions_fuzziness ON public.term_versions USING btree (fuzziness_score);


--
-- Name: idx_term_versions_temporal_period; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_term_versions_temporal_period ON public.term_versions USING btree (temporal_period);


--
-- Name: idx_term_versions_temporal_years; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_term_versions_temporal_years ON public.term_versions USING btree (temporal_start_year, temporal_end_year);


--
-- Name: idx_term_versions_term_id; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_term_versions_term_id ON public.term_versions USING btree (term_id);


--
-- Name: idx_terms_created_by; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_terms_created_by ON public.terms USING btree (created_by);


--
-- Name: idx_terms_research_domain; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_terms_research_domain ON public.terms USING btree (research_domain);


--
-- Name: idx_terms_status; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_terms_status ON public.terms USING btree (status);


--
-- Name: idx_terms_text; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_terms_text ON public.terms USING btree (term_text);


--
-- Name: ix_analysis_agents_agent_type; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_analysis_agents_agent_type ON public.analysis_agents USING btree (agent_type);


--
-- Name: ix_analysis_agents_is_active; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_analysis_agents_is_active ON public.analysis_agents USING btree (is_active);


--
-- Name: ix_context_anchors_anchor_term; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE UNIQUE INDEX ix_context_anchors_anchor_term ON public.context_anchors USING btree (anchor_term);


--
-- Name: ix_context_anchors_frequency; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_context_anchors_frequency ON public.context_anchors USING btree (frequency);


--
-- Name: ix_documents_user_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_documents_user_id ON public.documents USING btree (user_id);


--
-- Name: ix_entity_embedding_vector; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_entity_embedding_vector ON public.ontology_entities USING ivfflat (embedding public.vector_cosine_ops) WITH (lists='100');


--
-- Name: ix_experiments_user_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_experiments_user_id ON public.experiments USING btree (user_id);


--
-- Name: ix_extracted_entities_processing_job_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_extracted_entities_processing_job_id ON public.extracted_entities USING btree (processing_job_id);


--
-- Name: ix_extracted_entities_text_segment_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_extracted_entities_text_segment_id ON public.extracted_entities USING btree (text_segment_id);


--
-- Name: ix_fuzziness_adjustments_adjusted_by; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_fuzziness_adjustments_adjusted_by ON public.fuzziness_adjustments USING btree (adjusted_by);


--
-- Name: ix_fuzziness_adjustments_term_version_id; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_fuzziness_adjustments_term_version_id ON public.fuzziness_adjustments USING btree (term_version_id);


--
-- Name: ix_ontology_mappings_extracted_entity_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_ontology_mappings_extracted_entity_id ON public.ontology_mappings USING btree (extracted_entity_id);


--
-- Name: ix_processing_jobs_document_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_processing_jobs_document_id ON public.processing_jobs USING btree (document_id);


--
-- Name: ix_processing_jobs_parent_job_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_processing_jobs_parent_job_id ON public.processing_jobs USING btree (parent_job_id);


--
-- Name: ix_processing_jobs_user_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_processing_jobs_user_id ON public.processing_jobs USING btree (user_id);


--
-- Name: ix_semantic_drift_activities_activity_status; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_semantic_drift_activities_activity_status ON public.semantic_drift_activities USING btree (activity_status);


--
-- Name: ix_semantic_drift_activities_end_period; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_semantic_drift_activities_end_period ON public.semantic_drift_activities USING btree (end_period);


--
-- Name: ix_semantic_drift_activities_generated_entity; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_semantic_drift_activities_generated_entity ON public.semantic_drift_activities USING btree (generated_entity);


--
-- Name: ix_semantic_drift_activities_start_period; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_semantic_drift_activities_start_period ON public.semantic_drift_activities USING btree (start_period);


--
-- Name: ix_semantic_drift_activities_used_entity; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_semantic_drift_activities_used_entity ON public.semantic_drift_activities USING btree (used_entity);


--
-- Name: ix_semantic_drift_activities_was_associated_with; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_semantic_drift_activities_was_associated_with ON public.semantic_drift_activities USING btree (was_associated_with);


--
-- Name: ix_term_versions_corpus_source; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_term_versions_corpus_source ON public.term_versions USING btree (corpus_source);


--
-- Name: ix_term_versions_is_current; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_term_versions_is_current ON public.term_versions USING btree (is_current);


--
-- Name: ix_term_versions_temporal_end_year; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_term_versions_temporal_end_year ON public.term_versions USING btree (temporal_end_year);


--
-- Name: ix_term_versions_temporal_period; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_term_versions_temporal_period ON public.term_versions USING btree (temporal_period);


--
-- Name: ix_term_versions_temporal_start_year; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_term_versions_temporal_start_year ON public.term_versions USING btree (temporal_start_year);


--
-- Name: ix_term_versions_term_id; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_term_versions_term_id ON public.term_versions USING btree (term_id);


--
-- Name: ix_terms_created_by; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_terms_created_by ON public.terms USING btree (created_by);


--
-- Name: ix_terms_research_domain; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_terms_research_domain ON public.terms USING btree (research_domain);


--
-- Name: ix_text_segments_document_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_text_segments_document_id ON public.text_segments USING btree (document_id);


--
-- Name: ix_text_segments_parent_segment_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_text_segments_parent_segment_id ON public.text_segments USING btree (parent_segment_id);


--
-- Name: ix_users_email; Type: INDEX; Schema: public; Owner: postgres
--

CREATE UNIQUE INDEX ix_users_email ON public.users USING btree (email);


--
-- Name: ix_users_username; Type: INDEX; Schema: public; Owner: postgres
--

CREATE UNIQUE INDEX ix_users_username ON public.users USING btree (username);


--
-- Name: term_version_anchors trigger_update_context_anchor_frequency; Type: TRIGGER; Schema: public; Owner: ontextract_user
--

CREATE TRIGGER trigger_update_context_anchor_frequency AFTER INSERT OR DELETE ON public.term_version_anchors FOR EACH ROW EXECUTE FUNCTION public.update_context_anchor_frequency();


--
-- Name: terms trigger_update_terms_updated_at; Type: TRIGGER; Schema: public; Owner: ontextract_user
--

CREATE TRIGGER trigger_update_terms_updated_at BEFORE UPDATE ON public.terms FOR EACH ROW EXECUTE FUNCTION public.update_terms_updated_at();


--
-- Name: analysis_agents analysis_agents_user_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.analysis_agents
    ADD CONSTRAINT analysis_agents_user_id_fkey FOREIGN KEY (user_id) REFERENCES public.users(id);


--
-- Name: context_anchors context_anchors_first_used_in_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.context_anchors
    ADD CONSTRAINT context_anchors_first_used_in_fkey FOREIGN KEY (first_used_in) REFERENCES public.term_versions(id);


--
-- Name: context_anchors context_anchors_last_used_in_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.context_anchors
    ADD CONSTRAINT context_anchors_last_used_in_fkey FOREIGN KEY (last_used_in) REFERENCES public.term_versions(id);


--
-- Name: document_embeddings document_embeddings_document_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.document_embeddings
    ADD CONSTRAINT document_embeddings_document_id_fkey FOREIGN KEY (document_id) REFERENCES public.documents(id);


--
-- Name: documents documents_user_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.documents
    ADD CONSTRAINT documents_user_id_fkey FOREIGN KEY (user_id) REFERENCES public.users(id);


--
-- Name: experiment_documents experiment_documents_document_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.experiment_documents
    ADD CONSTRAINT experiment_documents_document_id_fkey FOREIGN KEY (document_id) REFERENCES public.documents(id);


--
-- Name: experiment_documents experiment_documents_experiment_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.experiment_documents
    ADD CONSTRAINT experiment_documents_experiment_id_fkey FOREIGN KEY (experiment_id) REFERENCES public.experiments(id);


--
-- Name: experiment_references experiment_references_experiment_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.experiment_references
    ADD CONSTRAINT experiment_references_experiment_id_fkey FOREIGN KEY (experiment_id) REFERENCES public.experiments(id);


--
-- Name: experiment_references experiment_references_reference_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.experiment_references
    ADD CONSTRAINT experiment_references_reference_id_fkey FOREIGN KEY (reference_id) REFERENCES public.documents(id);


--
-- Name: experiments experiments_user_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.experiments
    ADD CONSTRAINT experiments_user_id_fkey FOREIGN KEY (user_id) REFERENCES public.users(id);


--
-- Name: extracted_entities extracted_entities_processing_job_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.extracted_entities
    ADD CONSTRAINT extracted_entities_processing_job_id_fkey FOREIGN KEY (processing_job_id) REFERENCES public.processing_jobs(id);


--
-- Name: extracted_entities extracted_entities_text_segment_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.extracted_entities
    ADD CONSTRAINT extracted_entities_text_segment_id_fkey FOREIGN KEY (text_segment_id) REFERENCES public.text_segments(id);


--
-- Name: documents fk_documents_parent_document_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.documents
    ADD CONSTRAINT fk_documents_parent_document_id FOREIGN KEY (parent_document_id) REFERENCES public.documents(id) ON DELETE CASCADE;


--
-- Name: fuzziness_adjustments fuzziness_adjustments_adjusted_by_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.fuzziness_adjustments
    ADD CONSTRAINT fuzziness_adjustments_adjusted_by_fkey FOREIGN KEY (adjusted_by) REFERENCES public.users(id);


--
-- Name: fuzziness_adjustments fuzziness_adjustments_term_version_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.fuzziness_adjustments
    ADD CONSTRAINT fuzziness_adjustments_term_version_id_fkey FOREIGN KEY (term_version_id) REFERENCES public.term_versions(id);


--
-- Name: ontologies ontologies_domain_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.ontologies
    ADD CONSTRAINT ontologies_domain_id_fkey FOREIGN KEY (domain_id) REFERENCES public.domains(id);


--
-- Name: ontologies ontologies_parent_ontology_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.ontologies
    ADD CONSTRAINT ontologies_parent_ontology_id_fkey FOREIGN KEY (parent_ontology_id) REFERENCES public.ontologies(id);


--
-- Name: ontology_entities ontology_entities_ontology_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.ontology_entities
    ADD CONSTRAINT ontology_entities_ontology_id_fkey FOREIGN KEY (ontology_id) REFERENCES public.ontologies(id);


--
-- Name: ontology_mappings ontology_mappings_extracted_entity_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.ontology_mappings
    ADD CONSTRAINT ontology_mappings_extracted_entity_id_fkey FOREIGN KEY (extracted_entity_id) REFERENCES public.extracted_entities(id);


--
-- Name: ontology_versions ontology_versions_ontology_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.ontology_versions
    ADD CONSTRAINT ontology_versions_ontology_id_fkey FOREIGN KEY (ontology_id) REFERENCES public.ontologies(id);


--
-- Name: processing_jobs processing_jobs_document_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.processing_jobs
    ADD CONSTRAINT processing_jobs_document_id_fkey FOREIGN KEY (document_id) REFERENCES public.documents(id);


--
-- Name: processing_jobs processing_jobs_parent_job_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.processing_jobs
    ADD CONSTRAINT processing_jobs_parent_job_id_fkey FOREIGN KEY (parent_job_id) REFERENCES public.processing_jobs(id);


--
-- Name: processing_jobs processing_jobs_user_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.processing_jobs
    ADD CONSTRAINT processing_jobs_user_id_fkey FOREIGN KEY (user_id) REFERENCES public.users(id);


--
-- Name: provenance_chains provenance_chains_derivation_activity_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.provenance_chains
    ADD CONSTRAINT provenance_chains_derivation_activity_fkey FOREIGN KEY (derivation_activity) REFERENCES public.semantic_drift_activities(id);


--
-- Name: semantic_drift_activities semantic_drift_activities_created_by_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.semantic_drift_activities
    ADD CONSTRAINT semantic_drift_activities_created_by_fkey FOREIGN KEY (created_by) REFERENCES public.users(id);


--
-- Name: semantic_drift_activities semantic_drift_activities_generated_entity_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.semantic_drift_activities
    ADD CONSTRAINT semantic_drift_activities_generated_entity_fkey FOREIGN KEY (generated_entity) REFERENCES public.term_versions(id);


--
-- Name: semantic_drift_activities semantic_drift_activities_used_entity_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.semantic_drift_activities
    ADD CONSTRAINT semantic_drift_activities_used_entity_fkey FOREIGN KEY (used_entity) REFERENCES public.term_versions(id);


--
-- Name: semantic_drift_activities semantic_drift_activities_was_associated_with_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.semantic_drift_activities
    ADD CONSTRAINT semantic_drift_activities_was_associated_with_fkey FOREIGN KEY (was_associated_with) REFERENCES public.analysis_agents(id);


--
-- Name: term_version_anchors term_version_anchors_context_anchor_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.term_version_anchors
    ADD CONSTRAINT term_version_anchors_context_anchor_id_fkey FOREIGN KEY (context_anchor_id) REFERENCES public.context_anchors(id);


--
-- Name: term_version_anchors term_version_anchors_term_version_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.term_version_anchors
    ADD CONSTRAINT term_version_anchors_term_version_id_fkey FOREIGN KEY (term_version_id) REFERENCES public.term_versions(id);


--
-- Name: term_versions term_versions_created_by_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.term_versions
    ADD CONSTRAINT term_versions_created_by_fkey FOREIGN KEY (created_by) REFERENCES public.users(id);


--
-- Name: term_versions term_versions_term_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.term_versions
    ADD CONSTRAINT term_versions_term_id_fkey FOREIGN KEY (term_id) REFERENCES public.terms(id);


--
-- Name: term_versions term_versions_was_derived_from_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.term_versions
    ADD CONSTRAINT term_versions_was_derived_from_fkey FOREIGN KEY (was_derived_from) REFERENCES public.term_versions(id);


--
-- Name: terms terms_created_by_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.terms
    ADD CONSTRAINT terms_created_by_fkey FOREIGN KEY (created_by) REFERENCES public.users(id);


--
-- Name: terms terms_updated_by_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.terms
    ADD CONSTRAINT terms_updated_by_fkey FOREIGN KEY (updated_by) REFERENCES public.users(id);


--
-- Name: text_segments text_segments_document_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.text_segments
    ADD CONSTRAINT text_segments_document_id_fkey FOREIGN KEY (document_id) REFERENCES public.documents(id);


--
-- Name: text_segments text_segments_parent_segment_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.text_segments
    ADD CONSTRAINT text_segments_parent_segment_id_fkey FOREIGN KEY (parent_segment_id) REFERENCES public.text_segments(id);


--
-- Name: SCHEMA public; Type: ACL; Schema: -; Owner: pg_database_owner
--

GRANT ALL ON SCHEMA public TO ontextract_user;


--
-- Name: TABLE documents; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.documents TO ontextract_user;


--
-- Name: SEQUENCE documents_id_seq; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON SEQUENCE public.documents_id_seq TO ontextract_user;


--
-- Name: TABLE experiment_documents; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.experiment_documents TO ontextract_user;


--
-- Name: TABLE experiment_references; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.experiment_references TO ontextract_user;


--
-- Name: TABLE experiments; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.experiments TO ontextract_user;


--
-- Name: SEQUENCE experiments_id_seq; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON SEQUENCE public.experiments_id_seq TO ontextract_user;


--
-- Name: TABLE extracted_entities; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.extracted_entities TO ontextract_user;


--
-- Name: SEQUENCE extracted_entities_id_seq; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON SEQUENCE public.extracted_entities_id_seq TO ontextract_user;


--
-- Name: TABLE ontology_mappings; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.ontology_mappings TO ontextract_user;


--
-- Name: SEQUENCE ontology_mappings_id_seq; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON SEQUENCE public.ontology_mappings_id_seq TO ontextract_user;


--
-- Name: TABLE processing_jobs; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.processing_jobs TO ontextract_user;


--
-- Name: SEQUENCE processing_jobs_id_seq; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON SEQUENCE public.processing_jobs_id_seq TO ontextract_user;


--
-- Name: TABLE text_segments; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.text_segments TO ontextract_user;


--
-- Name: SEQUENCE text_segments_id_seq; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON SEQUENCE public.text_segments_id_seq TO ontextract_user;


--
-- Name: TABLE users; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.users TO ontextract_user;


--
-- Name: SEQUENCE users_id_seq; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON SEQUENCE public.users_id_seq TO ontextract_user;


--
-- PostgreSQL database dump complete
--

\unrestrict 8GYRd8dpdd6bg7SNL43N3J9QUBKlnTLhKqsLiYNY5z3c9ztcl4FQhlXV9kk4Vkh

